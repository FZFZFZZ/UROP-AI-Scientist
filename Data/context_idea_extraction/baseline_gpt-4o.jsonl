{"id": "nDvgHIBRxQ", "Context": "The evaluation of large language models (LLMs) for mathematical reasoning is crucial, yet current benchmarks mainly focus on problem-solving, risking overfitting and failing to accurately measure true mathematical reasoning abilities. There is a need for a comprehensive evaluation that reflects real-world user experience and assesses the robustness and generalization of these models across diverse tasks.", "Idea": "Introduce MathCheck, a checklist designed to test task generalization and reasoning robustness, along with an automatic tool for efficient checklist generation. MathCheck includes various mathematical reasoning tasks and robustness tests, and is used to develop MathCheck-GSM and MathCheck-GEO for evaluating mathematical textual reasoning and multi-modal reasoning capabilities, respectively."}
{"id": "ZsP3YbYeE9", "Context": "A common method for developing agents with Language Models involves iterative prompting, reflecting on outputs, and updating prompts until the task is completed. However, this approach faces challenges such as limited exploration of decision space due to repetitive reflections and the inability to utilize insights from previously solved tasks.", "Idea": "Introduce DoT (Diversity of Thoughts), a framework that reduces redundant reflections to improve decision-space exploration and incorporates a task-agnostic memory component for knowledge retrieval from past tasks."}
