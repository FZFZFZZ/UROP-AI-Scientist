{"id": "nDvgHIBRxQ", "Context": "The mathematical reasoning ability of large language models (LLMs) is a key indicator of their power. However, defining and evaluating these abilities comprehensively, especially in real-world scenarios, is challenging. Current benchmarks focus mainly on problem-solving, risking model overfitting and failing to accurately measure true mathematical reasoning capabilities.", "Idea": "Introduce MathCheck, a checklist designed to test task generalization and reasoning robustness, along with an automatic tool for efficient checklist generation. MathCheck includes various mathematical reasoning tasks and robustness tests, and is used to develop MathCheck-GSM and MathCheck-GEO for evaluating mathematical textual and multi-modal reasoning capabilities, respectively."}
{"id": "ZsP3YbYeE9", "Context": "A common method for developing agents with Language Models involves iterative prompting, reflecting on outputs, and updating prompts until the task is completed. However, this approach faces challenges such as limited exploration of decision space due to repetitive reflections and the inability to utilize insights from previously solved tasks.", "Idea": "Introduce DoT (Diversity of Thoughts), a framework that reduces redundant reflections to improve decision-space exploration and incorporates a task-agnostic memory component for knowledge retrieval from past tasks, enhancing the agent's problem-solving capabilities."}
{"id": "I4e82CIDxv", "Context": "Previous methods for explaining language model behaviors relied on circuits composed of polysemantic and difficult-to-interpret units such as attention heads or neurons. These circuits were not suitable for many downstream applications due to their complexity and lack of interpretability.", "Idea": "Introduce sparse feature circuits, which are causally implicated subnetworks of human-interpretable features, to provide a detailed understanding of neural network mechanisms. These circuits are based on fine-grained units, making them useful for downstream tasks, such as improving classifier generalization by ablating task-irrelevant features. Additionally, an unsupervised and scalable interpretability pipeline is demonstrated by discovering thousands of sparse feature circuits for automatically discovered model behaviors."}
{"id": "pHe4P1IVnb", "Context": "As large language models become more complex, the challenge arises in adapting alignment techniques, especially when human supervision is limited. This scenario is characterized by weak supervision attempting to leverage the full potential of stronger models.", "Idea": "Extend the Weak-to-Strong framework to WeakS-to-Strong by using an ensemble of weak models to simulate human opinion variability, employing Bayesian confidence scores for guidance, and applying direct preference optimization to enhance preference learning in text generation tasks."}
{"id": "Pj4Aid3XqL", "Context": "Pre-trained large language models (LLMs) that are further trained with image data show strong performance on vision-language tasks. However, there is uncertainty about the effectiveness of this two-step training process compared to vision-language models (VLMs) that integrate images earlier in the training phase.", "Idea": "Investigate the impact of introducing visual tokens at different stages of pre-training by training models across various datasets, scales, and image-text ratios, and then fine-tuning them to evaluate performance on vision-language and text-only tasks."}
{"id": "B2Fqu7Y2cd", "Context": "Large language models trained on text can infer instructions directly from data, but models trained solely on audio data lack this capability because audio data does not inherently contain the instructions used to generate it. This presents a challenge in achieving compositional abilities in audio synthesis and transformation tasks.", "Idea": "Introduce a specialized dataset generation approach optimized for audio generation and transformation tasks, revealing meaningful relationships between audio and language. Additionally, propose ComposableART, an inference-time technique that extends classifier-free guidance to compositional guidance, enabling flexible composition of instructions for customizable audio outputs."}
{"id": "MWHIIWrWWu", "Context": "Controlling high-dimensional nonlinear systems, such as those in biological and robotic applications, is difficult due to the large state and action spaces. Although deep reinforcement learning has been successful in these areas, it is computationally demanding and time-consuming, making it unsuitable for handling large sets of tasks that require extensive manual tuning.", "Idea": "Introduce MPC$^2$, a hierarchical model-based learning algorithm that combines a sampling-based model predictive controller for target posture planning with a morphology-aware proportional controller for actuator coordination, enabling zero-shot and near-real-time control of high-dimensional complex dynamical systems."}
{"id": "cmYScmfu4Q", "Context": "Reward inference is a crucial step in the Reinforcement Learning from Human Feedback (RLHF) process for fine-tuning Large Language Models (LLMs). However, RLHF encounters challenges like distribution shift, reward model overfitting, and problem misspecification. Direct Preference Optimization (DPO) offers an alternative by simplifying the pipeline, but it is limited to specific settings such as bandits or deterministic Markov Decision Processes (MDPs).", "Idea": "Develop two RLHF algorithms that bypass reward inference for broader RL problems, using human preferences to estimate local value function differences and approximate policy gradients with a zeroth-order gradient approximator, achieving polynomial convergence rates."}
{"id": "6HcnC3pPkp", "Context": "The rapid advancement of test-time compute search strategies has highlighted the need for robust verifiers to enhance the mathematical problem-solving capabilities of large language models. Current inference strategies depend on verifiers designed for Best-of-N search, which are not optimal for tree search techniques. These existing verifiers provide indirect assessments of partial solutions and may undervalue intermediate steps, leading to premature pruning of promising paths.", "Idea": "Introduce token-supervised value models (TVMs), a new class of verifiers that assign probabilities to each token, indicating the likelihood of reaching the correct final answer. This token-level supervision allows TVMs to directly evaluate partial solutions, effectively distinguishing between promising and incorrect intermediate steps during tree search."}
{"id": "BAelAyADqn", "Context": "Longitudinal human behavior modeling is crucial for applications like patient monitoring and lifestyle recommendations, using data from devices like smartphones and smartwatches. This field aims to predict health outcomes such as depression and stress based on time series of individual behaviors. However, existing models often lack accuracy and fail to consider realistic data aspects like diverse feature types and high missing value rates, as well as resource consumption issues, making their practical application questionable.", "Idea": "Introduce MuHBoost, a multi-label boosting method that leverages large language model prompting and multi-label classification to predict multiple health outcomes simultaneously. To address the issue of LLMs hallucinating when handling multiple questions, two variants of MuHBoost are developed to enhance predictive performance."}
{"id": "svp1EBA6hA", "Context": "Diffusion models are advanced generative models that allow for precise control over generated samples. These models, when trained on large datasets, have been successful, but there is often a need for additional controls during downstream fine-tuning processes, treating them as pre-trained models.", "Idea": "Introduce a novel method called CTRL, which uses reinforcement learning to add controls to pre-trained diffusion models. This method formulates the task as an RL problem, using a classifier learned from an offline dataset and the KL divergence against pre-trained models as reward functions, enabling sampling from the conditional distribution with additional controls during inference."}
{"id": "l2zFn6TIQi", "Context": "The growing capabilities and deployment of large generative models have led to concerns regarding their reliability, safety, and potential misuse. Recent efforts have focused on controlling model generation by steering model activations to manage the emergence of certain concepts or behaviors in the output.", "Idea": "Introduce Activation Transport (AcT), a framework that uses optimal transport theory to steer activations, offering fine-grained control over model behavior with minimal computational cost and impact on model performance. AcT is applicable across different modalities and generalizes previous activation-steering methods."}
{"id": "FpiCLJrSW8", "Context": "The trustworthiness of Large Language Models (LLMs) is a critical factor that encompasses the reliability, safety, and ethical alignment of their outputs. Reinforcement Learning From Human Feedback (RLHF) is commonly used to align LLMs with human preferences, but its impact on trustworthiness has not been thoroughly evaluated. This study examines the performance of models aligned with general-purpose preference data across various trustworthiness aspects, revealing that RLHF does not inherently ensure trustworthiness and may have adverse effects.", "Idea": "Propose the adaptation of efficient influence function-based data attribution methods to the RLHF setting to better understand the impact of fine-tuning data on specific trustworthiness benchmarks, demonstrating feasibility through estimated attribution scores."}
{"id": "5WEpbilssv", "Context": "High-content perturbation experiments offer detailed insights into biomolecular systems but are hindered by high costs and complex analysis. Current machine learning approaches fail to capture the semantic depth of biological data and are not well-aligned with biological analysis goals.", "Idea": "Introduce PerturbQA, a benchmark designed for structured reasoning over perturbation experiments, focusing on predicting differential expression, change of direction for unseen perturbations, and gene set enrichment. Additionally, propose Summer, a domain-informed LLM framework that effectively addresses these challenges."}
{"id": "9OfKxKoYNw", "Context": "Recent advancements in diffusion models have enabled text-guided image manipulation, allowing users to create realistic edited images with simple textual prompts. However, there is a significant concern about the misuse of these methods, particularly in generating misleading or harmful content. Existing defense strategies, which use imperceptible adversarial noise to cause model failure, are inadequate against sophisticated manipulations like editing with a mask.", "Idea": "Propose DiffusionGuard, a defense method against unauthorized edits by diffusion-based image editing models, which introduces a novel objective to generate adversarial noise targeting the early stage of the diffusion process. This method also includes a mask-augmentation technique to enhance robustness against various masks during test time."}
{"id": "oZkqkkvdND", "Context": "Variational Autoencoders (VAEs) are increasingly used in safety-critical applications where it is essential to provide certified probabilistic guarantees on performance, especially under adversarial attacks.", "Idea": "Propose a novel method called CIVET for certified training of VAEs, which involves bounding the worst-case VAE error by focusing on carefully chosen support sets at the latent layer, and introduce a new training algorithm based on this insight."}
{"id": "Pujt3ADZgI", "Context": "Reinforcement Learning with Human Feedback (RLHF) has been successful in aligning large language models with human preferences. Current RLHF methods are primarily reward-based and follow the Bradley-Terry model, which may not fully capture the complexity of human preferences, leading to high computational or annotation costs.", "Idea": "Propose a novel online algorithm called iterative Nash policy optimization (INPO), which formulates RLHF as a two-player game and uses no-regret learning to approximate the Nash policy. This approach introduces a new loss objective that is minimized over a preference dataset, eliminating the need for estimating expected win rates for individual responses."}
{"id": "9HK2rHNAhd", "Context": "Optimizing the Key-Value (KV) cache in Large Language Models (LLMs) is crucial for reducing inference costs. Existing KV-cache compression methods often focus on sparsifying token sequences based on token importance but allocate the same KV budget across all layers, which is inefficient as different layers have varying sensitivity to input tokens.", "Idea": "Introduce a method to optimize KV-cache allocation by identifying the importance of attention layers, allowing for joint optimization across sequence-wise and layer-wise dimensions. This involves measuring each layer's importance through cosine similarity of input prompt differences and adjusting KV budgets accordingly, incorporating sequence-wise algorithms for precise compression."}
{"id": "Mfnh1Sqdwf", "Context": "Predicting gene expressions from DNA sequences is challenging due to the difficulty in identifying regulatory elements that control gene expressions. Understanding the causal relationship between epigenomic signals, DNA sequences, and their regulatory elements is crucial for accurate prediction.", "Idea": "Introduce Seq2Exp, a Sequence to Expression network designed to discover and extract regulatory elements that drive gene expression. The approach involves decomposing epigenomic signals and DNA sequences based on causal active regulatory elements and applying an information bottleneck with the Beta distribution to filter out non-causal components."}
{"id": "pbre0HKsfE", "Context": "Large language models (LLMs) can generate personalized responses based on user interactions, but this capability raises significant privacy concerns. Homomorphic encryption (HE) offers a cryptographic solution for privacy-preserving machine learning by allowing computations on encrypted data. However, the computational demands of transformers make it challenging to apply HE to LLMs effectively.", "Idea": "Propose a modified transformer architecture that is compatible with homomorphic encryption, focusing on inference after personalized fine-tuning. This approach uses LoRA fine-tuning and Gaussian kernels to achieve substantial computational speedups while maintaining performance similar to models operating on plaintext data."}
{"id": "WeJEidTzff", "Context": "Commuting Origin-Destination (OD) flows are essential for urban planning and transportation, providing insights into population movement between residential and work areas. However, collecting this data is costly, leading researchers to develop models that use urban attributes like sociodemographics to generate OD flows for cities without historical data. Existing models use various techniques and achieve improvements on different datasets, making it difficult to establish a unified standard for performance comparison.", "Idea": "Introduce a large-scale dataset of commuting OD flows for 3,333 areas across diverse urban environments in the United States and benchmark widely used models for OD flow generation, revealing that network-based generative models perform optimally in precision and generalization."}
{"id": "kiOxNsrpQy", "Context": "Graph Neural Networks (GNNs) are increasingly used, necessitating reliable tools to explain their predictions. A key requirement for these explanations is faithfulness, meaning they accurately reflect the GNN's reasoning process. However, multiple faithfulness metrics exist, leading to confusion about what constitutes faithfulness and how to achieve it.", "Idea": "Demonstrate that existing faithfulness metrics are not interchangeable and can overlook important explanation properties. Prove that optimizing for faithfulness is not always beneficial, especially for injective regular GNNs, and explore the relationship between GNN architecture choices and faithfulness, highlighting the link between faithfulness and out-of-distribution generalization."}
{"id": "Kpjvm2mB0K", "Context": "The study focuses on one-pass streaming algorithms for underdetermined ℓp linear regression problems, where the matrix A has significantly more columns than rows. This problem generalizes basis pursuit and least squares solutions for underdetermined systems and is relevant in scenarios like edge insertion graph streams, capturing various flow problems on undirected graphs. The challenge is to design algorithms that use significantly less space than the entire data stream.", "Idea": "Develop a streaming algorithm that constructs a sparse instance of the ℓp regression problem, approximating the cost with reduced space requirements. For p in [2, ∞], the algorithm uses space proportional to the number of rows, while for p in (1, 2), it adapts based on the H\"older conjugate exponent. Additionally, provide sublinear space upper bounds for outputting approximate solutions, achieving efficient space usage for different values of p."}
{"id": "eENHKMTOfW", "Context": "The rise of large language models (LLMs) has created a significant disparity between industrial research labs and smaller entities. While large labs have the resources to fine-tune LLMs effectively, individual developers and small organizations struggle due to limited resources, hindering their ability to explore the experiment space.", "Idea": "Conduct a comprehensive study on supervised fine-tuning of small-sized LLMs using instruction-tuning datasets across diverse domains. The study explores various training configurations and strategies, challenging common practices and providing detailed documentation to guide practitioners in fine-tuning small LLMs efficiently."}
{"id": "TEkoMEjf7E", "Context": "Generative 3D modeling has advanced significantly but faces challenges due to its inherently ill-posed nature, affecting quality and controllability. Designers often refer to existing 3D models when creating new ones, highlighting the need for improved methods in 3D generation.", "Idea": "Introduce Phidias, a novel generative model that uses diffusion for reference-augmented 3D generation, leveraging a retrieved or user-provided 3D reference model to guide the process. It integrates meta-ControlNet for dynamic conditioning, dynamic reference routing to address misalignment, and self-reference augmentations for self-supervised training."}
{"id": "EV7FMBZxnx", "Context": "Detecting concealed objects, such as in vivo lesions or camouflage, requires specialized imaging systems. Lensless cameras, which are compact and flexible, present a promising alternative to traditional bulky lens systems. However, the lack of lenses results in measurements that lack visual semantics, creating significant challenges for concealed object detection.", "Idea": "Propose a region gaze-amplification network (RGANet) to progressively exploit concealed objects from lensless imaging measurements. This includes a region gaze module (RGM) to mine spatial-frequency cues and a region amplifier (RA) to enhance the details of object regions, improving concealed object detection performance."}
{"id": "21rSeWJHPF", "Context": "Ranking vertices in a graph is a fundamental task in computer science, often using centrality measures like PageRank. Traditional ranking algorithms can produce unbalanced rankings, especially in graphs with underlying communities, leading to information loss, polarized opinions, and reduced diversity.", "Idea": "Introduce a new approach called relative centrality, which uses iterative graph-dependent local normalization of centrality scores to promote balancedness in rankings while maintaining their validity. This method addresses the unbalancedness observed in centrality measures, particularly in structures like multi-core-periphery with communities (MCPC)."}
{"id": "l0gZS0sAlf", "Context": "The training and fine-tuning of large language models (LLMs) often involve diverse textual data from multiple sources, which poses challenges due to conflicting gradient directions, hindering optimization and specialization. These challenges can undermine model generalization across tasks, resulting in reduced downstream performance. Recent research suggests that fine-tuning LLMs on carefully selected, task-specific subsets of data can match or even surpass the performance of using the entire dataset.", "Idea": "Propose the Ensembles of Low-Rank Expert Adapters (ELREA) framework, which clusters training instructions based on gradient directions to reduce conflicts during optimization. Expert adapters are trained on these clusters using the low-rank adaptation (LoRA) technique for efficiency and scalability. During inference, ELREA combines predictions from the most relevant expert adapters based on input data's gradient similarity to the training clusters."}
{"id": "mFY0tPDWK8", "Context": "In recent years, machine learning has been increasingly used to predict initial solutions for mixed-integer linear programming (MILP) problems. These methods typically involve predicting a solution and fixing a subset of variables to reduce the problem's dimensionality before solving the reduced problem to obtain final solutions. However, directly fixing variable values based on predictions can result in low-quality solutions or infeasible problems if the predictions are not sufficiently accurate.", "Idea": "Introduce the Apollo-MILP framework, which alternates between prediction and correction steps to improve solution accuracy. It uses a trust-region search to refine solutions and incorporates an Uncertainty-based Error upper BOund (UEBO) to evaluate prediction uncertainty, fixing only those values with high confidence."}
{"id": "mYgoNEsUDi", "Context": "Diffusion models have recently gained attention as a powerful tool for generative AI on graphs, with applications in areas like drug design and knowledge discovery. Despite their potential, current graph diffusion models struggle to comprehensively capture higher-order topological graph properties, limiting their generalizability and effectiveness for various tasks.", "Idea": "Introduce a computationally efficient topological summary called zigzag spaghetti (ZS) that extracts latent topological graph descriptors at different resolutions using zigzag persistence. This approach integrates dynamic topological information into graph diffusion models, providing theoretical stability guarantees."}
{"id": "LBl7Hez0fF", "Context": "Hallucination is a significant issue in deploying large vision-language models (LVLMs), often caused by misalignments between visual inputs and textual outputs. This problem is distinct from hallucination in large language models (LLMs) due to the unique structure of LVLMs, where image encoders and text decoders are typically pre-trained separately, leading to sensitivity in text decoders to vision inputs.", "Idea": "Introduce Visual and Textual Intervention (VTI), a novel technique that reduces hallucinations by steering latent space representations during inference to enhance the stability of vision features. VTI is a task-agnostic, test-time intervention that can be applied without additional training costs."}
{"id": "dQ2xiSIYzp", "Context": "The task of learning 3D human Gaussians from a single image involves recovering detailed appearance and geometry, including unobserved regions. Existing methods face challenges in generating realistic human poses and shapes, and often struggle with generalization across different datasets and in-the-wild images.", "Idea": "Introduce a single-view generalizable Human Gaussian Model (HGM) that uses a generate-then-refine pipeline guided by human body and diffusion priors. The approach employs a ControlNet to refine back-view images and incorporates human priors from the SMPL-X model to improve pose and shape realism through sparse convolution and attention mechanisms."}
{"id": "uHLgDEgiS5", "Context": "Traditional data influence estimation methods, such as influence functions, assume that learning algorithms are permutation-invariant with respect to training data. However, modern training paradigms, particularly for foundation models using stochastic algorithms and non-convergent, multi-stage curricula, are sensitive to data ordering, violating this assumption. This mismatch makes influence functions inadequate for addressing questions about differentiating the influence of data at different training stages and capturing the dependence of data influence on the optimization trajectory.", "Idea": "Introduce the concept of trajectory-specific leave-one-out (LOO) influence to quantify the impact of removing a data point from a specific iteration during training, considering the sequence of data and the model's optimization trajectory. Propose data value embedding, a technique for efficiently approximating trajectory-specific LOO by computing a training data embedding that captures cumulative interactions between data and evolving model parameters, allowing for efficient approximation through a dot-product with the gradient of test data."}
{"id": "lPJUQsSIxm", "Context": "The integration of fully homomorphic encryption (FHE) with machine learning presents opportunities for secure data processing, allowing computations on encrypted data while maintaining confidentiality. However, current FHE-based implementations for deep neural networks are hindered by high computational costs, latency, and scalability issues, which restrict their practical use.", "Idea": "Introduce DCT-CryptoNets, a method that operates in the frequency-domain using the discrete cosine transform (DCT) to alleviate the computational demands of non-linear activations and homomorphic bootstrap operations during private inference, enhancing efficiency and scalability for encrypted deep learning on high-resolution images."}
{"id": "rfdblE10qm", "Context": "The Bradley-Terry (BT) model is widely used in reward modeling for aligning Large Language Models (LLMs), despite its original design for multi-player stochastic game matching. The model's application in converting pairwise response comparisons to reward values and making predictions is not well understood, especially given the sparse comparison of prompt-response pairs.", "Idea": "Introduce a theoretical foundation for the convergence rate of BT reward models using deep neural networks with embeddings, and propose an alternative order-consistent reward modeling objective using a simple upper-bound algorithm compatible with binary classifiers."}
{"id": "W2Wkp9MQsF", "Context": "Model compression is a critical task in machine learning, especially for deploying large-scale models in resource-constrained environments. Traditional compression techniques often require access to training data and fine-tuning, which can be impractical or impossible in certain scenarios. Existing data-free methods struggle to maintain model performance, particularly at high sparsity levels.", "Idea": "Introduce model folding, a data-free compression technique that merges structurally similar neurons across layers using k-means clustering, preserving data statistics and preventing variance issues without needing training data or fine-tuning."}
{"id": "sLKDbuyq99", "Context": "Multi-agent frameworks utilizing large language models have shown significant success in automated planning and task execution. However, there is a lack of research on effectively adjusting workflows during execution, which is essential for adapting to unforeseen challenges and changing conditions in real-time to ensure efficient task execution.", "Idea": "Define workflows as an activity-on-vertex (AOV) graph, enabling continuous refinement by LLM agents through dynamic subtask allocation adjustments based on historical performance and previous AOVs. Emphasize modularity in workflow design by evaluating parallelism and dependency complexity to achieve efficient concurrent execution, effective goal achievement, and enhanced error tolerance."}
{"id": "9OJflnNu6C", "Context": "Generative models have advanced significantly but pose issues like privacy breaches and biases. Machine unlearning has been proposed to address these concerns by removing specific training data from models. In Image-to-Image (I2I) generative models, previous approaches have treated unlearning as a single objective optimization problem, which does not account for diverse user expectations regarding the balance between unlearning and model utility.", "Idea": "Propose a controllable unlearning framework for I2I generative models using a control coefficient to manage the trade-off between unlearning and model utility. Reformulate the unlearning problem into a constrained optimization problem, solved with a gradient-based method to achieve Pareto optimal solutions within defined boundaries for the control coefficient."}
{"id": "zjeHLSiNv1", "Context": "Transformer models' performance is known to be logarithmically related to their number of parameters and computational complexity. Although Mixture of Experts (MoE) methods attempt to separate parameter count from computational complexity, they encounter difficulties during inference due to high memory access costs.", "Idea": "Introduce UltraMem, a large-scale, ultra-sparse memory layer designed to reduce inference latency while maintaining model performance, and explore its scaling laws to demonstrate superior scaling properties compared to MoE."}
{"id": "hgwGi81ndj", "Context": "In reinforcement learning, difficult exploration problems often arise, and there is interest in whether providing an agent with an object-centric mapping, which describes a set of items and their attributes, can lead to more efficient learning. The challenge lies in modeling items and their attributes at higher levels of abstraction, both in terms of state and temporal dynamics, to simplify the prediction of future states.", "Idea": "Propose a fully model-based algorithm that leverages hierarchical abstraction to learn a discriminative world model, enabling efficient exploration with a count-based intrinsic reward. This approach allows the agent to plan to reach any discovered abstract states, facilitating efficient task-solving, zero-shot and few-shot transfer across item types and environments, and planning across long horizons."}
{"id": "CI4sCBMXjP", "Context": "Enhancing the adaptive capabilities of large language models is a critical pursuit in both research and application. Traditional fine-tuning methods require substantial data, computational resources, and specific capabilities, while in-context learning is limited by the need for appropriate demonstrations and efficient token usage.", "Idea": "Propose ELICIT, a framework with two modules designed to store and reuse task vectors, enhancing the adaptive capabilities of models without additional training or inference tokens."}
{"id": "hJVdwBpWjt", "Context": "Large language models (LLMs) have achieved state-of-the-art performance in various auditory tasks, such as speech, music, and general audio, but their potential in bioacoustics tasks remains underexplored. These tasks, including detecting animal vocalizations, classifying rare species, and labeling context and behavior, are vital for conservation and biodiversity monitoring.", "Idea": "Introduce NatureLM-audio, the first audio-language foundation model specifically designed for bioacoustics, trained on curated text-audio pairs from bioacoustics, speech, and music to address the scarcity of annotated data in the field."}
{"id": "4GSOESJrk6", "Context": "Personalized image generation is a promising tool for assisting humans in daily tasks due to its ability to creatively generate personalized content. However, current evaluation methods either rely on automated processes that do not align well with human judgment or require human evaluations, which are time-consuming and costly.", "Idea": "Introduce DreamBench++, a human-aligned benchmark automated by advanced multimodal GPT models. This involves designing prompts for GPT to be both human-aligned and self-aligned, with task reinforcement, and constructing a comprehensive dataset of diverse images and prompts."}
{"id": "PkpNRmBZ32", "Context": "Traditional neural network architectures for audio processing tasks often rely on homogeneous configurations and specific types of operations like nonlinear recurrence, explicit convolutions, or attention mechanisms. These approaches can limit flexibility and efficiency in network design, particularly when balancing network size, performance, and computational resources.", "Idea": "Introduce Centaurus, a network architecture utilizing generalized state-space model blocks, where SSM operations are optimized as tensor contractions. This design allows for flexible configurations inspired by classical convolutional blocks, enabling a heterogeneous network that balances size and performance while enhancing training and inference efficiency without relying on traditional recurrence, convolutions, or attention mechanisms."}
{"id": "t8fu5m8R5m", "Context": "Anomaly Detection (AD) methods face challenges in robustness against adversarial attacks, which affects their reliability in critical applications like autonomous driving. This vulnerability is due to the AD setup, which typically uses only unlabeled normal samples for training, making detectors susceptible to adversarial anomalies during testing. Adversarial training is difficult to implement effectively without labeled data, as it requires an objective function that can maximize the margin between normal and anomaly distributions.", "Idea": "Propose creating a pseudo-anomaly group from normal samples and using adversarial training with contrastive loss as an objective function to enhance robustness. Address the issue of spurious negative pairs by defining opposite pairs and adversarially separating them to improve inter-group perturbations."}
{"id": "fGhr39bqZa", "Context": "Causal discovery with latent variables is a significant and complex problem. Most existing approaches depend on the assumption that latent variables have pure children, which can be limiting in practical scenarios and is not strictly necessary from a theoretical standpoint.", "Idea": "Introduce the concept of homologous surrogate to eliminate the need for pure children in causal discovery with latent variables. Formulate two assumptions involving homologous surrogates, leading to theoretical results that allow partial or full recovery of the causal graph, and develop an algorithm leveraging these properties for causal graph recovery."}
{"id": "4ua4wyAQLm", "Context": "Video anomaly detection (VAD) aims to identify novel actions or events that are not seen during training. Existing VAD techniques often focus on global patterns with redundant details, making it difficult to generalize to unseen samples.", "Idea": "Propose a framework that identifies and models the dynamics of local patterns to generalize to novel samples. This involves a two-stage process with image-text alignment and cross-modality attention, and includes a State Machine Module (SMM) for enhancing local patterns with temporal clues, complemented by temporal motion estimation."}
{"id": "xPxHQHDH2u", "Context": "Recent advancements in novel view synthesis have been driven by NeRF- and 3DGS-based methods. Despite these advancements, reconstructing reflective objects remains a challenge, as there is no existing solution that provides real-time, high-quality rendering while handling inter-reflection effectively.", "Idea": "Introduce the Reflective Gaussian splatting (Ref-Gaussian) framework, which includes physically based deferred rendering with pixel-level material properties and Gaussian-grounded inter-reflection within a Gaussian splatting paradigm. Additionally, it incorporates material-aware normal propagation and an initial per-Gaussian shading stage with 2D Gaussian primitives."}
{"id": "i3e92uSZCp", "Context": "Skill discovery methods allow agents to learn a variety of behaviors without needing explicit rewards. Achieving a semantically diverse set of skills is essential for their application in downstream tasks. Existing methods either use discriminators to create distinguishable skills or focus on increasing state coverage, but the direct pursuit of semantic diversity in skills is not well-explored.", "Idea": "Introduce Language Guided Skill Discovery (LGSD), a framework that uses large language models to enhance the semantic diversity of skills. LGSD utilizes user prompts to constrain the search space and guide agents to explore semantically diverse states, resulting in a set of semantically distinctive skills."}
{"id": "ws5phQki00", "Context": "Stance detection is crucial for enhancing online political discussions, aiding in content moderation, topic summarization, and fostering balanced debates. Traditionally, transformer-based models are used for stance detection, but they require large datasets, which are difficult to gather due to the diverse range of debate topics. Additionally, while large language models (LLMs) have revitalized stance detection, their deployment in online discussions is hindered by issues such as inconsistent outputs, biases, and susceptibility to adversarial attacks.", "Idea": "Utilize LLM-generated synthetic data to enhance stance detection in online political discussions by employing traditional stance detection models for deployment and leveraging LLMs for secure offline synthetic data generation. This involves generating synthetic data for specific debate questions using a Mistral-7B model, fine-tuning with this data to improve model performance, and further enhancing performance by identifying and using the most informative samples from an unlabelled dataset."}
{"id": "t8KLjiFNwn", "Context": "Transformer models have been extensively explored across various domains due to their ability to handle long-range dependencies and provide global contextual awareness, which has fueled the development of popular AI applications. Meanwhile, State Space Models (SSMs) have emerged as strong competitors in sequential modeling by incorporating a selective mechanism for dynamic parameter adjustment, although this increases computational complexity and bandwidth demands, posing challenges for deployment on resource-constrained mobile devices.", "Idea": "Propose a sparse learning framework that integrates architecture-aware compiler optimizations, introducing an end-to-end solution called C4^n kernel sparsity, which prunes n elements from every four contiguous weights. Develop a compiler-based acceleration solution to ensure execution efficiency for this sparsity on mobile devices, and propose C4^n-specific optimizations combined with a layout transformation elimination strategy to improve performance across operations."}
{"id": "Bp0HBaMNRl", "Context": "Discovering causal structures with latent variables from observational data is challenging due to the limitations of existing methods, which often rely on constraint-based, iterative discrete searches. These methods struggle with scalability for large variable sets and frequently assume linearity or invertibility, limiting their real-world applicability.", "Idea": "Develop a novel differentiable causal discovery algorithm that efficiently estimates the structure of non-linear latent hierarchical causal models, relaxing previous assumptions about the deterministic nature of latent variables and exogenous noise."}
{"id": "2IoFFexvuw", "Context": "Recent advancements in reinforcement learning have been successful in fine-tuning diffusion-based generative models. However, challenges persist in fine-tuning continuous flow-based generative models to align with arbitrary user-defined reward functions. These challenges include policy collapse from overoptimization and the high computational cost of likelihoods in continuous-time flows.", "Idea": "Propose an RL fine-tuning method called Online Reward-Weighted Conditional Flow Matching with Wasserstein-2 Regularization (ORW-CFM-W2), which integrates RL into the flow matching framework to fine-tune generative models with arbitrary reward functions. The method uses an online reward-weighting mechanism to prioritize high-reward regions and incorporates Wasserstein-2 distance regularization to prevent policy collapse and maintain diversity."}
{"id": "izjNI5bcOV", "Context": "The Earth's weather system is complex, involving various data modalities and tasks crucial to human life. Current data-driven models typically focus on individual weather understanding tasks, such as weather forecasting, but struggle to address multiple complex tasks within a single model. Additionally, these models are limited by their reliance on a small set of real observations for specific scenarios, which restricts their performance potential.", "Idea": "Introduce the WeatherGFM, a generalist foundation model for weather understanding tasks, which unifies the representation and definition of diverse tasks. It employs weather prompt formats to manage different data modalities and uses a visual prompting question-answering paradigm for training, enabling it to handle multiple tasks in a unified manner."}
{"id": "5IWJBStfU7", "Context": "As AI systems are increasingly used in critical applications, ensuring their interpretability is crucial. Mechanistic Interpretability (MI) seeks to reverse-engineer neural networks to extract human-understandable algorithms that explain their behavior. A fundamental question arises: for a fixed behavior, can MI guarantee a unique explanation? This is analogous to the concept of identifiability in statistics, which ensures the uniqueness of inferred parameters under specific assumptions.", "Idea": "Introduce two strategies for producing MI explanations: 'where-then-what', which identifies a network subset replicating behavior before deriving its interpretation, and 'what-then-where', which starts with candidate algorithms and searches for their implementation in the network's activation subspaces, using causal alignment concepts."}
{"id": "z8PcUSKXXN", "Context": "Recent advancements in deep image denoising have led to the development of models capable of handling various noise types. The current state-of-the-art method, Masked Training (MT), uses a masked swinir model trained on Gaussian noise, achieving good performance across different noise types. However, MT often results in over-smoothed images and presents challenges in optimizing mask ratios, complicating its integration with other methods.", "Idea": "Introduce RNINet, a novel architecture based on a streamlined encoder-decoder framework, which includes a noise injection block to enhance generalization across unseen noise types by injecting random noise into feature statistics, thereby improving efficiency and performance."}
{"id": "yitH9xAHQs", "Context": "Large language models (LLMs) have achieved impressive performance by training on diverse, high-quality task-specific data. Current methods often depend on human-annotated data or predefined task templates to guide LLMs in generating task-relevant data. This reliance on manually designed components may limit the scope of generated data, potentially missing critical edge cases or novel scenarios that could challenge the model.", "Idea": "Introduce ReverseGen, a novel approach that automatically generates training samples to expose LLM weaknesses. It employs a dedicated proposer to create queries that lead target models to produce unsatisfactory responses, which are then used to construct training data to address model shortcomings. This method is adaptable to various model scales and enhances performance across applications like safety, honesty, and math."}
{"id": "PUnD86UEK5", "Context": "Adam is known to outperform SGD in training language models, but the theoretical understanding of this advantage is limited. Previous analyses of convergence for both Adam and SGD have focused on the number of steps and have been optimal in non-convex scenarios, with both achieving a convergence rate of approximately O(T^-1/4).", "Idea": "Propose a new convergence analysis for Adam that leverages the smoothness of loss under the ℓ∞-geometry, rather than the traditional ℓ2-geometry, resulting in a better empirical smoothness constant for models like GPT-2 and ResNet. Extend this analysis to blockwise Adam with novel blockwise smoothness assumptions."}
{"id": "pPQPQ7Yd58", "Context": "In image-based control pipelines learned from behavior cloning, the geometry of the visual representation space plays a crucial role. This space acts as the information channel from the vision encoder to the action decoder. Inspired by neural collapse in image classification, there is an observed phenomenon where visual representations tend to cluster in specific ways depending on the control task, such as discrete or continuous image-based control.", "Idea": "Leverage the observed law of clustering in the visual representation space as an algorithmic tool by pretraining the vision encoder using neural collapse as a regularization. This encourages control-oriented clustering of visual features, which, when finetuned with the action decoder, enhances test-time performance significantly."}
{"id": "6qUUgw9bAZ", "Context": "Decoding procedures such as search, reranking, and self-critique are computationally intensive but can enhance the quality of language model outputs in tasks like code generation, numerical reasoning, and dialog. Traditionally, the same decoding procedure is applied uniformly to all inputs, regardless of the varying computational needs of different inputs.", "Idea": "Develop an approach that predicts the distribution of rewards based on input and computation budget, allowing for adaptive allocation of computation. This includes an adaptive best-of-k procedure for dynamic sample generation and a routing procedure that selects between expensive, accurate decoding and cheaper, less capable alternatives."}
{"id": "d8hYXbxX71", "Context": "Improving social welfare involves optimizing policy objectives across different time horizons, which is challenging because policies that seem suboptimal in the short term may have significant long-term benefits. Rawlsian policies prioritize those with the greatest need, while utilitarian policies focus on maximizing immediate welfare gains. These approaches are traditionally seen as conflicting, with Rawlsian policies thought to reduce average social welfare compared to utilitarian ones.", "Idea": "Analyze Rawlsian and utilitarian policies within a sequential decision-making framework where individuals' welfare levels stochastically decay over time, allowing policymakers to intervene. Prove that under certain conditions, Rawlsian policies can outperform utilitarian policies in the long run, despite utilitarian policies being more effective in the short term."}
{"id": "G0dksFayVq", "Context": "LLM-based judges are increasingly used as scalable alternatives to human evaluation for assessing and improving models. However, their reliability is often not thoroughly examined, especially as LLMs become more sophisticated. Existing benchmarks mainly focus on alignment with human preferences, which may not be suitable for tasks requiring factual and logical correctness.", "Idea": "Propose a novel evaluation framework and introduce JudgeBench, a benchmark designed to evaluate LLM-based judges on challenging tasks across knowledge, reasoning, math, and coding. JudgeBench uses a new pipeline to convert difficult datasets into response pairs with preference labels that reflect objective correctness."}
{"id": "vJkktqyU8B", "Context": "Current Vision Transformer (ViT) adapter methods, while achieving promising accuracy, suffer from inefficient memory access operations such as standard normalization and frequent reshaping, which hinder inference speed.", "Idea": "Propose META, a ViT adapter that enhances memory efficiency and reduces memory time consumption by minimizing inefficient memory access operations. It includes a memory-efficient adapter block that shares layer normalization between self-attention and feed-forward network layers, employs cross-shaped self-attention to reduce reshaping operations, and incorporates a lightweight convolutional branch to enhance local inductive biases. The adapter block is designed in a cascaded manner to compute diverse head features, enriching feature representation variety."}
{"id": "SiH7DwNKZZ", "Context": "Transformers have become a standard backbone in computer vision, although they were originally designed for natural language processing. The Long Short-Term Memory (LSTM) model has been recently enhanced into a scalable architecture known as xLSTM, which addresses traditional LSTM limitations through exponential gating and a parallelizable matrix memory structure.", "Idea": "Introduce Vision-LSTM (ViL), an adaptation of xLSTM for computer vision, consisting of a stack of xLSTM blocks where odd blocks process patch tokens from top to bottom and even blocks process them from bottom to top."}
{"id": "9NfHbWKqMF", "Context": "3D Gaussian Splatting (3DGS) has advanced photorealistic reconstruction, offering high visual fidelity and real-time performance. However, its rendering quality declines when test views differ from the training camera angles, posing challenges for immersive free-viewpoint rendering and navigation. Existing methods, even with regularization techniques and data-driven priors, struggle to generalize to out-of-distribution (OOD) views.", "Idea": "Introduce SplatFormer, a point transformer model designed to refine 3DGS sets optimized under limited training views in a single forward pass, effectively removing artifacts in OOD test views and improving rendering quality under extreme novel views."}
{"id": "84WmbzikPP", "Context": "Molecular structure elucidation is crucial for understanding chemical phenomena and has applications in various fields such as natural products, lab syntheses, forensic samples, and the interstellar medium. The task involves predicting a molecule's 3D structure from its molecular formula and moments of inertia, which can be measured with high precision using rotational spectroscopy. Existing generative models can sample 3D structures with approximately correct moments, but they do not fully utilize the precision available from experimental data.", "Idea": "Propose Stiefel Flow Matching as a generative model to predict 3D molecular structures under exact moment constraints by embedding the space of $n$-atom point clouds in the Stiefel manifold. This approach involves learning simpler and shorter flows through approximate solutions for equivariant optimal transport on the Stiefel manifold, enhancing success rates and sampling speed compared to traditional models."}
{"id": "9FqARW7dwB", "Context": "Residual connections are commonly used in neural networks to address issues like gradient vanishing and representation collapse. However, these connections often suffer from drawbacks such as the seesaw effect, which can hinder the performance of deep learning models.", "Idea": "Introduce hyper-connections, a method that allows networks to adjust connection strengths between features at different depths and dynamically rearrange layers, serving as an alternative to residual connections."}
{"id": "ho4mNiwr2n", "Context": "Anti-backdoor learning is a defense method aimed at training clean models from poisoned datasets, which is crucial for countering backdoor attacks. Existing methods struggle to restore backdoored samples to their original labels and do not generalize well to large pre-trained models due to non end-to-end training, making them inadequate for protecting large pre-trained models.", "Idea": "Introduce an end-to-end method called Mind Control through Causal Inference (MCCI) that uses both images and attack indicators to train clean models from poisoned datasets. This approach allows the model to control its perception of inputs as clean or backdoored, enabling correct predictions even for poisoned samples by using fake non-attack indicators."}
{"id": "WwmtcGr4lP", "Context": "Effective cancer treatment is challenging due to the individualized nature of patient responses, which are influenced by the heterogeneity of cancer-causing mutations across patient genomes. The limited availability of patient response data complicates the training of personalized treatment models. Existing methods use transfer learning with larger, labeled pre-clinical datasets to create a shared representation between cell line and patient domains, but they fail to capture patient-specific characteristics that significantly affect drug response.", "Idea": "Introduce GANDALF, a novel generative attention-based data augmentation and predictive modeling framework that directly augments patient genomic data while accounting for domain-specific characteristics."}
{"id": "d8cnezVcaW", "Context": "Direct Preference Optimization (DPO) is a method used to enhance reinforcement learning from human feedback, particularly for fine-tuning large language models. However, DPO struggles to effectively capture the diversity of human preferences, which limits its effectiveness.", "Idea": "Develop a new approach called MallowsPO, inspired by Mallows' theory of preference ranking, which introduces a dispersion index to reflect the diversity of human preferences. This approach unifies existing DPO models as special cases and enhances DPO performance across various tasks by utilizing the dispersion index."}
{"id": "MGKDBuyv4p", "Context": "Language models have the capability to memorize information, which can lead to the verbatim regurgitation of training data during inference. This poses a problem when the data is private or sensitive, necessitating methods to mitigate such memorization.", "Idea": "Investigate and introduce various methods to mitigate memorization in language models, including three regularizer-based, three fine-tuning-based, and eleven machine unlearning-based methods, with five new unlearning methods. Additionally, introduce TinyMem, a suite of small, computationally-efficient language models for developing and evaluating these memorization-mitigation methods."}
{"id": "vmulbBDCan", "Context": "Electron-multiplying charge-coupled devices (EMCCDs) are crucial for sensitive observations in low-light conditions across various fields such as astronomy, material science, and biology. Despite their advanced designs to enhance target signals and reduce read-out circuit noise, EMCCD images still suffer from noise, affecting outcomes in applications like fluorescence microscopy. Existing noise models for EMCCDs focus on theoretical statistical characteristics and have not integrated recent advancements in computational photography, which use physics-based noise models to guide deep learning for adaptive denoising in ordinary image sensors.", "Idea": "Introduce a systematic study on physics-based noise model calibration for EMCCD cameras, accurately estimating statistical features of noise components to generate authentic training samples for a recent neural network. Capture a real-world test image dataset for EMCCD, including both ordinary and microscopic scenes, to benchmark and demonstrate the advantages of the proposed model over previous methods."}
{"id": "iXCeQ2m6vT", "Context": "AI systems struggle with understanding visual relations, particularly when dealing with previously unseen objects, whereas humans excel at this task. Active vision theories suggest that learning visual relations is linked to eye movements that help fixate on objects and their parts, with spatial information from these movements aiding in representing relations between image parts.", "Idea": "Develop a system using a novel Glimpse-based Active Perception (GAP) that sequentially focuses on the most salient regions of an image, processing them at high resolution. This system uses the locations from these glimpsing actions, along with surrounding visual content, to represent relations between different image parts."}
{"id": "FoF5RaA3ug", "Context": "Recent advancements in dataset distillation have shown the advantages of using soft labels generated by pre-trained teacher models. However, the choice of loss function for utilizing these soft labels significantly affects the performance of models trained on synthetic datasets, indicating the need for a universal loss function.", "Idea": "Introduce GIFT, a simple and effective plug-and-play approach that includes soft label refinement and a cosine similarity-based loss function to fully utilize label information in dataset distillation."}
{"id": "R4h5PXzUuU", "Context": "Foundation models, trained on large-scale internet data, have shown impressive generalization capabilities and are increasingly used across various application domains. However, the trustworthiness of these models, particularly their ability to detect out-of-distribution data, remains insufficiently explored. This gap raises concerns about the safe deployment of large vision-language models, which are trained on extensive multi-modal datasets.", "Idea": "Introduce a self-guided prompting approach called Reflexive Guidance (ReGuide) to improve the out-of-distribution detection capabilities of large vision-language models by using self-generated image-adaptive concept suggestions."}
{"id": "gU4ZgQNsOC", "Context": "Pretraining large language models on extensive and varied datasets is essential for achieving top performance in various tasks. However, current training methods treat all data samples equally, ignoring the individual importance or relevance of each sample during training. Existing reweighting strategies focus on group-level data importance and fail to utilize fine-grained, instance-level information, lacking dynamic adaptation to sample importance as training progresses.", "Idea": "Introduce novel algorithms for dynamic, instance-level data reweighting that adjust the weight of each training sample based on its loss value in an online manner, allowing the model to focus on more informative samples. Develop a theoretical framework to analyze the impact of loss-based reweighting on the convergence of gradient-based optimization, providing a formal characterization of its effect on convergence bounds."}
{"id": "f7KxfUrRSb", "Context": "Aligning language models with human preferences is a significant research focus, aiming to enhance the models' ability to meet diverse user needs. The concept of weak-to-strong generalization, where a strong language model fine-tuned on labels from a weaker model can outperform its supervisor, serves as an inspiration for improving model alignment.", "Idea": "Propose a method called Weak-to-Strong Preference Optimization (WSPO) that achieves strong model alignment by learning the distribution differences before and after the alignment of the weak model, effectively transferring and amplifying alignment behavior from weaker to stronger models."}
{"id": "oU3tpaR8fm", "Context": "Retrieval-augmented generation (RAG) allows large language models (LLMs) to leverage external knowledge sources, with the potential to improve output quality by processing longer input sequences. It is assumed that a larger retrieval set would enhance performance by providing more relevant information. However, empirical findings show that for many long-context LLMs, the quality of generated output initially improves but then declines as the number of retrieved passages increases, due to the presence of 'hard negatives'.", "Idea": "Propose both training-free and training-based approaches to enhance the robustness of long-context LLM-based RAG. This includes retrieval reordering as a training-free optimization and RAG-specific implicit LLM fine-tuning and RAG-oriented fine-tuning with intermediate reasoning as training-based methods, along with a systematic analysis of design choices for these methods."}
{"id": "iylpeTI0Ql", "Context": "Test-time adaptation (TTA) is designed to handle distribution shifts between source and target data by using only target data during testing. In open-world scenarios, models frequently encounter noisy samples that fall outside the in-distribution label space, which poses a challenge for existing TTA methods as they experience significant performance declines under these conditions.", "Idea": "Introduce Zero-Shot Noisy TTA (ZS-NTTA), a framework that decouples the classifier and detector, focusing on developing an individual detector while keeping the classifier frozen. The Adaptive Noise Detector (AdaND) is proposed, which uses the frozen model's outputs as pseudo-labels to train a noise detector, effectively identifying noisy samples and improving zero-shot out-of-distribution detection."}
{"id": "4rEI2JdHH6", "Context": "Grokking is a phenomenon in neural networks where models initially memorize training data and generalize poorly, but eventually transition to near-perfect generalization after extended training. This delayed generalization is problematic as it affects predictability and efficiency, with the ideal scenario being immediate generalization without delay.", "Idea": "Propose GrokTransfer, a method to accelerate grokking by first training a smaller model to a suboptimal performance level, then using its learned input embedding to initialize a stronger model, thereby enabling direct generalization without delay."}
{"id": "vQhn4wrQ6j", "Context": "Fine-tuning Large Language Models (LLMs) for specific tasks in non-English languages is challenging due to the lack of task-specific data. This is particularly problematic for tasks like mathematical reasoning, where in-language math data is often unavailable.", "Idea": "Develop a model merging methodology that enhances cross-lingual transfer by composing language and math capabilities. This involves fine-tuning separate 'experts' on math instruction data in English and on generic instruction data in the target language, then swapping the top and bottom transformer layers between these experts to improve math performance in the target language."}
{"id": "uREg3OHjLL", "Context": "The expressive power of ReLU neural networks is believed to increase with their depth. A specific function, $F_n = \\max (0,x_1,\\ldots,x_n )$, has been used to explore this relationship. A conjecture suggests that any ReLU network exactly representing $F_n$ requires at least $\\lceil \\log_2 (n+1) \\rceil$ hidden layers, a claim recently confirmed for networks with integer weights.", "Idea": "Investigate ReLU networks with decimal fraction weights, demonstrating that $F_n$ requires at least $\\lceil \\log_3 (n+1) \\rceil$ hidden layers. Additionally, for networks with $N$-ary fraction weights, establish a lower bound of $\\Omega( \\frac{\\ln n}{\\ln \\ln N})$ layers, partially confirming the conjecture for rational ReLU networks."}
{"id": "vr1QdCNJmN", "Context": "The Bregman divergence is a pseudo-distance commonly used for comparing vectors or functions in continuous spaces, generated from a convex function. However, defining a similar divergence for discrete spaces is challenging. Previous work by Iyer & Bilmes explored Bregman divergences on discrete domains using submodular functions, which are discrete analogs of convex functions.", "Idea": "Generalize the framework to include generating functions that are neither submodular nor supermodular, creating the difference-of-submodular Bregman divergence. Introduce a learnable form of this divergence using permutation-invariant neural networks to capture structural properties in discrete data."}
{"id": "2e4ECh0ikn", "Context": "Recent advancements in audio foundation models (FMs) have the potential to enhance conversational modeling. However, there is a lack of comprehensive evaluation of these models' abilities to engage in natural and interactive conversations, particularly in managing turn-taking without excessive overlapping speech or prolonged silences.", "Idea": "Propose a novel evaluation protocol to assess spoken dialog systems' turn-taking capabilities using a supervised model trained to predict turn-taking events in human-human conversations. This protocol is used to conduct a comprehensive user study and evaluate multiple audio FMs on curated test benchmarks, with plans to open source the evaluation platform to foster the development of advanced conversational AI systems."}
{"id": "QG31By6S6w", "Context": "Recent advancements in medical vision-language pre-training models have significantly improved zero-shot disease recognition. However, transferring image-level knowledge to pixel-level tasks like lesion segmentation in 3D CT scans is challenging due to the complexity and variability of pathological visual characteristics. Existing methods struggle to align fine-grained lesion features not encountered during training with disease-related textual representations.", "Idea": "Introduce Malenia, a multi-scale lesion-level mask-attribute alignment framework for 3D zero-shot lesion segmentation, enhancing compatibility between mask representations and elemental attributes. It includes a Cross-Modal Knowledge Injection module to enrich visual and textual features with mutually beneficial information, guiding segmentation result generation."}
{"id": "X9OfMNNepI", "Context": "Scientific discovery is crucial for societal advancement, and there is growing interest in using large language models (LLMs) to accelerate this process. However, it remains uncertain if LLMs can autonomously generate novel and valid hypotheses in the field of chemistry. The challenge lies in determining whether LLMs can effectively discover new hypotheses based on a given research question.", "Idea": "Develop a multi-agent framework using LLMs to explore the potential of automatically generating chemistry hypotheses. This framework breaks down the task into three stages: retrieving inspirations from a background question, forming hypotheses from these inspirations, and ranking the hypotheses based on their quality."}
{"id": "keu6sxrPWn", "Context": "As large language models (LLMs) become more powerful, they also become harder to trust due to potential 'subversive misalignment,' where subtle errors bypass safety checks. This creates a challenge in balancing safety and the capabilities of untrusted models during deployment, as individual errors can increase the risk of safety failures.", "Idea": "Introduce the 'Diffuse Risk Management' problem, which aims to balance safety and usefulness in deploying untrusted models across multiple tasks. This is achieved through a two-level framework: micro-protocols at the single-task level using a trusted model to monitor the untrusted model, and a macro-protocol at the whole-scenario level that adaptively estimates risk to select between micro-protocols."}
{"id": "2ZK8zyIt7o", "Context": "The rapid advancement of text-to-image diffusion models has enabled them to generate unprecedented results from given texts. However, as text inputs become longer, existing encoding methods like CLIP face limitations, and aligning the generated images with long texts becomes challenging.", "Idea": "Propose LongAlign, which includes a segment-level encoding method for processing long texts and a decomposed preference optimization method for effective alignment training. The segment-level encoding divides long texts into multiple segments for separate processing, overcoming input length limits. The decomposed preference optimization involves a reweighting strategy to reduce overfitting by assigning different weights to text-relevant and text-irrelevant components of preference scores."}
{"id": "RaR3ETzyKp", "Context": "Recent studies have shown that different diffusion methods and architectures trained on the same dataset yield similar results when given the same input noise. This suggests the existence of preferable noises for specific samples. By visualizing noise-sample pairs in two-dimensional spaces, it is observed that preferable paths, which connect these noises to samples, are more organized and have fewer crossings compared to random paths. In high-dimensional spaces, paths rarely intersect, and the crossings in two-dimensional spaces indicate shorter inter-path distances in high-dimensional spaces.", "Idea": "Propose the Distance-Aware Noise-Sample Matching (DANSM) method to increase inter-path distance, thereby accelerating model training. DANSM is based on rectified flow models, utilizing a closed-form formula to calculate inter-path distance, and simplifies optimization by relating inter-path distance to path length, using the latter as an optimization surrogate."}
{"id": "e8qXTxMgPg", "Context": "The study focuses on dimensionality reduction for $s$-sparse vectors, which are vectors with at most $s$ non-zero coordinates. Traditional approaches provide average-case guarantees for embedding these vectors, with known upper bounds based on the birthday-paradox. These bounds suggest that a linear map can preserve the norm of most vectors in a collection, but achieving this requires mapping to a high-dimensional space. The challenge lies in finding efficient embeddings that maintain vector norms while reducing dimensionality.", "Idea": "The research introduces novel lower bounds for linear maps that show optimal dimensionality requirements for average-case guarantees. It also explores embeddings for sparse non-negative vectors, achieving improved upper bounds by non-linearly embedding these vectors into fewer dimensions while preserving pairwise distances. This approach leverages the non-negativity assumption to enable smaller embeddings and provides exact dimensionality reduction for certain norms, highlighting the necessity of non-linearity and non-negativity for these improvements."}
{"id": "Wvi8c0tgvt", "Context": "Existing blur datasets lack sufficient variety in scenes and blur patterns, making them inadequate for training. Expanding data diversity is challenging due to the complexity and effort required by dual-camera systems. Current data augmentation methods focus on 2D motion estimation, ignoring the 3D nature of camera and object movements, leading to unrealistic motion patterns.", "Idea": "Propose a 3D-aware blur synthesizer that generates diverse and realistic blur images for data augmentation by estimating 3D camera positions and synthesizing blur images through 3D transformations, without needing explicit depth measurements. This approach allows for controllable augmentation by adjusting blur magnitude, direction, and scenes."}
{"id": "c4OGMNyzPT", "Context": "Large Vision Language Models (LVLMs) have shown significant capabilities in processing both visual and textual data. However, current evaluation methods, such as those based on Visual Question Answering and image captioning benchmarks, are insufficient in fully assessing the capabilities of LVLMs. These methods are limited by issues like inadequate evaluation of detailed visual perception, data contamination, and insufficient focus on multi-turn reasoning.", "Idea": "Introduce LVLM-Playground, a game-based evaluation framework that comprehensively assesses LVLMs' cognitive and reasoning skills in structured environments. This framework evaluates LVLMs on tasks such as Perceiving, Question Answering, Rule Following, and End-to-End Playing, each designed to test specific abilities like visual perception and decision-making."}
{"id": "bc3sUsS6ck", "Context": "Large language models (LLMs) acquire substantial knowledge during pretraining but often require adaptation to new contexts, tasks, or domains. This adaptation is typically achieved through fine-tuning, which incurs significant training costs, or prompting, which increases inference overhead.", "Idea": "Introduce GenerativeAdapter, an adaptation method that encodes test-time context into language model parameters with a single forward pass. It augments a frozen pretrained LM with a lightweight adapter generator, trained via self-supervised learning, to produce parameter-efficient adapters. The generator is general-purpose, capable of adapting the corresponding base model for all language processing scenarios."}
{"id": "rpouyo09V0", "Context": "Large language models (LLMs) are crucial for code generation, especially in interactive settings. However, current benchmarks do not adequately reflect the diverse feedback encountered in multi-turn interactions, hindering the evaluation of LLMs in these scenarios.", "Idea": "Introduce novel benchmarks that model feedback quality for code generation LLMs, including CONVCODEWORLD, an environment simulating interactive scenarios with various feedback types, and CONVCODEBENCH, a static version using pre-generated feedback logs to maintain evaluation efficiency."}
{"id": "0mtz0pet1z", "Context": "In preventive medicine and non-fatal health conditions, such as HIV infection without AIDS, the timing of treatment initiation is crucial. Traditional causal inference has focused on determining the optimal timing for treatment and its effects, often considering the characteristics of the subjects involved.", "Idea": "Propose a method to identify and estimate the incremental causal effect of intervening on the timing of treatment initiation, without relying on the common positivity assumption, using an estimation framework based on inverse probability weighting."}
{"id": "u3TL0qxLWf", "Context": "Large Language Models (LLMs) have significantly advanced natural language processing but are hindered by high runtime costs, making widespread deployment challenging. These models require substantial memory access and computational resources, which limits their efficiency and scalability.", "Idea": "Introduce SeedLM, a post-training compression method that encodes model weights using seeds of a pseudo-random generator. This method employs a Linear Feedback Shift Register (LFSR) to generate random matrices during inference, which are combined with compressed coefficients to reconstruct weight blocks, reducing memory access and utilizing idle compute cycles without relying on calibration data."}
{"id": "4O0v4s3IzY", "Context": "There is ongoing debate about the reasoning capabilities of Large Language Models (LLMs). Initial hopes that reasoning would naturally emerge with increased scale have been challenged by various counterexamples. Despite this, many believe LLMs can iteratively self-improve their solutions, based on the assumption that verifying correctness is easier than generating solutions, a notion rooted in computational complexity theory.", "Idea": "Conduct a systematic investigation into the effectiveness of iterative prompting for reasoning and planning, using GPT-4 across three domains: Game of 24, Graph Coloring, and STRIPS planning. The study examines both self-critique by the model and verification by an external reasoner, analyzing the impact of criticisms on performance and the effects of removing elements from the augmented system."}
{"id": "P4XmKjXTrM", "Context": "Reproducibility is a major challenge in machine learning for healthcare due to the private nature of datasets, model pipelines, and task or cohort definitions. This creates barriers in sharing, iterating, and understanding ML results on electronic health record datasets.", "Idea": "Introduce the Automatic Cohort Extraction System (ACES) for event-stream data, which simplifies the development and reproduction of tasks and cohorts in healthcare ML. ACES offers a domain-specific configuration language for defining dataset-specific and agnostic criteria, and a pipeline for extracting patient records that meet these criteria from real-world data."}
{"id": "SgymXhOEA5", "Context": "Person re-identification (ReID) models often suffer from camera bias, which becomes more pronounced when there are data distribution shifts. Previous methods to address camera bias have been limited to the training domains of the models. Additionally, unsupervised ReID models exhibit significant bias towards camera labels, even within seen domain data, indicating a need for improvement.", "Idea": "Revisit feature normalization on embedding vectors as a debiasing method for unseen domain data, analyzing its effectiveness in reducing bias related to low-level image properties and body angle. Propose simple training strategies to mitigate camera bias in unsupervised learning, enhancing existing algorithms with minor modifications."}
