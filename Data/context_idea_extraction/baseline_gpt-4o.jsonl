{"id": "nDvgHIBRxQ", "Context": "The evaluation of mathematical reasoning abilities in large language models (LLMs) is crucial, yet current benchmarks mainly focus on problem-solving, risking model overfitting and failing to accurately measure true reasoning capabilities. There is a need for a comprehensive evaluation method that reflects real-world user experiences and assesses the robustness and generalization of these models across diverse tasks.", "Idea": "Introduce MathCheck, a checklist designed to test task generalization and reasoning robustness, along with an automatic tool for efficient checklist generation. MathCheck includes various mathematical reasoning tasks and robustness tests, and is used to develop MathCheck-GSM and MathCheck-GEO for evaluating mathematical textual and multi-modal reasoning capabilities, respectively, offering an improved assessment over existing benchmarks."}
{"id": "ZsP3YbYeE9", "Context": "A common method for developing agents with Language Models involves iterative prompting, reflecting on outputs, and updating prompts until the task is completed. However, this approach faces challenges such as limited exploration of decision space due to repetitive reflections and an inability to utilize insights from previously solved tasks.", "Idea": "Introduce DoT (Diversity of Thoughts), a framework that reduces redundant reflections to improve decision-space exploration and incorporates a task-agnostic memory component for knowledge retrieval from previously solved tasks, enhancing the agent's problem-solving capabilities."}
{"id": "I4e82CIDxv", "Context": "Previous methods for interpreting language model behaviors relied on identifying circuits composed of polysemantic and difficult-to-interpret units such as attention heads or neurons. These circuits were not suitable for many downstream applications due to their complexity and lack of human interpretability.", "Idea": "Introduce sparse feature circuits, which are causally implicated subnetworks of human-interpretable features, to explain language model behaviors. These circuits allow for detailed understanding of neural network mechanisms and are useful for downstream tasks. Additionally, propose SHIFT, a method to improve classifier generalization by ablating task-irrelevant features, and demonstrate an unsupervised interpretability pipeline for discovering sparse feature circuits."}
{"id": "pHe4P1IVnb", "Context": "As large language models become more complex, the challenge arises in adapting alignment techniques, especially when human supervision is limited. In scenarios where weak supervision is used, it becomes crucial to effectively leverage the capabilities of stronger models.", "Idea": "Extend the Weak-to-Strong framework to WeakS-to-Strong by using an ensemble of weak models to simulate human opinion variability, employing a Bayesian approach for confidence scoring. This framework is further applied to text generation tasks, incorporating advanced supervision strategies and direct preference optimization to enhance the student model's learning."}
{"id": "Pj4Aid3XqL", "Context": "Pre-trained large language models (LLMs) that are further trained with image data show strong performance on vision-language tasks. However, there is uncertainty about the effectiveness of this two-step training process compared to vision-language models (VLMs) that integrate images earlier in the training phase.", "Idea": "Investigate the impact of introducing visual tokens at different stages of pre-training by training models across various datasets, scales, image-text ratios, and pre-training extents. Fine-tune these models to evaluate their performance on both vision-language and text-only tasks, aiming to determine the optimal point for integrating visual data to enhance task performance."}
{"id": "B2Fqu7Y2cd", "Context": "Large language models trained on text can infer instructions directly from data, but models trained solely on audio data lack this capability because audio data does not inherently contain the instructions used to generate it. This presents a challenge in developing audio synthesis models that can follow free-form text instructions and achieve compositional abilities such as combining or interpolating instructions.", "Idea": "Introduce a specialized dataset generation approach optimized for audio generation and transformation tasks, revealing meaningful relationships between audio and language. Additionally, propose ComposableART, an inference-time technique that extends classifier-free guidance to compositional guidance, enabling flexible composition of instructions for customizable audio outputs."}
{"id": "MWHIIWrWWu", "Context": "Controlling high-dimensional nonlinear systems, such as those in biological and robotic applications, is difficult due to the large state and action spaces. Although deep reinforcement learning has been successful in these areas, it is computationally demanding and time-consuming, making it unsuitable for handling large sets of tasks that require extensive manual tuning.", "Idea": "Introduce MPC$^2$, a hierarchical model-based learning algorithm that combines a sampling-based model predictive controller for target posture planning with a morphology-aware proportional controller for actuator coordination, enabling zero-shot and near-real-time control of high-dimensional complex dynamical systems."}
{"id": "cmYScmfu4Q", "Context": "Reward inference is a crucial step in the Reinforcement Learning from Human Feedback (RLHF) process for fine-tuning Large Language Models (LLMs). However, RLHF encounters challenges like distribution shift, reward model overfitting, and problem misspecification. Direct Preference Optimization (DPO) offers an alternative by simplifying the pipeline, but it is limited to specific settings such as bandits or deterministic Markov Decision Processes (MDPs).", "Idea": "Develop two RLHF algorithms that bypass reward inference for broader RL problems, using human preferences to estimate local value function differences and approximate policy gradients with a zeroth-order gradient approximator, achieving polynomial convergence rates."}
{"id": "6HcnC3pPkp", "Context": "The rapid advancement of test-time compute search strategies has highlighted the need for robust verifiers to enhance the mathematical problem-solving capabilities of large language models (LLMs). Current inference strategies depend on verifiers designed for Best-of-N search, which are not optimal for tree search techniques. These existing verifiers provide indirect assessments of partial solutions and may undervalue intermediate steps, leading to premature pruning of promising paths.", "Idea": "Introduce token-supervised value models (TVMs), a new class of verifiers that assign probabilities to each token, indicating the likelihood of reaching the correct final answer. This token-level supervision allows TVMs to directly evaluate partial solutions, effectively distinguishing between promising and incorrect intermediate steps during tree search."}
{"id": "BAelAyADqn", "Context": "Longitudinal human behavior modeling is crucial for applications like patient monitoring and lifestyle recommendations, using data from devices such as smartphones and smartwatches. This field aims to develop predictive models for health outcomes based on individual behavior data. However, existing models often lack accuracy and fail to consider realistic data aspects like diverse feature types and high missing value rates, as well as resource consumption issues.", "Idea": "Introduce MuHBoost, a multi-label boosting method that leverages large language model prompting and multi-label classification to predict multiple health outcomes. To address the issue of LLMs hallucinating when handling multiple questions, two variants of MuHBoost are developed to enhance predictive performance."}
{"id": "svp1EBA6hA", "Context": "Diffusion models are advanced generative models that allow for precise control over generated samples. These models, when trained on large datasets, often require additional controls during downstream fine-tuning processes, treating them as pre-trained diffusion models.", "Idea": "Introduce a novel method called CTRL, which uses reinforcement learning to add controls to pre-trained diffusion models. This method formulates the task as an RL problem, using a classifier learned from an offline dataset and the KL divergence against pre-trained models as reward functions, enabling sampling from the conditional distribution with additional controls during inference."}
{"id": "l2zFn6TIQi", "Context": "The growing capabilities and deployment of large generative models have led to concerns regarding their reliability, safety, and potential misuse. Recent efforts have focused on controlling model generation by steering model activations to manage the emergence of certain concepts or behaviors in the output.", "Idea": "Introduce Activation Transport (AcT), a framework that uses optimal transport theory to steer model activations, offering fine-grained control over model behavior with minimal computational cost and impact on model performance."}
{"id": "FpiCLJrSW8", "Context": "The trustworthiness of Large Language Models (LLMs) is a critical factor that encompasses the reliability, safety, and ethical alignment of their outputs. Reinforcement Learning From Human Feedback (RLHF) is commonly used to align LLMs with human preferences, but its impact on trustworthiness has not been thoroughly evaluated. This study examines the performance of models aligned with general-purpose preference data across various trustworthiness aspects, revealing that RLHF does not inherently ensure trustworthiness and may have adverse effects.", "Idea": "Propose the adaptation of efficient influence function-based data attribution methods to the RLHF setting to better understand the impact of fine-tuning data on specific trustworthiness benchmarks, demonstrating feasibility through estimated attribution scores."}
{"id": "5WEpbilssv", "Context": "High-content perturbation experiments offer detailed insights into biomolecular systems but are hindered by high costs and complex analysis. Current machine learning approaches fail to capture the semantic depth of biological data and are not well-aligned with biological analysis goals.", "Idea": "Introduce PerturbQA, a benchmark designed for structured reasoning over perturbation experiments, focusing on predicting differential expression and gene set enrichment for unseen perturbations. Additionally, propose Summer, a domain-informed LLM framework that effectively addresses these challenges."}
{"id": "9OfKxKoYNw", "Context": "Recent advancements in diffusion models have enabled text-guided image manipulation, allowing users to create realistic edited images with simple textual prompts. However, there is a significant concern about the misuse of these methods, particularly in generating misleading or harmful content. Existing defense strategies, which use imperceptible adversarial noise to cause model failure, are inadequate against sophisticated manipulations like editing with a mask.", "Idea": "Propose DiffusionGuard, a defense method against unauthorized edits by diffusion-based image editing models, which introduces a novel objective to generate adversarial noise targeting the early stage of the diffusion process. This method also includes a mask-augmentation technique to enhance robustness against various masks during test time."}
{"id": "oZkqkkvdND", "Context": "Variational Autoencoders (VAEs) are increasingly used in safety-critical applications where it is essential to provide certified probabilistic guarantees on performance, especially under adversarial attacks.", "Idea": "Propose a novel method called CIVET for certified training of VAEs, which involves bounding the worst-case VAE error by focusing on carefully chosen support sets at the latent layer, and introduce a new training algorithm based on this insight."}
{"id": "Pujt3ADZgI", "Context": "Reinforcement Learning with Human Feedback (RLHF) has been successful in aligning large language models with human preferences. Current RLHF methods are primarily reward-based and rely on the Bradley-Terry model, which may not fully capture the complexity of human preferences.", "Idea": "Formulate RLHF as a two-player game and propose an online algorithm called iterative Nash policy optimization (INPO), which uses no-regret learning to approximate the Nash policy. This approach introduces a new loss objective that is minimized over a preference dataset, avoiding the need for estimating expected win rates for individual responses."}
{"id": "9HK2rHNAhd", "Context": "Optimizing the Key-Value (KV) cache in Large Language Models (LLMs) is crucial for reducing inference costs. Existing KV-cache compression methods often focus on token sparsification based on token importance but allocate the same KV budget across all layers, which is inefficient as different layers have varying sensitivity to input tokens.", "Idea": "Propose a system that optimizes KV-cache allocation by identifying the importance of attention layers, allowing for joint optimization across sequence-wise and layer-wise dimensions. This involves measuring each layer's importance through cosine similarity of input prompt differences and adjusting KV budgets accordingly, incorporating sequence-wise algorithms for precise layer-specific compression."}
{"id": "Mfnh1Sqdwf", "Context": "Predicting gene expressions from DNA sequences is challenging due to the difficulty in identifying regulatory elements that control gene expressions. Understanding the causal relationship between epigenomic signals, DNA sequences, and their regulatory elements is crucial for accurate prediction.", "Idea": "Introduce Seq2Exp, a Sequence to Expression network designed to discover and extract regulatory elements that drive gene expression. The approach involves decomposing epigenomic signals and DNA sequences based on causal active regulatory elements and applying an information bottleneck with the Beta distribution to filter out non-causal components."}
{"id": "pbre0HKsfE", "Context": "Large language models (LLMs) are capable of providing personalized responses based on user interactions, but this capability raises significant privacy concerns. Homomorphic encryption (HE) offers a cryptographic solution for privacy-preserving machine learning by allowing arithmetic computations on encrypted data. However, the computational demands of transformers make it challenging to apply HE to LLMs effectively.", "Idea": "Propose a modified transformer architecture that is compatible with homomorphic encryption, focusing on inference after personalized fine-tuning. This approach uses LoRA fine-tuning and Gaussian kernels to achieve substantial computational speedups while maintaining performance comparable to models operating on plaintext data."}
{"id": "WeJEidTzff", "Context": "Commuting Origin-Destination (OD) flows are essential for urban planning and transportation, providing insights into the movement of populations between residential and work areas. Due to the high costs associated with data collection, researchers have developed models to generate these flows using available urban attributes like sociodemographics and points of interest. However, the diversity in modeling techniques and evaluation metrics has made it difficult to establish a unified standard for comparing model performance.", "Idea": "Introduce a large-scale dataset containing commuting OD flows for 3,333 areas across various urban environments in the United States, and benchmark widely used models for commuting OD flow generation to identify that network-based generative models achieve optimal performance in precision and generalization."}
{"id": "kiOxNsrpQy", "Context": "Graph Neural Networks (GNNs) are increasingly used, necessitating reliable tools to explain their predictions. A key requirement for these explanations is faithfulness, meaning they accurately reflect the GNN's reasoning process. However, multiple faithfulness metrics exist, leading to confusion about what constitutes faithfulness and how to achieve it.", "Idea": "Demonstrate that existing faithfulness metrics are not interchangeable and can overlook important explanation properties. Prove that optimizing for faithfulness is not always beneficial, especially for injective regular GNN architectures, and explore the relationship between architectural choices and faithfulness, highlighting its connection to out-of-distribution generalization."}
{"id": "Kpjvm2mB0K", "Context": "The study focuses on one-pass streaming algorithms for underdetermined ℓp linear regression problems, where the matrix A has significantly more columns than rows. This problem generalizes basis pursuit and least squares solutions for underdetermined systems. The column-arrival streaming model is considered, where columns of A are presented sequentially, relevant to graph-based problems like transshipment, electrical flows, and max flow. The challenge is to design algorithms that use significantly less space than the entire data stream.", "Idea": "Develop a streaming algorithm that constructs a sparse instance with a reduced number of columns to approximate the cost of the ℓp regression problem within a (1±ε) factor, using sublinear space. For p > 1, provide sublinear space upper bounds for outputting a solution, achieving a κ-approximation with space poly(n) · Õ(d/κ^q), and for p = 1, achieve a √n-approximation using poly(n, log d) space."}
{"id": "eENHKMTOfW", "Context": "The emergence of large language models (LLMs) has led to a significant gap between industrial research labs, which have the resources to fine-tune these models, and individual developers or small organizations, which lack the necessary resources to explore the experiment space effectively. This disparity is due to the high computational demands and expertise required for fine-tuning LLMs.", "Idea": "Conduct a comprehensive study on supervised fine-tuning of small-sized LLMs (3B to 7B parameters) using instruction-tuning datasets across diverse domains. The study explores various training configurations and strategies on four open-source pre-trained models, providing detailed documentation and challenging common training practices to offer guidance for cost-efficient and accessible fine-tuning."}
{"id": "TEkoMEjf7E", "Context": "Generative 3D modeling has recently advanced but faces challenges due to its ill-posed nature, affecting quality and controllability. Designers often refer to existing 3D models when creating new ones, highlighting a need for improved methods in 3D generation.", "Idea": "Introduce Phidias, a novel generative model that uses diffusion for reference-augmented 3D generation, leveraging a retrieved or user-provided 3D reference model to guide the process. It integrates meta-ControlNet for dynamic conditioning, dynamic reference routing to address misalignment, and self-reference augmentations for self-supervised training."}
{"id": "EV7FMBZxnx", "Context": "Detecting concealed objects, such as in vivo lesions or camouflage, requires specialized imaging systems. Lensless cameras, which are compact and flexible, present a promising alternative to traditional bulky lens systems. However, the lack of lenses results in measurements that lack visual semantics, creating significant challenges for concealed object detection.", "Idea": "Propose a region gaze-amplification network (RGANet) to progressively detect concealed objects from lensless imaging measurements. This includes a region gaze module (RGM) to extract spatial-frequency cues and a region amplifier (RA) to enhance the details of object regions, improving concealed object detection performance."}
{"id": "21rSeWJHPF", "Context": "Ranking vertices in a graph is a fundamental task in computer science, often using centrality measures like PageRank. Traditional ranking algorithms can produce unbalanced rankings, especially in graphs with underlying communities, leading to information loss, polarized opinions, and reduced diversity.", "Idea": "Introduce a new approach called relative centrality, which uses iterative graph-dependent local normalization of centrality scores to promote balancedness in rankings while maintaining their validity. This method addresses the unbalancedness observed in traditional centrality measures, particularly in structures like multi-core-periphery with communities (MCPC)."}
{"id": "l0gZS0sAlf", "Context": "Training and fine-tuning large language models (LLMs) with diverse textual data from multiple sources present challenges due to conflicting gradient directions, which hinder optimization and specialization. These challenges can negatively impact model generalization across tasks, leading to reduced downstream performance. Recent research indicates that fine-tuning LLMs on carefully selected, task-specific data subsets can achieve or exceed the performance of using the entire dataset.", "Idea": "Propose the Ensembles of Low-Rank Expert Adapters (ELREA) framework, which clusters training instructions based on gradient directions to reduce optimization conflicts. Expert adapters are trained on these clusters using the low-rank adaptation (LoRA) technique for efficiency and scalability. During inference, ELREA selects the most relevant expert adapters based on input data's gradient similarity to ensure optimal task performance."}
{"id": "mFY0tPDWK8", "Context": "Machine learning methods have been increasingly used to predict initial solutions for mixed-integer linear programming (MILP) problems. These methods aim to reduce the problem's dimensionality by predicting solutions and fixing a subset of variables. However, this approach can result in low-quality or infeasible solutions if the predictions are inaccurate.", "Idea": "Introduce the Apollo-MILP framework, which alternates between prediction and correction steps to improve solution accuracy. It uses a trust-region search to refine solutions and employs an Uncertainty-based Error upper BOund (UEBO) to assess prediction reliability, fixing only those values with high confidence."}
{"id": "mYgoNEsUDi", "Context": "Diffusion models have become a powerful tool for generative AI on graphs, with applications in areas like drug design and knowledge discovery. However, these models struggle to fully capture the intrinsic higher-order topological properties of graphs, which limits their generalizability and effectiveness for various tasks.", "Idea": "Introduce zigzag spaghetti (ZS), a computationally efficient topological summary that extracts latent topological graph descriptors at multiple resolutions using zigzag persistence, and integrate dynamic topological information into graph diffusion models."}
{"id": "LBl7Hez0fF", "Context": "Hallucination is a significant issue in deploying large vision-language models (LVLMs), often caused by misalignments between visual inputs and textual outputs. This problem is distinct from hallucination in large language models (LLMs) due to the unique structure of LVLMs, where image encoders and text decoders are typically pre-trained separately, leading to sensitivity in text decoders to vision inputs.", "Idea": "Introduce Visual and Textual Intervention (VTI), a novel technique that reduces hallucinations by steering latent space representations during inference to enhance the stability of vision features, applicable as a task-agnostic test-time intervention without additional training costs."}
{"id": "dQ2xiSIYzp", "Context": "The task of learning 3D human Gaussians from a single image involves recovering detailed appearance and geometry, including unobserved regions. Existing methods face challenges in generating realistic human poses and shapes, and often struggle with generalization across different datasets and in-the-wild images.", "Idea": "Introduce a single-view generalizable Human Gaussian Model (HGM) that uses a generate-then-refine pipeline guided by human body and diffusion priors. The approach employs a ControlNet to refine back-view images and incorporates human priors from the SMPL-X model to improve pose and shape realism through sparse convolution and attention mechanisms."}
{"id": "uHLgDEgiS5", "Context": "Traditional data influence estimation methods, such as influence functions, assume that learning algorithms are permutation-invariant with respect to training data. However, modern training paradigms, particularly for foundation models using stochastic algorithms and non-convergent, multi-stage curricula, are sensitive to data ordering, violating this assumption. This mismatch makes influence functions inadequate for addressing questions about differentiating the influence of the same data at different training stages and capturing the dependence of data influence on the optimization trajectory.", "Idea": "Introduce the concept of trajectory-specific leave-one-out (LOO) influence to quantify the impact of removing a data point from a specific iteration during training, considering the sequence of data and the model's optimization trajectory. Propose data value embedding as a novel technique to efficiently approximate trajectory-specific LOO by computing a training data embedding that encapsulates interactions between data and evolving model parameters, allowing for efficient approximation through a dot-product with the gradient of test data."}
{"id": "lPJUQsSIxm", "Context": "The integration of fully homomorphic encryption (FHE) with machine learning presents opportunities for secure data processing, allowing computations on encrypted data while maintaining confidentiality. However, current FHE-based implementations for deep neural networks are hindered by high computational costs, latency, and scalability issues, which restrict their practical use.", "Idea": "Introduce DCT-CryptoNets, a method that operates in the frequency-domain using the discrete cosine transform (DCT) to alleviate the computational demands of non-linear activations and homomorphic bootstrap operations during private inference, enhancing efficiency and scalability for encrypted deep learning tasks."}
{"id": "rfdblE10qm", "Context": "The Bradley-Terry (BT) model is widely used in reward modeling for aligning Large Language Models (LLMs), despite its original design for multi-player stochastic game matching. The model's application in converting pairwise response comparisons to reward values and making predictions is not well understood, especially given the sparse comparison of prompt-response pairs.", "Idea": "Introduce a theoretical foundation for the convergence rate of BT reward models using deep neural networks with embeddings. Highlight the concept of order consistency in reward modeling and propose an alternative order-consistent reward modeling objective using a simple upper-bound algorithm compatible with binary classifiers."}
{"id": "W2Wkp9MQsF", "Context": "Model compression is a critical task in machine learning, especially for deploying large-scale models in resource-constrained environments. Traditional compression techniques often require access to training data and fine-tuning, which can be impractical or impossible in certain scenarios. Existing data-free methods struggle to maintain model performance, particularly at high sparsity levels.", "Idea": "Introduce model folding, a data-free compression technique that merges structurally similar neurons across layers using k-means clustering, preserving data statistics and preventing variance issues without needing training data or fine-tuning."}
{"id": "sLKDbuyq99", "Context": "Multi-agent frameworks utilizing large language models have shown significant success in automated planning and task execution. However, there is a lack of research on effectively adjusting workflows during execution, which is essential for adapting to unforeseen challenges and changing conditions in real-time to ensure efficient task execution.", "Idea": "Define workflows as an activity-on-vertex (AOV) graph, enabling continuous refinement by LLM agents through dynamic subtask allocation based on historical performance and previous AOVs. Emphasize modularity in workflow design by evaluating parallelism and dependency complexity to achieve efficient concurrent execution, effective goal achievement, and enhanced error tolerance."}
{"id": "9OJflnNu6C", "Context": "Generative models have advanced significantly but pose issues like privacy breaches and biases. Machine unlearning has been proposed to address these concerns by removing specific training data from models. In Image-to-Image (I2I) generative models, previous approaches have treated unlearning as a single objective optimization problem, which does not account for diverse user expectations regarding the balance between unlearning and model utility.", "Idea": "Propose a controllable unlearning framework for I2I generative models using a control coefficient to manage the trade-off between unlearning and model utility. Reformulate the unlearning problem into a constrained optimization problem, solved with a gradient-based method to achieve Pareto optimal solutions within defined boundaries for the control coefficient."}
{"id": "zjeHLSiNv1", "Context": "Transformer models' performance is known to be logarithmically related to their number of parameters and computational complexity. Although Mixture of Experts (MoE) methods attempt to separate parameter count from computational complexity, they encounter difficulties during inference due to high memory access costs.", "Idea": "Introduce UltraMem, a large-scale, ultra-sparse memory layer designed to reduce inference latency while maintaining model performance, and explore its scaling laws to demonstrate superior scaling properties compared to MoE."}
{"id": "hgwGi81ndj", "Context": "In reinforcement learning, difficult exploration problems arise when agents need to efficiently learn in complex environments. Traditional approaches often struggle with these challenges due to the complexity of state and action spaces. A potential solution involves using object-centric mappings, which describe items and their attributes, to simplify the learning process.", "Idea": "Propose a fully model-based algorithm that utilizes hierarchical abstraction by modeling items and attribute changes at higher levels of state and temporal abstraction. This approach simplifies transition dynamics, enabling efficient exploration through a discriminative world model and count-based intrinsic rewards, allowing the agent to plan and reach discovered abstract states."}
{"id": "CI4sCBMXjP", "Context": "Enhancing the adaptive capabilities of large language models is a critical pursuit in both research and application. Traditional fine-tuning methods require substantial data, computational resources, and specific capabilities, while in-context learning is limited by the need for appropriate demonstrations and efficient token usage.", "Idea": "Introduce ELICIT, a framework with two modules designed to store and reuse task vectors, enhancing the adaptive capabilities of models without additional training or inference tokens."}
{"id": "hJVdwBpWjt", "Context": "Large language models (LLMs) have achieved state-of-the-art performance in various auditory tasks, such as speech, music, and general audio, but their potential in bioacoustics tasks remains underexplored. These tasks, including detecting animal vocalizations, classifying rare species, and labeling context and behavior, are vital for conservation and biodiversity monitoring.", "Idea": "Introduce NatureLM-audio, the first audio-language foundation model specifically designed for bioacoustics, trained on curated text-audio pairs from bioacoustics, speech, and music to address the scarcity of annotated data in the field."}
{"id": "4GSOESJrk6", "Context": "Personalized image generation is a promising tool for assisting humans in daily tasks due to its ability to creatively generate personalized content. However, current evaluation methods are either automated but misaligned with human judgment or require costly and time-consuming human evaluations.", "Idea": "Introduce DreamBench++, a human-aligned benchmark automated by advanced multimodal GPT models. This involves systematically designing prompts for GPT to be both human-aligned and self-aligned, along with constructing a comprehensive dataset of diverse images and prompts."}
{"id": "PkpNRmBZ32", "Context": "Traditional neural network architectures for audio processing tasks often rely on homogeneous configurations and specific types of operations like nonlinear recurrence, explicit convolutions, or attention mechanisms. These approaches can limit flexibility and efficiency in network design, particularly when balancing network size, performance, and computational resources.", "Idea": "Introduce Centaurus, a network architecture composed of generalized state-space model blocks, allowing tensor contractions to optimize training efficiency. This design incorporates a mixture of classical convolutional block inspirations, such as group convolutions and bottleneck blocks, to enhance flexibility and efficiency without relying on traditional recurrence, convolution, or attention mechanisms."}
{"id": "t8fu5m8R5m", "Context": "Anomaly Detection (AD) methods face challenges in robustness against adversarial attacks, which affects their reliability in critical applications like autonomous driving. This vulnerability is due to the AD setup, which typically uses only unlabeled normal samples for training, making detectors susceptible to adversarial anomalies during testing. Adversarial training is difficult to implement effectively without labeled data, as it requires an objective function that can maximize the margin between normal and anomaly distributions.", "Idea": "Propose creating a pseudo-anomaly group from normal samples and using adversarial training with contrastive loss as an objective function to enhance robustness. Address the issue of spurious negative pairs by defining opposite pairs and adversarially separating them to improve inter-group perturbations."}
{"id": "fGhr39bqZa", "Context": "Causal discovery with latent variables is a significant and complex problem. Most existing approaches depend on the assumption that latent variables have pure children, which can be limiting in practical scenarios and is not strictly necessary from a theoretical standpoint.", "Idea": "Introduce the concept of homologous surrogate to eliminate the need for pure children in causal discovery with latent variables. Formulate two assumptions involving homologous surrogates and develop theoretical results under each, leading to an algorithm that utilizes these properties for causal graph recovery."}
{"id": "4ua4wyAQLm", "Context": "Video anomaly detection (VAD) is tasked with identifying novel actions or events that were not seen during training. Current VAD techniques often focus on global patterns, which include redundant details, and face challenges in generalizing to unseen samples.", "Idea": "Propose a framework that identifies and models the dynamics of local patterns to generalize to novel samples. This involves a two-stage process with image-text alignment and cross-modality attention to extract spatial local patterns, and a State Machine Module (SMM) to enhance these patterns with temporal clues, complemented by temporal motion estimation."}
{"id": "xPxHQHDH2u", "Context": "Recent advancements in novel view synthesis have been driven by NeRF- and 3DGS-based methods. Despite these advancements, reconstructing reflective objects remains a significant challenge, as there is no existing solution that provides real-time, high-quality rendering while effectively handling inter-reflection.", "Idea": "Introduce the Reflective Gaussian splatting (Ref-Gaussian) framework, which includes a physically based deferred rendering approach using split-sum approximation for pixel-level material properties, and a Gaussian-grounded inter-reflection function within a Gaussian splatting paradigm. Additionally, it incorporates material-aware normal propagation and an initial per-Gaussian shading stage with 2D Gaussian primitives to enhance geometry modeling."}
{"id": "i3e92uSZCp", "Context": "Skill discovery methods allow agents to learn a variety of behaviors without needing explicit rewards. Achieving a semantically diverse set of skills is important for their applicability in downstream tasks. Existing methods either use discriminators to create distinguishable skills or focus on increasing state coverage, but the direct pursuit of semantic diversity in skills is not well-explored.", "Idea": "Introduce Language Guided Skill Discovery (LGSD), a framework that uses large language models to enhance the semantic diversity of skills. LGSD utilizes user prompts to constrain the search space and guide agents to explore semantically diverse states, resulting in a set of semantically distinctive skills."}
{"id": "ws5phQki00", "Context": "Stance detection is crucial for enhancing online political discussions, aiding in content moderation, topic summarization, and fostering balanced debates. Traditionally, transformer-based models are used for stance detection, but they require large datasets, which are difficult to gather due to the diverse range of debate topics. Although large language models (LLMs) have revitalized stance detection, their deployment in online discussions is hindered by issues such as inconsistent outputs, biases, and susceptibility to adversarial attacks.", "Idea": "Utilize LLM-generated synthetic data to enhance stance detection in online political discussions by employing traditional stance detection models for deployment and leveraging LLMs for secure offline synthetic data generation. This involves generating synthetic data for specific debate questions using a Mistral-7B model and fine-tuning with this data to improve stance detection performance. Additionally, by using synthetic data as a reference, the approach identifies the most informative samples in an unlabelled dataset to further enhance model performance."}
{"id": "t8KLjiFNwn", "Context": "Transformer models have been extensively explored across various domains due to their ability to handle long-range dependencies and provide global contextual awareness, which has fueled the development of popular AI applications. Meanwhile, State Space Models (SSMs) have emerged as strong competitors in sequential modeling by incorporating a selective mechanism for dynamic parameter adjustment based on input data. However, this mechanism increases computational complexity and bandwidth demands, creating challenges for deployment on resource-constrained mobile devices.", "Idea": "Propose a sparse learning framework that integrates architecture-aware compiler optimizations, introducing an end-to-end solution called C4^n kernel sparsity, which prunes n elements from every four contiguous weights. Develop a compiler-based acceleration solution to ensure execution efficiency for this sparsity on mobile devices, and propose C4^n-specific optimizations combined with a layout transformation elimination strategy to improve performance across operations."}
{"id": "Bp0HBaMNRl", "Context": "Discovering causal structures with latent variables from observational data is a fundamental challenge in causal discovery. Existing methods often rely on constraint-based, iterative discrete searches, limiting their scalability for large numbers of variables. Moreover, these methods frequently assume linearity or invertibility, restricting their applicability to real-world scenarios.", "Idea": "Present new theoretical results on the identifiability of non-linear latent hierarchical causal models, relaxing previous assumptions about the deterministic nature of latent variables and exogenous noise. Develop a novel differentiable causal discovery algorithm that efficiently estimates the structure of such models."}
{"id": "2IoFFexvuw", "Context": "Recent advancements in reinforcement learning have successfully fine-tuned diffusion-based generative models. However, challenges persist in fine-tuning continuous flow-based generative models to align with arbitrary user-defined reward functions. These challenges include policy collapse from overoptimization and the high computational cost of likelihoods in continuous-time flows.", "Idea": "Propose an RL fine-tuning method called Online Reward-Weighted Conditional Flow Matching with Wasserstein-2 Regularization (ORW-CFM-W2), which integrates RL into the flow matching framework to fine-tune generative models with arbitrary reward functions. The method uses an online reward-weighting mechanism to prioritize high-reward regions and incorporates Wasserstein-2 distance regularization to prevent policy collapse and maintain diversity."}
{"id": "izjNI5bcOV", "Context": "The Earth's weather system is complex, involving various data modalities and tasks that are crucial for human life. Current data-driven models typically focus on individual weather understanding tasks, such as weather forecasting, but struggle to address multiple complex tasks within a single model. Additionally, these models are limited by their reliance on a small set of real observations for specific scenarios, which restricts their performance potential.", "Idea": "Introduce the WeatherGFM, a generalist weather foundation model that unifies the representation and definition of diverse weather understanding tasks. It employs weather prompt formats to manage different data modalities and uses a visual prompting question-answering paradigm for training, enabling it to handle multiple weather tasks in a unified manner."}
{"id": "5IWJBStfU7", "Context": "As AI systems are increasingly used in critical applications, ensuring their interpretability is crucial. Mechanistic Interpretability (MI) seeks to reverse-engineer neural networks to extract human-understandable algorithms that explain their behavior. A fundamental question arises: for a fixed behavior, can MI guarantee a unique explanation? This is analogous to the concept of identifiability in statistics, which ensures the uniqueness of inferred parameters under specific assumptions.", "Idea": "Introduce two strategies for producing MI explanations: 'where-then-what', which identifies a network subset replicating behavior before deriving its interpretation, and 'what-then-where', which starts with candidate algorithms and searches for their implementation in the network's activation subspaces, using causal alignment. The study tests the identifiability of these strategies, revealing non-identifiability and discussing the necessity of unique explanations."}
{"id": "z8PcUSKXXN", "Context": "Recent advancements in deep image denoising have led to the development of models capable of handling various noise types. The current state-of-the-art method, Masked Training (MT), uses a masked swinir model trained on Gaussian noise, achieving good performance across different noise types. However, MT often results in over-smoothed images and presents challenges in optimizing mask ratios, complicating integration with other methods.", "Idea": "Introduce RNINet, a novel architecture based on a streamlined encoder-decoder framework, which includes a noise injection block that injects random noise into feature statistics. This approach enhances generalization across unseen noise types and simplifies the architectural complexity of existing methods."}
{"id": "yitH9xAHQs", "Context": "Large language models (LLMs) have achieved impressive performance by training on diverse, high-quality task-specific data. Current methods often depend on human-annotated data or predefined task templates, which may limit the scope of generated data and miss critical edge cases or novel scenarios.", "Idea": "Introduce ReverseGen, a novel approach that automatically generates training samples to expose LLM weaknesses by using a dedicated proposer to create queries that lead to unsatisfactory model responses. These queries are used to construct training data, improving model performance across various scales and applications."}
{"id": "PUnD86UEK5", "Context": "Adam is known to outperform SGD in training language models, but the theoretical understanding of this advantage is limited. Previous analyses of convergence for both Adam and SGD have focused on the number of steps and have been optimal in non-convex scenarios, with both achieving a convergence rate of approximately O(T^-1/4).", "Idea": "Propose a new convergence analysis for Adam that leverages the smoothness of loss under the ℓ∞-geometry, rather than the traditional ℓ2-geometry, leading to a better empirical smoothness constant for models like GPT-2 and ResNet. Extend this analysis to blockwise Adam with novel blockwise smoothness assumptions."}
{"id": "pPQPQ7Yd58", "Context": "In image-based control pipelines learned from behavior cloning, the geometry of the visual representation space, which serves as the information channel from the vision encoder to the action decoder, is crucial. Inspired by the neural collapse phenomenon in image classification, there is an observed clustering in the visual representation space. In discrete control tasks, visual representations cluster according to discrete action labels, while in continuous control tasks, clustering is based on control-oriented classes related to the relative pose between objects and targets.", "Idea": "Leverage the observed law of clustering as an algorithmic tool by pretraining the vision encoder using neural collapse as a regularization technique. This encourages control-oriented clustering of visual features, which, when finetuned with the action decoder, enhances test-time performance significantly."}
{"id": "6qUUgw9bAZ", "Context": "Decoding procedures such as search, reranking, and self-critique are computationally intensive but can enhance the quality of language model outputs in tasks like code generation, numerical reasoning, and dialog. Traditionally, the same decoding procedure is applied uniformly to all inputs, regardless of the varying computational needs of different inputs.", "Idea": "Develop an approach that predicts the distribution of rewards based on input and computation budget, allowing for adaptive allocation of computation resources. This includes an adaptive best-of-k procedure for dynamic sample generation and a routing procedure that selects between expensive, accurate decoding and cheaper, less capable alternatives."}
{"id": "d8hYXbxX71", "Context": "Improving social welfare involves optimizing policy objectives across different time horizons, which is challenging because policies that seem suboptimal in the short term may have significant long-term benefits. Rawlsian policies prioritize those with the greatest need, while utilitarian policies focus on maximizing immediate welfare gains. These approaches are traditionally seen as conflicting, with Rawlsian policies thought to reduce average social welfare compared to utilitarian ones.", "Idea": "Analyze Rawlsian and utilitarian policies within a sequential decision-making framework where individuals' welfare levels stochastically decay over time, and policymakers can intervene to prevent this decay. Prove that under certain conditions, Rawlsian policies can outperform utilitarian policies in the long run, despite utilitarian policies being more effective in the short term."}
{"id": "G0dksFayVq", "Context": "LLM-based judges are increasingly used as scalable alternatives to human evaluation for assessing and improving models. However, their reliability is often not thoroughly examined, especially as LLMs become more sophisticated. Existing benchmarks mainly focus on alignment with human preferences, which may not be suitable for tasks requiring factual and logical correctness.", "Idea": "Propose a novel evaluation framework and introduce JudgeBench, a benchmark designed to evaluate LLM-based judges on challenging tasks across knowledge, reasoning, math, and coding. JudgeBench uses a new pipeline to convert difficult datasets into response pairs with preference labels that reflect objective correctness."}
{"id": "vJkktqyU8B", "Context": "Current Vision Transformer (ViT) adapter methods, while achieving promising accuracy, suffer from inefficient memory access operations such as standard normalization and frequent reshaping, which hinder inference speed.", "Idea": "Propose META, a ViT adapter that enhances memory efficiency and reduces memory time consumption by minimizing inefficient memory access operations. It includes a memory-efficient adapter block that shares layer normalization between self-attention and feed-forward network layers, employs cross-shaped self-attention to reduce reshaping operations, and incorporates a lightweight convolutional branch to enhance local inductive biases. The adapter block is designed in a cascaded manner to compute diverse head features, enriching feature representation variety."}
{"id": "SiH7DwNKZZ", "Context": "Transformers have become a popular choice as generic backbones in computer vision, although they were originally developed for natural language processing. The Long Short-Term Memory (LSTM) model has been recently enhanced to a more scalable and efficient version known as xLSTM, which addresses traditional LSTM limitations through innovations like exponential gating and a parallelizable matrix memory structure.", "Idea": "Introduce Vision-LSTM (ViL), an adaptation of xLSTM for computer vision, consisting of a stack of xLSTM blocks that alternately process sequences of patch tokens from top to bottom and bottom to top."}
{"id": "9NfHbWKqMF", "Context": "3D Gaussian Splatting (3DGS) has recently advanced photorealistic reconstruction, offering high visual fidelity and real-time performance. However, a significant challenge arises as rendering quality declines when test views differ from the camera angles used during training, which is problematic for immersive free-viewpoint rendering and navigation. Existing methods, even those with regularization techniques and data-driven priors, struggle to generalize effectively to out-of-distribution (OOD) views.", "Idea": "Introduce SplatFormer, the first point transformer model designed to operate on Gaussian splats, which refines an initial 3DGS set optimized under limited training views in a single forward pass, effectively removing artifacts in OOD test views."}
{"id": "84WmbzikPP", "Context": "Molecular structure elucidation is crucial for understanding chemical phenomena and has applications in various fields such as natural products, lab syntheses, forensic samples, and the interstellar medium. The task involves predicting a molecule's 3D structure from its molecular formula and moments of inertia, which can be measured with high precision using rotational spectroscopy. Existing generative models can sample 3D structures with approximately correct moments, but they do not fully utilize the precision available from experimental data.", "Idea": "Propose Stiefel Flow Matching as a generative model to predict 3D molecular structures under exact moment constraints by embedding the space of $n$-atom point clouds in the Stiefel manifold. This approach includes learning simpler and shorter flows through approximate solutions for equivariant optimal transport on the Stiefel manifold."}
{"id": "9FqARW7dwB", "Context": "Residual connections are commonly used in neural network architectures but suffer from issues such as the seesaw effect between gradient vanishing and representation collapse. These drawbacks can hinder the performance and efficiency of training large language models and other AI tasks.", "Idea": "Introduce hyper-connections as an alternative to residual connections, allowing networks to adjust connection strengths between features at different depths and dynamically rearrange layers."}
{"id": "ho4mNiwr2n", "Context": "Anti-backdoor learning is a defense method aimed at training clean models from poisoned datasets, which is crucial for countering backdoor attacks. Existing methods struggle to restore backdoored samples to their original labels and do not generalize well to large pre-trained models due to non end-to-end training, making them inadequate for protecting these increasingly common models.", "Idea": "Introduce an end-to-end method called Mind Control through Causal Inference (MCCI) that uses both images and attack indicators to train clean models from poisoned datasets. This approach allows the model to control its perception of inputs as clean or backdoored, enabling correct predictions even for poisoned samples by using fake non-attack indicators."}
{"id": "WwmtcGr4lP", "Context": "Effective cancer treatment is challenging due to the individualized nature of patient responses, which are influenced by diverse mutations across patient genomes. The limited availability of patient response data complicates the training of personalized treatment models. Existing methods use transfer learning with larger pre-clinical datasets to create a shared representation between cell line and patient domains, but they fail to capture patient-specific characteristics crucial for drug response prediction.", "Idea": "Introduce GANDALF, a generative attention-based data augmentation and predictive modeling framework that directly augments patient genomic data while accounting for domain-specific characteristics."}
{"id": "d8cnezVcaW", "Context": "Direct Preference Optimization (DPO) is a method used to enhance reinforcement learning from human feedback, particularly for fine-tuning large language models. However, DPO struggles to effectively capture the diversity of human preferences, which limits its performance.", "Idea": "Develop a new approach called MallowsPO, inspired by Mallows' theory of preference ranking, which introduces a dispersion index to reflect the diversity of human preferences. This approach unifies existing DPO models as special cases and uses the dispersion index to improve DPO's performance across various tasks."}
{"id": "MGKDBuyv4p", "Context": "Language models have the capability to memorize information, which can lead to the verbatim regurgitation of training data during inference. This poses a problem when the data is private or sensitive, necessitating methods to mitigate such memorization.", "Idea": "Investigate and introduce various methods to mitigate memorization in language models, including three regularizer-based, three fine-tuning-based, and eleven machine unlearning-based methods, with five new unlearning methods. Additionally, introduce TinyMem, a suite of small, computationally-efficient language models for developing and evaluating these memorization-mitigation methods."}
{"id": "vmulbBDCan", "Context": "Electron-multiplying charge-coupled devices (EMCCDs) are crucial for sensitive observations in low-light conditions across various fields such as astronomy, material science, and biology. Despite their advanced designs to enhance target signals and reduce read-out circuit noise, EMCCD images still contain noise, affecting outcomes in applications like fluorescence microscopy. Existing noise models for EMCCDs focus on theoretical statistical characteristics and have not integrated recent advancements in computational photography, which use physics-based noise models to guide deep learning for adaptive denoising in ordinary image sensors.", "Idea": "Introduce a systematic study on physics-based noise model calibration for EMCCD cameras, accurately estimating statistical features of noise components to generate authentic training samples for a recent neural network. Capture a real-world test image dataset for EMCCD, including both ordinary and microscopic scenes, to benchmark and demonstrate the advantages of the proposed model over previous methods."}
{"id": "iXCeQ2m6vT", "Context": "AI systems struggle with understanding visual relations, particularly when dealing with previously unseen objects, whereas humans excel at this task. Active vision theories suggest that learning visual relations is linked to eye movements that help fixate on objects and their parts, with spatial information from these movements aiding in representing relations between image parts.", "Idea": "Develop a system using Glimpse-based Active Perception (GAP) that sequentially focuses on salient regions of an image at high resolution, utilizing the locations from these glimpses and the surrounding visual content to represent relations between different image parts."}
{"id": "FoF5RaA3ug", "Context": "Recent advancements in dataset distillation have shown the advantages of using soft labels generated by pre-trained teacher models. However, the choice of loss function for utilizing these soft labels significantly affects the performance of models trained on synthetic datasets, indicating the need for a universal loss function.", "Idea": "Introduce GIFT, a simple and effective plug-and-play approach that includes soft label refinement and a cosine similarity-based loss function to fully utilize label information in dataset distillation."}
{"id": "R4h5PXzUuU", "Context": "Foundation models, trained on vast internet-scale data, have shown impressive generalization capabilities and are increasingly adopted across various application domains. However, the trustworthiness of these models, particularly their out-of-distribution detection (OoDD) capabilities, remains underexplored. This gap is especially evident in large vision-language models (LVLMs) like GPT-4o, which are trained on extensive multi-modal data, raising concerns about their safe and reliable deployment.", "Idea": "Introduce a self-guided prompting approach called Reflexive Guidance (ReGuide) to enhance the out-of-distribution detection capabilities of large vision-language models by utilizing self-generated image-adaptive concept suggestions."}
{"id": "gU4ZgQNsOC", "Context": "Pretraining large language models on extensive and varied datasets is essential for achieving top performance in various tasks. However, current training methods treat all data samples equally, ignoring the individual importance or relevance of each sample during training. Existing strategies that reweight data focus on group-level importance and fail to utilize detailed instance-level information, lacking adaptability to the changing importance of samples as training progresses.", "Idea": "Introduce novel algorithms for dynamic, instance-level data reweighting that adjust the weight of each training sample based on its loss value in real-time. This approach allows the model to focus on more informative samples during training, systematically deprioritizing redundant data, and includes a new theoretical framework for analyzing the impact of loss-based reweighting on optimization convergence."}
{"id": "f7KxfUrRSb", "Context": "Aligning language models with human preferences is a significant research focus, aiming to enhance the models' ability to meet diverse user needs. The concept of weak-to-strong generalization, where a strong language model fine-tuned on labels from a weaker model can outperform its supervisor, serves as an inspiration for improving model alignment.", "Idea": "Propose a method called Weak-to-Strong Preference Optimization (WSPO) that achieves strong model alignment by learning the distribution differences before and after the alignment of the weak model, effectively transferring and amplifying alignment behavior from weaker to stronger models."}
{"id": "oU3tpaR8fm", "Context": "Retrieval-augmented generation (RAG) allows large language models (LLMs) to leverage external knowledge sources, with the potential to improve output quality by processing longer input sequences. However, increasing the number of retrieved passages can initially enhance but eventually degrade the quality of generated outputs due to the presence of 'hard negatives'.", "Idea": "Propose both training-free and training-based approaches to improve the robustness of long-context LLM-based RAG. This includes retrieval reordering as a training-free optimization and RAG-specific implicit LLM fine-tuning along with RAG-oriented fine-tuning with intermediate reasoning as training-based methods."}
{"id": "iylpeTI0Ql", "Context": "Test-time adaptation (TTA) addresses distribution shifts between source and target data by using only target data during testing. In open-world scenarios, models often encounter noisy samples that are outside the in-distribution label space, which can severely degrade the performance of existing TTA methods.", "Idea": "Introduce Zero-Shot Noisy TTA (ZS-NTTA), which leverages the zero-shot capability of pre-trained vision-language models to adapt to target data with noisy samples during test-time. Propose a novel framework that decouples the classifier and detector, focusing on developing an individual detector called Adaptive Noise Detector (AdaND) that uses pseudo-labels from the frozen model to train a noise detector, and injects Gaussian noise to prevent misclassification of clean samples."}
{"id": "4rEI2JdHH6", "Context": "Grokking is a phenomenon in neural networks where models initially memorize training data and generalize poorly, but eventually transition to near-perfect generalization after extended training. This delayed generalization is problematic as it affects predictability and efficiency, with the ideal scenario being immediate generalization without delay.", "Idea": "Propose GrokTransfer, a method to accelerate grokking by first training a smaller model to achieve nontrivial test performance, then using its learned input embedding to initialize a stronger model, thereby enabling direct generalization without delay."}
{"id": "vQhn4wrQ6j", "Context": "Fine-tuning Large Language Models (LLMs) for specific tasks in non-English languages is challenging due to the lack of task-specific data. This is particularly problematic for tasks like mathematical reasoning, where in-language math data is often unavailable.", "Idea": "Develop a model merging methodology that enhances cross-lingual transfer by composing language and math capabilities. This involves fine-tuning separate 'experts' on math instruction data in English and on generic instruction data in the target language, then swapping the top and bottom transformer layers between these experts to improve math performance in the target language."}
{"id": "uREg3OHjLL", "Context": "The expressive power of ReLU neural networks is believed to increase with their depth. A specific function, $F_n = \\max (0,x_1,\\ldots,x_n )$, has been used to explore this relationship. A conjecture suggests that any ReLU network exactly representing $F_n$ requires at least $\\lceil \\log_2 (n+1) \\rceil$ hidden layers, a claim recently confirmed for networks with integer weights.", "Idea": "Investigate ReLU networks with decimal fraction weights, showing that $F_n$ requires at least $\\lceil \\log_3 (n+1) \\rceil$ hidden layers, and for networks with $N$-ary fraction weights, at least $\\Omega( \\frac{\\ln n}{\\ln \\ln N})$ layers, providing a non-constant lower bound on the depth of rational ReLU networks."}
{"id": "vr1QdCNJmN", "Context": "The Bregman divergence is a pseudo-distance commonly used for comparing vectors or functions in continuous spaces, generated from a convex function. However, defining a similar divergence for discrete spaces is challenging. Previous work by Iyer & Bilmes explored Bregman divergences on discrete domains using submodular functions, which are discrete analogs of convex functions.", "Idea": "Generalize the framework to include generating functions that are neither submodular nor supermodular, creating the difference-of-submodular Bregman divergence. Introduce a learnable form of this divergence using permutation-invariant neural networks to effectively capture structural properties in discrete data."}
{"id": "2e4ECh0ikn", "Context": "The recent development of audio foundation models (FMs) offers potential advancements in conversational modeling. However, there is a lack of comprehensive evaluation of these models' abilities to engage in natural and interactive conversations, particularly in managing turn-taking without excessive overlapping speech or prolonged silences.", "Idea": "Propose a novel evaluation protocol to assess spoken dialog systems' turn-taking capabilities using a supervised model trained to predict turn-taking events in human-human conversations. This protocol is used to conduct a comprehensive user study and evaluate multiple audio FMs on curated test benchmarks, with plans to open source the evaluation platform to encourage further development in conversational AI."}
{"id": "QG31By6S6w", "Context": "Recent advancements in medical vision-language pre-training models have significantly improved zero-shot disease recognition. However, transferring image-level knowledge to pixel-level tasks like lesion segmentation in 3D CT scans is challenging due to the complexity and variability of pathological visual characteristics. Existing methods struggle to align fine-grained lesion features not encountered during training with disease-related textual representations.", "Idea": "Introduce Malenia, a multi-scale lesion-level mask-attribute alignment framework for 3D zero-shot lesion segmentation, enhancing compatibility between mask representations and elemental attributes. It includes a Cross-Modal Knowledge Injection module to enrich visual and textual features with mutually beneficial information, guiding segmentation result generation."}
{"id": "X9OfMNNepI", "Context": "Scientific discovery is crucial for societal advancement, and there is growing interest in using large language models (LLMs) to accelerate this process. However, it remains uncertain if LLMs can autonomously generate novel and valid hypotheses in the field of chemistry. The challenge lies in determining whether LLMs can effectively discover new hypotheses based on a given research question.", "Idea": "Develop a multi-agent framework using LLMs to explore the potential of automatically generating chemistry hypotheses. This framework breaks down the task into three stages: retrieving inspirations from a background question, forming hypotheses from these inspirations, and ranking the hypotheses based on their quality."}
{"id": "keu6sxrPWn", "Context": "As large language models (LLMs) become more advanced, they also become harder to trust due to potential 'subversive misalignment,' where subtle errors bypass safety checks. This creates a challenge in balancing safety with the capabilities of these untrusted models during deployment, as each error increases the risk of a safety failure.", "Idea": "Introduce the 'Diffuse Risk Management' problem, which aims to balance safety and usefulness in deploying untrusted models across multiple tasks. This is achieved through a two-level framework: micro-protocols at the single-task level using a trusted model to monitor the untrusted model, and a macro-protocol at the whole-scenario level that adaptively estimates risk to select between micro-protocols."}
{"id": "2ZK8zyIt7o", "Context": "The rapid advancement of text-to-image diffusion models has enabled them to generate unprecedented results from given texts. However, as text inputs become longer, existing encoding methods like CLIP face limitations, and aligning the generated images with long texts becomes challenging.", "Idea": "Propose LongAlign, which includes a segment-level encoding method for processing long texts and a decomposed preference optimization method for effective alignment training. The segment-level encoding divides long texts into multiple segments for separate processing, overcoming input length limits. The decomposed preference optimization involves a reweighting strategy to reduce overfitting by assigning different weights to text-relevant and text-irrelevant components of preference scores."}
{"id": "RaR3ETzyKp", "Context": "Recent studies have shown that different diffusion methods and architectures trained on the same dataset yield similar results when given the same input noise. This suggests the existence of preferable noises for specific samples. By visualizing noise-sample pairs in two-dimensional spaces, it is observed that preferable paths, which connect these noises to samples, are more organized and have fewer crossings compared to random paths. In high-dimensional spaces, paths rarely intersect, and the crossings in two-dimensional spaces indicate shorter inter-path distances in high-dimensional spaces.", "Idea": "Propose the Distance-Aware Noise-Sample Matching (DANSM) method to increase inter-path distance, thereby accelerating model training. DANSM is based on rectified flow models, utilizing a closed-form formula to calculate inter-path distance, and simplifies optimization by relating inter-path distance to path length, using the latter as an optimization surrogate."}
{"id": "e8qXTxMgPg", "Context": "The study focuses on dimensionality reduction for $s$-sparse vectors, which are vectors with at most $s$ non-zero coordinates. Traditional approaches provide average-case guarantees for embedding these vectors, with known upper bounds based on the birthday-paradox. These bounds suggest that a linear map can preserve the norm of most vectors in a collection, but achieving this requires mapping to a high-dimensional space. The challenge lies in finding efficient embeddings that maintain vector norms while reducing dimensionality.", "Idea": "The research introduces novel lower bounds for dimensionality reduction, showing that any oblivious linear map must map to a certain dimensionality to maintain average-case guarantees. It also explores embeddings for sparse non-negative vectors, achieving smaller dimensionality by leveraging non-linearity and non-negativity, which allows for preserving pairwise distances with improved bounds. This approach provides exact dimensionality reduction for certain norms and highlights the necessity of non-linearity and non-negativity for achieving these results."}
{"id": "Wvi8c0tgvt", "Context": "Existing blur datasets lack sufficient variety in scenes and blur patterns, making them inadequate for training. Expanding data diversity is challenging due to the complexity and effort required by dual-camera systems. Current data augmentation methods focus on 2D motion estimation, ignoring the 3D nature of camera and object movements, leading to unrealistic motion patterns.", "Idea": "Propose a 3D-aware blur synthesizer that generates diverse and realistic blur images for data augmentation by estimating 3D camera positions within the motion blur interval and synthesizing realistic blur images without explicit depth measurements. This method allows for controllable blur augmentation by adjusting blur magnitude, direction, and scenes."}
{"id": "c4OGMNyzPT", "Context": "Large Vision Language Models (LVLMs) have shown significant capabilities in processing both visual and textual information. However, current evaluation methods, such as those based on Visual Question Answering and image captioning benchmarks, are insufficient in fully assessing LVLMs' abilities. These methods are limited by inadequate evaluation of detailed visual perception, data contamination, and insufficient focus on multi-turn reasoning.", "Idea": "Propose LVLM-Playground, a game-based evaluation framework that comprehensively assesses LVLMs' cognitive and reasoning skills in structured environments. This framework evaluates LVLMs on tasks like Perceiving, Question Answering, Rule Following, and End-to-End Playing, each designed to test specific abilities such as visual perception and decision-making."}
{"id": "bc3sUsS6ck", "Context": "Large language models (LLMs) acquire substantial knowledge during pretraining but often require adaptation to new contexts, tasks, or domains. This adaptation is typically achieved through fine-tuning, which incurs significant training costs, or prompting, which increases inference overhead.", "Idea": "Introduce GenerativeAdapter, an adaptation method that encodes test-time context into language model parameters with a single forward pass. It augments a frozen pretrained LM with a lightweight adapter generator, trained via self-supervised learning, to produce parameter-efficient adapters, allowing one generator to adapt the base model for all language processing scenarios."}
{"id": "rpouyo09V0", "Context": "Large language models (LLMs) are crucial for code generation, especially in interactive settings. However, current benchmarks do not adequately capture the diverse feedback encountered in multi-turn interactions, limiting the evaluation of LLMs in these scenarios.", "Idea": "Introduce novel benchmarks that model feedback quality for code generation LLMs, including CONVCODEWORLD, an environment simulating interactive scenarios with various feedback types, and CONVCODEBENCH, a static version using pre-generated feedback logs to maintain evaluation efficiency."}
{"id": "0mtz0pet1z", "Context": "In preventive medicine and non-fatal health conditions, such as HIV infection without AIDS, the timing of treatment initiation is crucial. Traditional causal inference has focused on determining the optimal timing for treatment and its effects, which may depend on individual characteristics.", "Idea": "Propose a method to identify and estimate the incremental causal effect of intervening on the timing of treatment initiation, without relying on the common positivity assumption, using inverse probability weighting."}
{"id": "u3TL0qxLWf", "Context": "Large Language Models (LLMs) have significantly advanced natural language processing but are hindered by high runtime costs, making widespread deployment challenging. These models require substantial memory access and computational resources, which limits their efficiency and scalability.", "Idea": "Introduce SeedLM, a post-training compression method that encodes model weights using seeds of a pseudo-random generator. This method employs a Linear Feedback Shift Register (LFSR) to generate random matrices during inference, which are combined with compressed coefficients to reconstruct weight blocks, reducing memory access and utilizing idle compute cycles without relying on calibration data."}
{"id": "4O0v4s3IzY", "Context": "There is ongoing debate about the reasoning capabilities of Large Language Models (LLMs). Initial hopes that reasoning would naturally emerge with increased scale have been challenged by various counterexamples. Despite this, many believe LLMs can iteratively self-improve their solutions, based on the assumption that verifying correctness is easier than generating solutions, a notion rooted in computational complexity theory.", "Idea": "Conduct a systematic investigation into the effectiveness of iterative prompting for reasoning and planning, using GPT-4 across three domains: Game of 24, Graph Coloring, and STRIPS planning. The study examines both self-critique by the model and verification by an external reasoner, analyzing the impact of criticisms on performance and the effects of removing elements from the augmented system."}
{"id": "P4XmKjXTrM", "Context": "Reproducibility is a major challenge in machine learning for healthcare due to the private nature of datasets, model pipelines, and task or cohort definitions. This creates barriers in sharing, iterating, and understanding ML results on electronic health record datasets.", "Idea": "Introduce the Automatic Cohort Extraction System (ACES) for event-stream data, which simplifies the development and reproduction of tasks and cohorts in ML for healthcare. ACES offers a domain-specific configuration language for defining dataset-specific and agnostic criteria, and a pipeline to extract patient records meeting these criteria from real-world data."}
{"id": "SgymXhOEA5", "Context": "Person re-identification (ReID) models often suffer from camera bias, which becomes more pronounced under data distribution shifts. Previous camera-aware methods have been limited to the training domains of the models. Additionally, unsupervised learning of ReID models shows a strong bias towards camera labels, even for seen domain data, indicating a need for improvement.", "Idea": "Revisit feature normalization on embedding vectors as a debiasing method for unseen domain data, analyzing its effectiveness in reducing bias related to low-level image properties and body angle. Propose simple training strategies to mitigate camera bias in unsupervised learning, demonstrating significant performance improvements with minor modifications to existing algorithms."}
