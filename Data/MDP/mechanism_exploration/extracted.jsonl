{"id": "nDvgHIBRxQ", "Context": "The evaluation of large language models (LLMs) for mathematical reasoning is crucial, yet current benchmarks mainly focus on problem-solving, risking overfitting and failing to accurately measure true mathematical reasoning abilities. There is a need for a comprehensive evaluation method that reflects real-world user experiences and assesses the robustness and generalization of these models across diverse tasks.", "Idea": "Introduce MathCheck, a checklist designed to test task generalization and reasoning robustness, along with an automatic tool for efficient checklist generation. MathCheck includes various mathematical reasoning tasks and robustness tests, and is used to develop MathCheck-GSM and MathCheck-GEO for evaluating mathematical textual and multi-modal reasoning capabilities, respectively."}
{"id": "ZsP3YbYeE9", "Context": "A common method for developing agents with Language Models involves iteratively prompting the model, reflecting on its outputs, and updating the prompts until the task is completed. However, this approach faces challenges such as limited exploration of the decision space due to repetitive reflections and an inability to utilize insights from previously solved tasks.", "Idea": "Introduce DoT (Diversity of Thoughts), a framework that reduces redundant reflections to improve decision-space exploration and incorporates a task-agnostic memory component for knowledge retrieval from previously solved tasks, enhancing the agent's performance across various reasoning tasks."}
{"id": "I4e82CIDxv", "Context": "Previous methods for explaining language model behaviors relied on circuits composed of polysemantic and difficult-to-interpret units such as attention heads or neurons. These circuits were not suitable for many downstream applications due to their complexity and lack of interpretability.", "Idea": "Introduce sparse feature circuits, which are causally implicated subnetworks of human-interpretable features, to provide a detailed understanding of neural network mechanisms. These circuits are based on fine-grained units, making them useful for downstream tasks, such as improving classifier generalization by ablating task-irrelevant features. Additionally, an unsupervised and scalable interpretability pipeline is demonstrated by discovering thousands of sparse feature circuits for automatically discovered model behaviors."}
{"id": "pHe4P1IVnb", "Context": "As large language models become more complex, the challenge arises in adapting alignment techniques, especially when human supervision is limited. In scenarios where weak supervision is used, it becomes crucial to effectively harness the capabilities of stronger models.", "Idea": "Extend the Weak-to-Strong framework to WeakS-to-Strong by using an ensemble of weak models to simulate human opinion variability, employing a Bayesian approach for confidence scoring, and applying direct preference optimization to enhance preference learning in text generation tasks."}
{"id": "Pj4Aid3XqL", "Context": "Pre-trained large language models (LLMs) that are further trained with image data have shown strong performance on vision-language tasks. However, there is uncertainty about the effectiveness of this two-step training process compared to vision-language models (VLMs) that integrate images earlier in the training process. This raises questions about the optimal timing and method for introducing visual data during model training.", "Idea": "Investigate the impact of introducing visual tokens at different stages of pre-training by training models across various datasets, scales, and image-text ratios. Evaluate these models on a range of vision-language and text-only tasks to determine the optimal strategy for integrating visual data into pre-trained LLMs."}
{"id": "B2Fqu7Y2cd", "Context": "Large language models (LLMs) trained on text can infer instructions directly from data, but models trained solely on audio data lack this capability because audio data does not inherently contain the instructions used to generate it. This presents a challenge in creating audio synthesis and transformation models that can follow free-form text instructions.", "Idea": "Introduce a specialized dataset generation approach optimized for audio generation and transformation tasks, revealing meaningful relationships between audio and language. Additionally, propose ComposableART, an inference-time technique that extends classifier-free guidance to compositional guidance, enabling flexible composition of instructions for customizable audio outputs."}
{"id": "MWHIIWrWWu", "Context": "Controlling high-dimensional nonlinear systems, such as those in biological and robotic applications, is difficult due to the large state and action spaces involved. Although deep reinforcement learning has been successful in these areas, it is often computationally demanding and time-consuming, making it unsuitable for handling large sets of tasks that require extensive manual tuning.", "Idea": "Introduce Model Predictive Control with Morphology-aware Proportional Control (MPC$^2$), a hierarchical model-based learning algorithm designed for zero-shot and near-real-time control of high-dimensional complex dynamical systems. MPC$^2$ employs a sampling-based model predictive controller for target posture planning and incorporates a morphology-aware proportional controller for actuator coordination, facilitating robust control in high-dimensional tasks."}
{"id": "cmYScmfu4Q", "Context": "Reward inference is a crucial step in the Reinforcement Learning from Human Feedback (RLHF) process for fine-tuning Large Language Models. However, RLHF encounters challenges like distribution shift, reward model overfitting, and problem misspecification. Direct Preference Optimization (DPO) offers a simpler alternative by optimizing policies directly without reward inference, but it is limited to specific settings like bandits or deterministic MDPs.", "Idea": "Develop two RLHF algorithms that bypass reward inference for broader RL problems, using human preferences to estimate local value function differences and approximate policy gradients with a zeroth-order gradient approximator."}
{"id": "6HcnC3pPkp", "Context": "The rapid advancement of test-time compute search strategies has highlighted the need for robust verifiers to enhance the mathematical problem-solving capabilities of large language models (LLMs). Current inference strategies depend on verifiers designed for Best-of-N search, which are not optimal for tree search techniques. These existing verifiers provide indirect assessments of partial solutions and may undervalue promising intermediate steps, leading to premature pruning during tree search.", "Idea": "Introduce token-supervised value models (TVMs), a new class of verifiers that assign probabilities to each token, indicating the likelihood of reaching the correct final answer. This token-level supervision allows TVMs to directly evaluate partial solutions, effectively distinguishing between promising and incorrect intermediate steps during tree search."}
{"id": "BAelAyADqn", "Context": "Longitudinal human behavior modeling is crucial for applications like patient monitoring and lifestyle recommendations, using data from devices such as smartphones and smartwatches. This field aims to predict health outcomes based on time series of individual behaviors. However, existing models often lack accuracy and fail to consider realistic data aspects like diverse feature types and high missing value rates, as well as resource consumption issues, making their practical application questionable.", "Idea": "Introduce MuHBoost, a multi-label boosting method that uses advanced large language model prompting and multi-label classification to predict multiple health outcomes. To address the issue of LLMs hallucinating when handling multiple questions, two variants of MuHBoost are developed to improve predictive performance."}
{"id": "svp1EBA6hA", "Context": "Diffusion models are advanced generative models that enable precise control over generated samples. Despite their success when trained on large datasets, there is a need for additional controls during downstream fine-tuning processes, treating these models as pre-trained diffusion models.", "Idea": "Introduce a novel method called CTRL, which uses reinforcement learning to add controls to pre-trained diffusion models. This method formulates the task as an RL problem, using an offline dataset for learning a classifier and employing KL divergence against pre-trained models as reward functions, enabling sampling from the conditional distribution with additional controls during inference."}
{"id": "l2zFn6TIQi", "Context": "The deployment of large generative models has led to concerns regarding their reliability, safety, and potential misuse. Recent efforts have focused on controlling model generation by steering model activations to manage the emergence of certain concepts or behaviors in the output.", "Idea": "Introduce Activation Transport (AcT), a framework that uses optimal transport theory to steer model activations, offering fine-grained control over model behavior with minimal computational overhead and impact on model abilities."}
{"id": "FpiCLJrSW8", "Context": "The trustworthiness of Large Language Models (LLMs) is a critical factor that encompasses the reliability, safety, and ethical alignment of their outputs. Reinforcement Learning From Human Feedback (RLHF) is commonly used to align LLMs with human preferences, but its impact on trustworthiness has not been thoroughly evaluated. This study examines the performance of models aligned with general-purpose preference data across five trustworthiness dimensions: toxicity, stereotypical bias, machine ethics, truthfulness, and privacy.", "Idea": "Propose the adaptation of efficient influence function-based data attribution methods to the RLHF setting to better understand the impact of fine-tuning data on trustworthiness benchmarks, demonstrating feasibility through estimated attribution scores."}
{"id": "5WEpbilssv", "Context": "High-content perturbation experiments offer detailed insights into biomolecular systems but are hindered by high costs and complex analysis. Current machine learning approaches fail to capture the semantic depth of biological data and are not well-aligned with biological analysis goals.", "Idea": "Propose PerturbQA, a benchmark designed for structured reasoning over perturbation experiments, focusing on predicting differential expression and gene set enrichment for unseen perturbations. Introduce Summer, a domain-informed LLM framework that effectively addresses these challenges."}
{"id": "9OfKxKoYNw", "Context": "Recent advancements in diffusion models have enabled text-guided image manipulation, allowing users to create realistic edited images with simple textual prompts. However, there is a significant concern about the misuse of these methods, particularly in creating misleading or harmful content. Existing defense strategies, which use imperceptible adversarial noise to induce model failure, are ineffective against sophisticated manipulations like editing with a mask.", "Idea": "Propose DiffusionGuard, a defense method against unauthorized edits by diffusion-based image editing models, which introduces a novel objective generating adversarial noise targeting the early stage of the diffusion process. Additionally, implement a mask-augmentation technique to enhance robustness against various masks during test time."}
{"id": "oZkqkkvdND", "Context": "Variational Autoencoders (VAEs) are increasingly used in safety-critical applications where it is essential to provide certified probabilistic guarantees on performance, especially under adversarial attacks.", "Idea": "Propose a novel method called CIVET for certified training of VAEs, which involves bounding the worst-case VAE error by focusing on carefully chosen support sets at the latent layer, and develop a new training algorithm based on this insight."}
{"id": "Pujt3ADZgI", "Context": "Reinforcement Learning with Human Feedback (RLHF) has been successful in aligning large language models with human preferences. However, existing RLHF methods are primarily reward-based and rely on the Bradley-Terry model, which may not fully capture the complexity of human preferences.", "Idea": "Propose a novel approach to RLHF using a game-theoretic perspective by formulating it as a two-player game and introducing an online algorithm called iterative Nash policy optimization (INPO). This method allows the policy to play against itself using no-regret learning, avoiding the need for estimating expected win rates and instead minimizing a new loss objective over a preference dataset."}
{"id": "9HK2rHNAhd", "Context": "Optimizing the Key-Value (KV) cache in Large Language Models (LLMs) is crucial for reducing inference costs. Existing KV-cache compression methods often focus on sparsifying token sequences by leveraging token importance but typically allocate the same KV budget across all layers, which is inefficient as different layers have varying sensitivity to input tokens.", "Idea": "Propose a system that optimizes KV-cache allocation by identifying the importance of attention layers, allowing for joint optimization across sequence-wise and layer-wise dimensions. This involves measuring each layer's importance through cosine similarity of input prompt differences and adjusting KV budgets accordingly, incorporating sequence-wise algorithms for layer-specific compression."}
{"id": "Mfnh1Sqdwf", "Context": "Predicting gene expressions from DNA sequences is a complex task due to the challenge of identifying regulatory elements that control gene expressions. Understanding the causal relationship between epigenomic signals, DNA sequences, and their regulatory elements is crucial for accurate prediction.", "Idea": "Introduce Seq2Exp, a Sequence to Expression network designed to discover and extract regulatory elements that influence gene expression. The approach involves decomposing epigenomic signals and DNA sequences based on causal active regulatory elements and applying an information bottleneck with the Beta distribution to filter out non-causal components."}
{"id": "pbre0HKsfE", "Context": "Large language models (LLMs) can generate personalized responses based on user interactions, but this capability raises significant privacy concerns. Homomorphic encryption (HE) offers a cryptographic solution for privacy-preserving machine learning by allowing computations on encrypted data. However, the computational demands of transformers make it challenging to apply HE to LLMs effectively.", "Idea": "Propose a modified transformer architecture that is compatible with homomorphic encryption, focusing on inference after personalized fine-tuning. This approach uses LoRA fine-tuning and Gaussian kernels to achieve substantial computational speedups while maintaining performance similar to models operating on plaintext data."}
{"id": "WeJEidTzff", "Context": "Commuting Origin-Destination (OD) flows are essential for urban planning and transportation, providing insights into the movement of populations between residential and work areas. Collecting this data is costly, leading researchers to develop models that generate commuting OD flows using available urban attributes like sociodemographics and points of interest. However, the diversity in modeling techniques and evaluation metrics has made it difficult to establish a unified standard for comparing model performance.", "Idea": "Introduce a large-scale dataset containing commuting OD flows for 3,333 areas across the United States, enabling benchmarking of widely used models for commuting OD flow generation. This dataset reveals that network-based generative models perform optimally in terms of precision and generalization, suggesting new research directions in graph generative modeling."}
{"id": "kiOxNsrpQy", "Context": "Graph Neural Networks (GNNs) are increasingly used, necessitating reliable tools to explain their predictions. A key requirement for these explanations is faithfulness, meaning they accurately reflect the GNN's reasoning process. However, multiple faithfulness metrics exist, leading to confusion about what constitutes faithfulness and how to achieve it.", "Idea": "Demonstrate that existing faithfulness metrics are not interchangeable and can overlook important explanation properties. Prove that optimizing for faithfulness is not always beneficial, especially for injective regular GNN architectures, and explore the relationship between architectural choices and faithfulness, highlighting its connection to out-of-distribution generalization."}
{"id": "Kpjvm2mB0K", "Context": "The study focuses on one-pass streaming algorithms for underdetermined ℓp linear regression problems, where the matrix A has more columns than rows, and the columns arrive in a stream. This problem generalizes basis pursuit and least squares solutions and is relevant in scenarios like edge insertion graph streams, capturing various flow problems on undirected graphs. The challenge is to design algorithms that use significantly less space than the entire data stream.", "Idea": "Develop a streaming algorithm that constructs a sparse instance of the ℓp regression problem, approximating the cost with reduced space requirements. For p in [2, ∞], the algorithm uses space proportional to the number of columns, while for p in (1, 2), it adapts based on the H\"older conjugate exponent. Additionally, provide sublinear space upper bounds for outputting approximations of the solution, achieving efficient space usage for different values of p."}
{"id": "eENHKMTOfW", "Context": "The emergence of large language models (LLMs) has led to a significant gap between industrial research labs, which have the resources to fine-tune these models, and individual developers or small organizations, who lack the necessary resources to explore the experiment space effectively. This disparity is due to the high computational demands and expertise required for fine-tuning LLMs.", "Idea": "Conduct a comprehensive study on supervised fine-tuning of small-sized LLMs (3B to 7B parameters) using instruction-tuning datasets across diverse domains. The study explores various training configurations and strategies on open-source pre-trained models, providing detailed documentation and challenging common training practices to offer guidance for practitioners."}
{"id": "TEkoMEjf7E", "Context": "Generative 3D modeling has recently advanced but faces challenges due to its inherently ill-posed nature, affecting quality and controllability. Designers often refer to existing 3D models when creating new ones, highlighting the need for improved methods in 3D generation.", "Idea": "Introduce Phidias, a novel generative model that uses diffusion for reference-augmented 3D generation, leveraging a retrieved or user-provided 3D reference model to guide the process. It integrates meta-ControlNet for dynamic conditioning, dynamic reference routing to address misalignment, and self-reference augmentations for self-supervised training."}
{"id": "EV7FMBZxnx", "Context": "Detecting concealed objects, such as in vivo lesions or camouflage, requires specialized imaging systems. Lensless cameras, which are compact and flexible, present a promising alternative to traditional bulky lens systems. However, the lack of lenses results in measurements that lack visual semantics, creating significant challenges for concealed object detection.", "Idea": "Propose a region gaze-amplification network (RGANet) to progressively exploit concealed objects from lensless imaging measurements. This includes a region gaze module (RGM) to mine spatial-frequency cues and a region amplifier (RA) to enhance the details of object regions, improving concealed object detection performance."}
{"id": "21rSeWJHPF", "Context": "Ranking vertices in a graph is a fundamental task in computer science, often using centrality measures like PageRank. Traditional ranking algorithms can produce unbalanced rankings, especially in graphs with underlying communities, leading to information loss, polarized opinions, and reduced diversity.", "Idea": "Introduce a new approach called 'relative centrality' for unsupervised ranking on graphs, which involves an iterative graph-dependent local normalization of centrality scores to promote balancedness while maintaining ranking validity."}
{"id": "l0gZS0sAlf", "Context": "The training and fine-tuning of large language models (LLMs) often involve diverse textual data from multiple sources, which poses challenges due to conflicting gradient directions, hindering optimization and specialization. These challenges can undermine model generalization across tasks, resulting in reduced downstream performance. Recent research suggests that fine-tuning LLMs on carefully selected, task-specific subsets of data can match or even surpass the performance of using the entire dataset.", "Idea": "Propose the Ensembles of Low-Rank Expert Adapters (ELREA) framework, which clusters training instructions based on gradient directions to reduce conflicts during optimization. Expert adapters are trained on these clusters using the low-rank adaptation (LoRA) technique for efficiency and scalability. During inference, ELREA combines predictions from the most relevant expert adapters based on input data's gradient similarity to the training clusters."}
{"id": "mFY0tPDWK8", "Context": "In recent years, machine learning has been increasingly used to predict initial solutions for mixed-integer linear programming (MILP) problems. These methods typically involve predicting a solution and fixing a subset of variables to reduce the problem's dimensionality before solving the reduced problem to obtain final solutions. However, directly fixing variable values based on predictions can result in low-quality solutions or infeasible problems if the predictions are inaccurate.", "Idea": "Introduce the Apollo-MILP framework, which alternates between prediction and correction steps to improve solution accuracy. It uses a trust-region search to refine solutions and incorporates an Uncertainty-based Error upper BOund (UEBO) to evaluate prediction uncertainty, fixing only those values with high confidence."}
{"id": "mYgoNEsUDi", "Context": "Diffusion models have recently gained attention as a powerful tool for generative artificial intelligence on graphs, with applications in areas such as drug design and knowledge discovery. Despite their potential, current graph diffusion models struggle to comprehensively capture the intrinsic higher-order topological properties of graphs, which limits their generalizability and effectiveness for various tasks.", "Idea": "Introduce a computationally efficient topological summary called zigzag spaghetti (ZS) that extracts latent salient topological graph descriptors at different resolutions using zigzag persistence. This approach integrates dynamic topological information into graph diffusion models, providing theoretical stability guarantees and enhancing model performance and robustness."}
{"id": "LBl7Hez0fF", "Context": "Hallucination is a significant issue in deploying large vision-language models (LVLMs), often caused by misalignments between visual inputs and textual outputs. This problem is distinct from hallucination in large language models (LLMs) due to the unique structure of LVLMs, where text decoders are sensitive to vision inputs, especially when image encoders and text decoders are pre-trained separately.", "Idea": "Introduce Visual and Textual Intervention (VTI), a novel technique that reduces hallucinations by steering latent space representations during inference to enhance the stability of vision features. VTI is a task-agnostic, test-time intervention that can be applied without additional training costs."}
{"id": "dQ2xiSIYzp", "Context": "The task of learning 3D human Gaussians from a single image involves recovering detailed appearance and geometry, including unobserved regions. Existing methods face challenges in generating realistic human poses and shapes, and often struggle with inaccuracies in initial estimations.", "Idea": "Introduce a Human Gaussian Model (HGM) that uses a generate-then-refine pipeline guided by human body and diffusion priors. The model employs a ControlNet to refine back-view images and incorporates human priors from the SMPL-X model to improve pose and shape accuracy through sparse convolution and attention mechanisms."}
{"id": "uHLgDEgiS5", "Context": "Traditional data influence estimation methods, such as influence functions, assume that learning algorithms are permutation-invariant with respect to training data. However, modern training paradigms, particularly for foundation models using stochastic algorithms and non-convergent, multi-stage curricula, are sensitive to data ordering, violating this assumption. This mismatch makes influence functions inadequate for addressing questions about the influence of data at different training stages and its dependence on the optimization trajectory.", "Idea": "Introduce the concept of trajectory-specific leave-one-out (LOO) influence to quantify the impact of removing a data point from a specific iteration during training, considering the data sequence and optimization trajectory. Propose data value embedding, a technique for efficiently approximating trajectory-specific LOO by computing a training data embedding that captures interactions between data and model parameters, allowing approximation through a dot-product with the test data gradient."}
{"id": "lPJUQsSIxm", "Context": "The integration of fully homomorphic encryption (FHE) with machine learning presents opportunities for secure data processing, allowing computations on encrypted data while maintaining confidentiality. However, current FHE-based implementations for deep neural networks are hindered by high computational costs, latency, and scalability issues, which restrict their practical use.", "Idea": "Introduce DCT-CryptoNets, a method that operates in the frequency-domain using the discrete cosine transform (DCT) to alleviate the computational demands of non-linear activations and homomorphic bootstrap operations during private inference, enhancing efficiency and scalability for encrypted deep learning on high-resolution images."}
{"id": "rfdblE10qm", "Context": "The Bradley-Terry (BT) model is widely used in reward modeling for aligning Large Language Models (LLMs), despite its origins in multi-player stochastic game matching. The model's application in converting pairwise response comparisons to reward values and making predictions is not well understood, especially given the sparse comparison of prompt-response pairs.", "Idea": "Introduce a theoretical foundation for the convergence rate of BT reward models using deep neural networks with embeddings, and propose an alternative order-consistent reward modeling objective using a simple upper-bound algorithm compatible with binary classifiers."}
{"id": "W2Wkp9MQsF", "Context": "Model compression is a critical task in deploying large-scale neural networks in resource-constrained environments. Traditional compression techniques often require access to training data and fine-tuning, which can be impractical or impossible in certain scenarios. Existing data-free methods struggle to maintain model performance, especially at high sparsity levels.", "Idea": "Introduce model folding, a data-free compression technique that merges structurally similar neurons across layers using k-means clustering, preserving data statistics and preventing variance issues without needing training data or fine-tuning."}
{"id": "sLKDbuyq99", "Context": "Multi-agent frameworks utilizing large language models have been successful in automated planning and task execution. However, there is a lack of study on effectively adjusting agentic workflows during execution, which is crucial for adapting to unforeseen challenges and changing conditions in real-world scenarios.", "Idea": "Define workflows as an activity-on-vertex (AOV) graph to enable continuous refinement by LLM agents through dynamic subtask allocation adjustments. Emphasize modularity in workflow design to enhance performance by evaluating parallelism and dependency complexity, allowing efficient concurrent execution and improved error tolerance."}
{"id": "9OJflnNu6C", "Context": "Generative models have advanced significantly but pose issues like privacy breaches and biases. Machine unlearning has been proposed to address these concerns by removing specific training data from models. In the context of Image-to-Image (I2I) generative models, previous approaches have treated unlearning as a single objective optimization problem, which does not account for diverse user expectations regarding the balance between unlearning and model utility.", "Idea": "Propose a controllable unlearning framework for I2I generative models using a control coefficient to manage the trade-off between unlearning and utility. Reformulate the unlearning problem into a constrained optimization problem, solved with a gradient-based method to achieve Pareto optimal solutions within defined boundaries for the control coefficient."}
{"id": "zjeHLSiNv1", "Context": "Transformer models' performance is known to be logarithmically related to their number of parameters and computational complexity. Although Mixture of Experts (MoE) approaches attempt to separate parameter count from computational complexity, they encounter difficulties during inference due to high memory access costs.", "Idea": "Introduce UltraMem, a large-scale, ultra-sparse memory layer designed to reduce inference latency while maintaining model performance, and explore its scaling laws to demonstrate superior scaling properties compared to MoE."}
{"id": "hgwGi81ndj", "Context": "Reinforcement learning often faces challenges with difficult exploration problems, where agents struggle to efficiently learn and predict future states. Traditional methods may not effectively handle the complexity of state and action spaces, leading to inefficient learning processes.", "Idea": "Propose a fully model-based algorithm that utilizes object-centric mapping with hierarchical state and temporal abstraction, simplifying transition dynamics. This approach involves learning a discriminative world model, planning with a count-based intrinsic reward, and enabling efficient exploration and planning to reach discovered abstract states."}
{"id": "CI4sCBMXjP", "Context": "Enhancing the adaptive capabilities of large language models is a critical pursuit in both research and application. Traditional fine-tuning methods require substantial data, computational resources, and specific capabilities, while in-context learning is limited by the need for appropriate demonstrations and efficient token usage.", "Idea": "Propose ELICIT, a framework with two modules designed to store and reuse task vectors, enhancing the adaptive capabilities of models without additional training or inference tokens."}
{"id": "hJVdwBpWjt", "Context": "Large language models (LLMs) have achieved state-of-the-art performance in various auditory tasks, such as speech, music, and general audio, but their potential in bioacoustics tasks remains underexplored. These tasks, which include detecting animal vocalizations, classifying rare species, and labeling context and behavior, are vital for conservation, biodiversity monitoring, and animal behavior studies. The field faces challenges due to the limited availability of annotated data.", "Idea": "Introduce NatureLM-audio, the first audio-language foundation model specifically designed for bioacoustics, trained on a curated dataset of text-audio pairs from bioacoustics, speech, and music to address data scarcity. The model successfully transfers learned representations from music and speech to bioacoustics, demonstrating promising generalization to unseen taxa and tasks."}
{"id": "4GSOESJrk6", "Context": "Personalized image generation is a promising tool for assisting humans in daily tasks due to its ability to creatively generate personalized content. However, current evaluation methods either rely on automated systems that do not align well with human judgment or require human evaluations, which are time-consuming and costly.", "Idea": "Introduce DreamBench++, a human-aligned benchmark automated by advanced multimodal GPT models. This involves designing prompts for GPT to be both human-aligned and self-aligned, with task reinforcement, and constructing a comprehensive dataset of diverse images and prompts."}
{"id": "PkpNRmBZ32", "Context": "Traditional neural network architectures for audio processing tasks often rely on homogeneous configurations and specific types of operations like nonlinear recurrence, explicit convolutions, or attention mechanisms. These approaches can limit flexibility and efficiency in network design, particularly when balancing network size, performance, and computational resources.", "Idea": "Introduce Centaurus, a network architecture composed of generalized state-space model blocks, allowing tensor contractions to optimize training efficiency. This design incorporates a mixture of classical convolutional block inspirations, such as group convolutions and bottleneck blocks, to enhance flexibility and efficiency without relying on traditional recurrence, convolution, or attention mechanisms."}
{"id": "t8fu5m8R5m", "Context": "Anomaly Detection (AD) methods face challenges in robustness against adversarial attacks, which affects their reliability in critical applications like autonomous driving. This vulnerability is due to the AD setup, which typically uses only unlabeled normal samples for training, making detectors susceptible to adversarial anomalies during testing. Adversarial training is difficult to implement effectively without labeled data, as it requires an objective function that can maximize the margin between normal and anomaly distributions.", "Idea": "Propose creating a pseudo-anomaly group from normal samples and using adversarial training with contrastive loss as an objective function to enhance robustness. Address the issue of spurious negative pairs by defining opposite pairs and adversarially separating them to improve inter-group perturbations."}
{"id": "fGhr39bqZa", "Context": "Causal discovery with latent variables is a significant and complex problem. Most existing approaches depend on the assumption that latent variables have pure children, which can be limiting in practical scenarios and is not strictly necessary from a theoretical standpoint.", "Idea": "Introduce the concept of homologous surrogate to eliminate the need for pure children in causal discovery with latent variables. Formulate two assumptions involving homologous surrogates and develop theoretical results under each, leading to an algorithm that utilizes these properties for causal graph recovery."}
{"id": "4ua4wyAQLm", "Context": "Video anomaly detection (VAD) seeks to identify new actions or events that were not observed during training. Current VAD methods often concentrate on global patterns with excessive details, which hampers their ability to generalize to unseen samples.", "Idea": "Propose a framework that focuses on identifying and modeling local patterns to generalize to novel samples, using a two-stage process of image-text alignment and cross-modality attention. Introduce a State Machine Module (SMM) to enhance local patterns with temporal clues, and employ temporal motion estimation to detect anomalies with unique spatial distributions or dynamics."}
{"id": "xPxHQHDH2u", "Context": "Recent advancements in novel view synthesis have been driven by NeRF- and 3DGS-based methods. Despite these advancements, reconstructing reflective objects remains a challenge, as there is no existing solution that provides real-time, high-quality rendering while handling inter-reflection effectively.", "Idea": "Introduce the Reflective Gaussian splatting (Ref-Gaussian) framework, which includes a physically based deferred rendering approach using split-sum approximation for pixel-level material properties, and a Gaussian-grounded inter-reflection function within a Gaussian splatting paradigm. Additionally, enhance geometry modeling with material-aware normal propagation and an initial per-Gaussian shading stage using 2D Gaussian primitives."}
{"id": "i3e92uSZCp", "Context": "Skill discovery methods allow agents to learn a variety of behaviors without needing explicit rewards. Achieving a semantically diverse set of skills is important for their applicability in downstream tasks. Existing methods either use discriminators to differentiate skills or focus on expanding state coverage, but the direct pursuit of semantic diversity in skills is not well-explored.", "Idea": "Introduce Language Guided Skill Discovery (LGSD), a framework that uses large language models to enhance semantic diversity in skill discovery. LGSD utilizes user prompts to constrain the search space and guide agents to explore semantically diverse states, resulting in a set of semantically distinctive skills."}
{"id": "ws5phQki00", "Context": "Stance detection is crucial for enhancing online political discussions, aiding in content moderation, topic summarization, and fostering balanced debates. Traditionally, transformer-based models are used for stance detection, but they require large datasets, which are difficult to gather due to the diverse range of debate topics. Although large language models (LLMs) have revitalized stance detection, their deployment in online discussions is hindered by issues such as inconsistent outputs, biases, and susceptibility to adversarial attacks.", "Idea": "Utilize LLM-generated synthetic data to enhance stance detection in online political discussions by employing traditional stance detection models for deployment and leveraging LLMs for secure offline synthetic data generation. This involves generating synthetic data with a Mistral-7B model for specific debate questions and fine-tuning models with this data to improve performance. Additionally, by using synthetic data as a reference, the approach identifies and fine-tunes with the most informative samples from an unlabelled dataset to further enhance model performance."}
{"id": "t8KLjiFNwn", "Context": "Transformer models have been extensively explored for their ability to handle long-range dependencies and provide global contextual awareness, which has been crucial for the development of AI applications like ChatGPT, Gemini, and Alexa. State Space Models (SSMs) have emerged as strong competitors in sequential modeling by incorporating a selective mechanism for dynamic parameter adjustment, enhancing performance. However, this mechanism increases computational complexity and bandwidth demands, posing challenges for deployment on resource-constrained mobile devices.", "Idea": "Propose a sparse learning framework that integrates architecture-aware compiler optimizations, introducing an end-to-end solution called C4^n kernel sparsity, which prunes n elements from every four contiguous weights. Develop a compiler-based acceleration solution to ensure execution efficiency for this sparsity on mobile devices, and propose C4^n-specific optimizations combined with a layout transformation elimination strategy to improve performance across operations."}
{"id": "Bp0HBaMNRl", "Context": "Discovering causal structures with latent variables from observational data is a fundamental challenge in causal discovery. Existing methods often rely on constraint-based, iterative discrete searches, limiting their scalability for large numbers of variables. Moreover, these methods frequently assume linearity or invertibility, restricting their applicability to real-world scenarios.", "Idea": "Present new theoretical results on the identifiability of non-linear latent hierarchical causal models, relaxing previous assumptions about the deterministic nature of latent variables and exogenous noise. Develop a novel differentiable causal discovery algorithm that efficiently estimates the structure of such models, marking the first differentiable causal discovery method for non-linear latent hierarchical models."}
{"id": "2IoFFexvuw", "Context": "Recent advancements in reinforcement learning have been successful in fine-tuning diffusion-based generative models. However, challenges persist in fine-tuning continuous flow-based generative models to align with arbitrary user-defined reward functions. These challenges include policy collapse from overoptimization and the high computational cost of likelihoods in continuous-time flows.", "Idea": "Propose an RL fine-tuning method called Online Reward-Weighted Conditional Flow Matching with Wasserstein-2 Regularization (ORW-CFM-W2), which integrates RL into the flow matching framework to fine-tune generative models with arbitrary reward functions. The method introduces an online reward-weighting mechanism to prioritize high-reward regions and incorporates Wasserstein-2 distance regularization to prevent policy collapse and maintain diversity."}
{"id": "izjNI5bcOV", "Context": "The Earth's weather system is complex, involving various data modalities and tasks that are crucial for human life. Current data-driven models typically focus on individual weather understanding tasks, such as weather forecasting, but struggle to address multiple complex tasks within a single model. Additionally, these models are limited by their reliance on a small set of real observations for specific scenarios, which restricts their performance potential.", "Idea": "Introduce the WeatherGFM, a generalist foundation model for weather understanding tasks, which unifies the representation and definition of diverse tasks. It employs weather prompt formats to manage different data modalities and uses a visual prompting question-answering paradigm for training, enabling it to handle multiple tasks in a unified manner."}
{"id": "5IWJBStfU7", "Context": "As AI systems are increasingly used in critical applications, ensuring their interpretability is crucial. Mechanistic Interpretability (MI) seeks to reverse-engineer neural networks to extract human-understandable algorithms that explain their behavior. A fundamental question arises: for a fixed behavior, can MI guarantee a unique explanation? This is analogous to the concept of identifiability in statistics, which ensures the uniqueness of inferred parameters under specific assumptions.", "Idea": "Introduce two strategies for producing MI explanations: 'where-then-what', which identifies a network subset replicating behavior before deriving its interpretation, and 'what-then-where', which starts with candidate algorithms and searches for their implementation in the network's activation subspaces, using causal alignment between algorithm states and the network."}
{"id": "z8PcUSKXXN", "Context": "Recent advancements in deep image denoising have led to the development of models capable of handling various noise types. The current state-of-the-art method, Masked Training (MT), uses a masked swinir model trained on Gaussian noise, achieving good performance across different noise types. However, MT often results in over-smoothed images and presents challenges in optimizing mask ratios, complicating integration with other methods.", "Idea": "Introduce RNINet, a novel architecture based on a streamlined encoder-decoder framework, which includes a noise injection block to inject random noise into feature statistics, enhancing generalization across unseen noise types and simplifying the complexity found in MT."}
{"id": "yitH9xAHQs", "Context": "Large language models (LLMs) have achieved impressive performance by training on diverse, high-quality task-specific data. Current methods often depend on human-annotated data or predefined task templates to guide LLMs in generating relevant data for effective training. This reliance on manually designed components may limit the scope of generated data, potentially missing critical edge cases or novel scenarios that could challenge the model.", "Idea": "Introduce ReverseGen, a novel approach that automatically generates training samples to expose LLM weaknesses by using a dedicated proposer to create queries that lead models to produce unsatisfactory responses. These queries are used to construct training data, addressing model shortcomings and improving performance across various scales and applications."}
{"id": "PUnD86UEK5", "Context": "Adam optimizer is known to outperform Stochastic Gradient Descent (SGD) in training language models, but the theoretical understanding of this advantage is limited. Previous analyses of convergence for both Adam and SGD have focused on the number of steps and have been optimal in non-convex scenarios, with both achieving a convergence rate of approximately O(T^-1/4).", "Idea": "Propose a new convergence analysis for Adam that leverages the smoothness of loss under the ℓ∞-geometry, rather than the traditional ℓ2-geometry, resulting in a better empirical smoothness constant for models like GPT-2 and ResNet. Extend this analysis to blockwise Adam with novel blockwise smoothness assumptions."}
{"id": "pPQPQ7Yd58", "Context": "In image-based control pipelines learned from behavior cloning, the geometry of the visual representation space serves as the information channel between the vision encoder and the action decoder. Inspired by neural collapse in image classification, there is an observed phenomenon where visual representations cluster according to action labels in discrete control tasks and according to control-oriented classes in continuous control tasks.", "Idea": "Leverage the observed law of clustering as an algorithmic tool by pretraining the vision encoder using neural collapse as a regularization technique to encourage control-oriented clustering of visual features, thereby improving test-time performance when training a policy with limited expert demonstrations."}
{"id": "6qUUgw9bAZ", "Context": "Decoding procedures such as search, reranking, and self-critique are computationally intensive but can enhance the quality of language model outputs in tasks like code generation, numerical reasoning, and dialog. Traditionally, the same decoding procedure is applied uniformly to all inputs, regardless of the varying computational needs of different inputs.", "Idea": "Develop an approach that predicts the distribution of rewards based on input and computation budget, allowing for adaptive allocation of computation. This includes an adaptive best-of-k procedure for dynamic sample generation and a routing procedure that chooses between expensive, accurate decoding and cheaper, less capable alternatives."}
{"id": "hXm0Wu2U9K", "Context": "Language model alignment methods, such as reinforcement learning from human feedback (RLHF), have significantly improved language model capabilities. However, these methods face a challenge known as overoptimization, where the model's quality deteriorates during the alignment process due to overfitting to inaccuracies in the offline reward model. Existing methods use KL-regularization to mitigate this issue, but it remains insufficient to prevent performance degradation.", "Idea": "Introduce a new algorithm called $χ^2$-Preference Optimization ($χ$PO), which modifies the logarithmic link function in Direct Preference Optimization (DPO). This change incorporates regularization with the $χ^2$-divergence, effectively implementing the principle of pessimism in the face of uncertainty, and provides sample-complexity guarantees based on single-policy concentrability, making it robust to overoptimization."}
{"id": "d8hYXbxX71", "Context": "Improving social welfare involves optimizing policy objectives across different time horizons, which is challenging because policies that seem suboptimal in the short term may have significant long-term benefits. Rawlsian policies prioritize those with the greatest need, while utilitarian policies focus on maximizing immediate welfare gains. These approaches are traditionally seen as conflicting, with Rawlsian policies thought to reduce average social welfare compared to utilitarian ones.", "Idea": "Analyze Rawlsian and utilitarian policies within a sequential decision-making framework where individuals' welfare levels decay over time, and policymakers can intervene. Prove that under certain conditions, Rawlsian policies can outperform utilitarian policies in the long run, despite utilitarian policies being more effective in the short term."}
{"id": "G0dksFayVq", "Context": "LLM-based judges are increasingly used as scalable alternatives to human evaluation for assessing and improving models. However, their reliability is often not thoroughly examined, especially as LLMs become more sophisticated. Existing benchmarks mainly focus on alignment with human preferences, which may not be suitable for tasks requiring factual and logical correctness.", "Idea": "Propose a novel evaluation framework and introduce JudgeBench, a benchmark designed to evaluate LLM-based judges on challenging tasks across knowledge, reasoning, math, and coding. JudgeBench uses a new pipeline to convert difficult datasets into response pairs with preference labels that reflect objective correctness."}
{"id": "vJkktqyU8B", "Context": "Current Vision Transformer (ViT) adapter methods, while achieving promising accuracy, suffer from inefficient memory access operations such as standard normalization and frequent reshaping, which hinder inference speed.", "Idea": "Propose META, a ViT adapter that enhances memory efficiency and reduces memory time consumption by minimizing inefficient memory access operations. It features a memory-efficient adapter block that shares layer normalization between self-attention and feed-forward network layers, employs cross-shaped self-attention to reduce reshaping operations, and includes a lightweight convolutional branch to enhance local inductive biases for dense prediction tasks."}
{"id": "SiH7DwNKZZ", "Context": "Transformers have become a standard backbone in computer vision, although they were originally designed for natural language processing. The Long Short-Term Memory (LSTM) architecture has been recently enhanced to a more scalable and efficient version known as xLSTM, which addresses traditional LSTM limitations through exponential gating and a parallelizable matrix memory structure.", "Idea": "Introduce Vision-LSTM (ViL), an adaptation of xLSTM for computer vision, consisting of a stack of xLSTM blocks that alternately process sequences of patch tokens from top to bottom and bottom to top."}
{"id": "9NfHbWKqMF", "Context": "3D Gaussian Splatting (3DGS) has recently advanced photorealistic reconstruction, offering high visual fidelity and real-time performance. However, a significant challenge arises as rendering quality declines when test views differ from the camera angles used during training, which is problematic for immersive free-viewpoint rendering and navigation. Existing methods, even those with regularization techniques and data-driven priors, struggle to generalize effectively to out-of-distribution (OOD) views.", "Idea": "Introduce SplatFormer, the first point transformer model designed to operate on Gaussian splats, which refines an initial 3DGS set optimized under limited training views in a single forward pass, effectively removing artifacts in OOD test views."}
{"id": "84WmbzikPP", "Context": "Molecular structure elucidation is crucial for understanding chemical phenomena and has applications in various fields such as natural products, lab syntheses, forensic samples, and the interstellar medium. The task involves predicting a molecule's 3D structure from its molecular formula and moments of inertia, which can be measured with high precision using rotational spectroscopy. Existing generative models can sample 3D structures with approximately correct moments, but they do not fully utilize the precision available from experimental data.", "Idea": "Propose Stiefel Flow Matching as a generative model to predict 3D molecular structures under exact moment constraints by embedding the space of $n$-atom point clouds in the Stiefel manifold. This approach includes learning simpler and shorter flows through approximate solutions for equivariant optimal transport on the Stiefel manifold."}
{"id": "9FqARW7dwB", "Context": "Residual connections are commonly used in neural networks to address issues like gradient vanishing and representation collapse. However, these connections often suffer from drawbacks such as the seesaw effect, which can hinder the performance of deep learning models.", "Idea": "Introduce hyper-connections, a method that allows networks to adjust connection strengths between features at different depths and dynamically rearrange layers, serving as an alternative to residual connections."}
{"id": "ho4mNiwr2n", "Context": "Anti-backdoor learning is a defense method designed to train clean models from datasets that have been compromised by backdoor attacks. Existing methods struggle to revert backdoored samples to their original labels and do not generalize well to large pre-trained models due to their non end-to-end training approach, making them inadequate for protecting large pre-trained models.", "Idea": "Introduce an end-to-end method called Mind Control through Causal Inference (MCCI) that uses both images and attack indicators to train clean models from poisoned datasets. This method allows the model to control its perception of inputs as clean or backdoored by using fake non-attack indicators, ensuring correct predictions even for poisoned samples."}
{"id": "WwmtcGr4lP", "Context": "Effective cancer treatment is challenging due to the individualized nature of patient responses, which are influenced by the heterogeneity of cancer-causing mutations across patient genomes. The limited availability of patient response data complicates the training of personalized treatment models. Existing methods use transfer learning with larger, labeled pre-clinical datasets to create a shared representation between cell line and patient domains, but they fail to capture patient-specific characteristics that significantly affect drug response.", "Idea": "Introduce GANDALF, a novel generative attention-based data augmentation and predictive modeling framework that directly augments patient genomic data while accounting for domain-specific characteristics, addressing the limitations of previous methods."}
{"id": "d8cnezVcaW", "Context": "Direct Preference Optimization (DPO) is a method used to enhance reinforcement learning from human feedback, particularly for fine-tuning large language models. However, DPO struggles to effectively capture the diversity of human preferences, which limits its applicability and performance.", "Idea": "Develop a new approach called MallowsPO, inspired by Mallows' theory of preference ranking, which introduces a dispersion index to reflect the diversity of human preferences. This approach unifies existing DPO models as special cases and uses the dispersion index to improve DPO's performance across various tasks."}
{"id": "MGKDBuyv4p", "Context": "Language models have the capability to memorize information from their training data, which can lead to the unintended regurgitation of verbatim data during inference. This poses a problem, especially when the data involved is private or sensitive.", "Idea": "Investigate and develop various methods to mitigate memorization in language models, including three regularizer-based, three fine-tuning-based, and eleven machine unlearning-based methods, with five new unlearning methods introduced. Additionally, introduce TinyMem, a suite of small, computationally-efficient language models designed for the rapid development and evaluation of these memorization-mitigation methods."}
{"id": "vmulbBDCan", "Context": "Electron-multiplying charge-coupled devices (EMCCDs) are crucial for sensitive observations in low-light conditions across various fields such as astronomy, material science, and biology. Despite their advanced designs to enhance target signals and mitigate read-out circuit noises, EMCCD images still suffer from noise, affecting outcomes in applications like fluorescence microscopy. Existing noise models for EMCCDs focus on theoretical statistical characteristics and have not integrated recent advancements in computational photography, which use physics-based noise models to guide deep learning for adaptive denoising in ordinary image sensors.", "Idea": "Introduce a systematic study on physics-based noise model calibration for EMCCD cameras, accurately estimating statistical features of noise components to generate authentic training samples for a recent neural network. Capture a real-world test image dataset for EMCCD, including both ordinary and microscopic scenes, to benchmark and demonstrate the advantages of the proposed model over previous methods."}
{"id": "iXCeQ2m6vT", "Context": "AI systems struggle with understanding visual relations, particularly when dealing with previously unseen objects, whereas humans excel at this task. Active vision theories suggest that learning visual relations is linked to eye movements that help fixate on objects and their parts, with spatial information from these movements aiding in representing relations between image parts.", "Idea": "Develop a system using Glimpse-based Active Perception (GAP) that sequentially focuses on salient regions of an image at high resolution, utilizing the locations from these glimpses and the surrounding visual content to represent relations between different image parts."}
{"id": "FoF5RaA3ug", "Context": "Recent advancements in dataset distillation have shown the benefits of using soft labels generated by pre-trained teacher models. However, the choice of loss function for utilizing these soft labels significantly affects the performance of models trained on synthetic datasets, indicating the need for a universal loss function.", "Idea": "Introduce GIFT, a simple plug-and-play approach that includes soft label refinement and a cosine similarity-based loss function to fully utilize label information, enhancing dataset distillation methods without additional computational costs."}
{"id": "R4h5PXzUuU", "Context": "The emergence of foundation models trained on large-scale internet data has led to their widespread adoption across various application domains. However, the trustworthiness of these models, particularly their out-of-distribution detection (OoDD) capabilities, remains underexplored. This gap raises concerns about the safe deployment of large vision-language models (LVLMs), such as GPT-4o, which are trained on extensive multi-modal data.", "Idea": "Introduce a self-guided prompting approach called Reflexive Guidance (ReGuide) to enhance the OoDD capabilities of LVLMs by using self-generated image-adaptive concept suggestions."}
{"id": "gU4ZgQNsOC", "Context": "Pretraining large language models on extensive and varied datasets is essential for achieving top performance in various tasks. However, current training methods treat all data samples equally, ignoring the individual importance or relevance of each sample during training. Existing strategies that reweight data focus on group-level importance and fail to utilize detailed instance-level information, lacking adaptability to the changing importance of samples as training progresses.", "Idea": "Introduce novel algorithms for dynamic, instance-level data reweighting that adjust the weight of each training sample based on its loss value in real-time. This approach allows the model to focus on more informative samples during training and deprioritize redundant data, supported by a new theoretical framework analyzing the impact of loss-based reweighting on optimization convergence."}
{"id": "f7KxfUrRSb", "Context": "Aligning language models with human preferences is a significant research focus, aiming to enhance the models' ability to meet diverse user needs. The concept of weak-to-strong generalization, where a strong language model fine-tuned on labels from a weaker model can outperform its supervisor, serves as an inspiration for improving model alignment.", "Idea": "Propose a method called Weak-to-Strong Preference Optimization (WSPO) that achieves strong model alignment by learning the distribution differences before and after the alignment of the weak model, effectively transferring and amplifying alignment behavior from weaker to stronger models."}
{"id": "oU3tpaR8fm", "Context": "Retrieval-augmented generation (RAG) allows large language models (LLMs) to leverage external knowledge sources, with the potential to improve output quality by processing longer input sequences. It is assumed that a larger retrieval set would enhance performance by providing more relevant information. However, empirical findings show that for many long-context LLMs, the quality of generated output initially improves but then declines as the number of retrieved passages increases, due to the presence of 'hard negatives'.", "Idea": "Propose both training-free and training-based approaches to mitigate the negative impact of 'hard negatives' in long-context LLM-based RAG. This includes retrieval reordering as a training-free optimization, and RAG-specific implicit LLM fine-tuning and RAG-oriented fine-tuning with intermediate reasoning as training-based methods, along with a systematic analysis of design choices for these methods."}
{"id": "iylpeTI0Ql", "Context": "Test-time adaptation (TTA) is designed to handle distribution shifts between source and target data by using only target data during testing. In open-world scenarios, models frequently encounter noisy samples that fall outside the in-distribution label space, which poses a challenge for existing TTA methods. These methods often experience a significant performance drop when dealing with noisy samples, as the negative impact of unfiltered noisy data can outweigh the benefits of clean data during model updating.", "Idea": "Introduce Zero-Shot Noisy TTA (ZS-NTTA), a framework that decouples the classifier and detector, focusing on developing an individual detector while keeping the classifier frozen. The Adaptive Noise Detector (AdaND) is proposed, which uses the frozen model's outputs as pseudo-labels to train a noise detector, effectively identifying noisy samples. Additionally, Gaussian noise is injected during adaptation to prevent misclassification of clean samples as noisy."}
{"id": "4rEI2JdHH6", "Context": "Grokking is a phenomenon in neural networks where models initially memorize training data and generalize poorly, but after extended training, they suddenly achieve near-perfect generalization. This delayed generalization is problematic as it affects predictability and efficiency, with the ideal scenario being immediate generalization without delay.", "Idea": "Propose GrokTransfer, a method that accelerates grokking by first training a smaller model to a suboptimal performance level, then using its learned input embedding to initialize a stronger model, thereby enabling direct generalization without delay."}
{"id": "vQhn4wrQ6j", "Context": "Model merging, such as model souping, involves combining models with the same architecture without additional training. Fine-tuning Large Language Models (LLMs) for specific tasks in non-English languages is challenging due to the lack of task-specific data, particularly for mathematical reasoning tasks.", "Idea": "Develop a model merging methodology that enhances cross-lingual transfer by composing language and math capabilities. This involves fine-tuning separate 'experts' on math instruction data in English and generic instruction data in the target language, then swapping the top and bottom transformer layers between these experts to improve math performance in the target language."}
{"id": "uREg3OHjLL", "Context": "The expressive power of ReLU neural networks is believed to increase with their depth. A specific function, $F_n = \\max (0,x_1,\\ldots,x_n )$, has been used to explore this relationship. A conjecture suggested that any ReLU network exactly representing $F_n$ requires at least $\\lceil \\log_2 (n+1) \\rceil$ hidden layers, a claim recently confirmed for networks with integer weights.", "Idea": "Investigate ReLU networks with decimal fraction weights, demonstrating that $F_n$ requires at least $\\lceil \\log_3 (n+1) \\rceil$ hidden layers. Additionally, for networks with $N$-ary fraction weights, establish a lower bound of $\\Omega( \\frac{\\ln n}{\\ln \\ln N})$ layers, partially confirming the conjecture for rational ReLU networks."}
{"id": "vr1QdCNJmN", "Context": "The Bregman divergence is a pseudo-distance commonly used for comparing vectors or functions in continuous spaces, generated from a convex function. However, defining a similar divergence for discrete spaces is challenging. Previous work by Iyer & Bilmes explored Bregman divergences on discrete domains using submodular functions, which are discrete analogs of convex functions.", "Idea": "Generalize the framework to include generating functions that are neither submodular nor supermodular, creating a more flexible divergence called the difference-of-submodular Bregman divergence. Introduce a learnable form of this divergence using permutation-invariant neural networks to capture structural properties in discrete data."}
{"id": "2e4ECh0ikn", "Context": "The recent development of audio foundation models (FMs) offers potential advancements in conversational modeling. However, there is a lack of comprehensive evaluation of these models' abilities to engage in natural and interactive conversations, particularly in managing turn-taking without excessive overlapping speech or prolonged silences.", "Idea": "Propose a novel evaluation protocol to assess spoken dialog systems' turn-taking capabilities using a supervised model trained to predict turn-taking events in human-human conversations. This protocol facilitates a comprehensive user study to evaluate existing systems and identify areas for improvement, with plans to open source the evaluation platform to advance conversational AI development."}
{"id": "QG31By6S6w", "Context": "Recent advancements in medical vision-language pre-training models have significantly improved zero-shot disease recognition. However, transferring image-level knowledge to pixel-level tasks like lesion segmentation in 3D CT scans is challenging due to the complexity and variability of pathological visual characteristics. Existing methods struggle to align fine-grained lesion features not encountered during training with disease-related textual representations.", "Idea": "Introduce Malenia, a multi-scale lesion-level mask-attribute alignment framework for 3D zero-shot lesion segmentation, which enhances compatibility between mask representations and their elemental attributes. It includes a Cross-Modal Knowledge Injection module to improve visual and textual features, guiding the generation of segmentation results."}
{"id": "X9OfMNNepI", "Context": "Scientific discovery is crucial for societal advancement, and recent developments suggest that large language models (LLMs) might accelerate this process. However, it remains uncertain if LLMs can autonomously generate novel and valid hypotheses in the field of chemistry. The research explores whether LLMs can automatically discover new chemistry research hypotheses from a given research question, based on the assumption that most chemistry hypotheses arise from a research background question and several inspirations.", "Idea": "Develop a benchmark using 51 high-level chemistry papers to test if LLMs can rediscover hypotheses from background questions and a corpus of inspiration papers. Implement an LLM-based multi-agent framework that operates in three stages to address the smaller questions of retrieving inspirations, forming hypotheses, and ranking them, thereby validating the ability of LLMs to generate hypotheses similar to the original ones."}
{"id": "keu6sxrPWn", "Context": "As large language models become more powerful, they also become harder to trust due to potential 'subversive misalignment,' where subtle errors bypass safety checks. This creates a challenge in deploying these models, as there is a tradeoff between ensuring safety and leveraging their capabilities. The uncertainty in model behavior increases the risk of safety failures over time.", "Idea": "Introduce the 'Diffuse Risk Management' problem, which aims to balance safety and usefulness in deploying untrusted models across multiple tasks. This is achieved through a two-level framework: micro-protocols at the single-task level using a trusted model to monitor the untrusted one, and a macro-protocol at the whole-scenario level that adaptively estimates risk to select appropriate micro-protocols."}
{"id": "2ZK8zyIt7o", "Context": "The rapid advancement of text-to-image (T2I) diffusion models has enabled them to generate unprecedented results from given texts. However, as text inputs become longer, existing encoding methods like CLIP face limitations, and aligning the generated images with long texts becomes challenging.", "Idea": "Propose LongAlign, which includes a segment-level encoding method for processing long texts and a decomposed preference optimization method for effective alignment training. This involves dividing long texts into segments for separate processing and using decomposed CLIP-based preference models to fine-tune diffusion models, with a reweighting strategy to reduce overfitting and enhance alignment."}
{"id": "RaR3ETzyKp", "Context": "Recent studies have found that different diffusion methods and architectures trained on the same dataset yield similar results when given the same input noise. This suggests the existence of preferable noises for specific samples. By visualizing noise-sample pairs in two-dimensional spaces, it was observed that preferable paths, which connect these noises to samples, are more organized and have fewer crossings compared to random paths. In high-dimensional spaces, paths rarely intersect, and crossings in two-dimensional spaces indicate shorter inter-path distances in high-dimensional spaces.", "Idea": "Propose the Distance-Aware Noise-Sample Matching (DANSM) method to increase inter-path distance, thereby accelerating model training. DANSM is based on rectified flow models, utilizing a closed-form formula to calculate inter-path distance, and simplifies optimization by relating inter-path distance to path length."}
{"id": "e8qXTxMgPg", "Context": "Dimensionality reduction for sparse vectors often relies on worst-case analysis, which can be overly conservative. Traditional methods focus on preserving vector norms across all vectors, but this can be inefficient, especially for sparse vectors. The challenge is to find more efficient embeddings that maintain essential properties of the vectors while reducing dimensionality.", "Idea": "The study explores beyond worst-case dimensionality reduction by providing average-case guarantees for embedding sparse vectors, establishing optimal lower bounds for linear maps, and demonstrating improved upper bounds for non-negative sparse vectors using non-linear embeddings. This approach leverages the non-negativity of vectors to achieve smaller embeddings and exact dimensionality reduction in certain norms, highlighting the necessity of non-linearity and non-negativity for these improvements."}
{"id": "Wvi8c0tgvt", "Context": "Existing blur datasets lack sufficient variety in scenes and blur patterns, making them inadequate for training. Expanding data diversity is challenging due to the complexity and effort required by dual-camera systems. Current data augmentation methods focus on 2D motion estimation, ignoring the 3D nature of camera and object movements, leading to unrealistic motion patterns.", "Idea": "Propose a 3D-aware blur synthesizer that generates diverse and realistic blur images for data augmentation by estimating 3D camera positions during motion blur intervals, generating scene images, and aggregating them. This approach allows for 3D transformation without explicit depth measurements by estimating a 3D residual component via a neural network, enabling controllable blur data augmentation."}
{"id": "c4OGMNyzPT", "Context": "Large Vision Language Models (LVLMs) have shown impressive capabilities in processing both visual and textual information. However, current evaluation methods, such as those based on Visual Question Answering and image captioning benchmarks, are insufficient in fully capturing the capabilities of LVLMs. These methods are limited by inadequate assessment of detailed visual perception, data contamination, and a lack of emphasis on multi-turn reasoning.", "Idea": "Propose LVLM-Playground, a game-based evaluation framework that comprehensively assesses LVLMs' cognitive and reasoning skills in structured environments. This framework evaluates LVLMs on four core tasks: Perceiving, Question Answering, Rule Following, and End-to-End Playing, each designed to test specific abilities like visual perception, reasoning, and decision-making."}
{"id": "bc3sUsS6ck", "Context": "Large language models (LLMs) acquire substantial knowledge during pretraining but often require adaptation to new contexts, tasks, or domains. This adaptation is typically achieved through fine-tuning, which incurs significant training costs, or prompting, which increases inference overhead.", "Idea": "Introduce GenerativeAdapter, a method that encodes test-time context into language model parameters with a single forward pass. It augments a frozen pretrained LM with a lightweight adapter generator, trained via self-supervised learning, to produce parameter-efficient adapters. The generator is general-purpose, capable of adapting the base model for all language processing scenarios."}
{"id": "rpouyo09V0", "Context": "Large language models (LLMs) are crucial for code generation, especially in interactive settings. However, current benchmarks do not adequately reflect the diverse feedback encountered in multi-turn interactions, hindering the evaluation of LLMs in these scenarios.", "Idea": "Introduce novel benchmarks that model feedback quality for code generation LLMs, including CONVCODEWORLD, an environment simulating interactive scenarios with various feedback types, and CONVCODEBENCH, a static version using pre-generated feedback logs to maintain evaluation efficiency."}
{"id": "0mtz0pet1z", "Context": "In preventive medicine and non-fatal health conditions, such as HIV infection without AIDS, the timing of treatment initiation is crucial. Traditional causal inference has focused on determining the optimal timing for treatment and its effects, which may depend on individual characteristics.", "Idea": "Propose a method to identify and estimate the incremental causal effect of intervening on the timing of treatment initiation using inverse probability weighting, without relying on the common positivity assumption."}
{"id": "u3TL0qxLWf", "Context": "Large Language Models (LLMs) have significantly advanced natural language processing but are hindered by high runtime costs, making widespread deployment challenging. These models require substantial memory access and computational resources, which limits their efficiency and scalability.", "Idea": "Introduce SeedLM, a post-training compression method that encodes model weights using seeds of a pseudo-random generator. This method employs a Linear Feedback Shift Register (LFSR) to generate random matrices during inference, which are combined with compressed coefficients to reconstruct weight blocks, reducing memory access and utilizing idle compute cycles without relying on calibration data."}
{"id": "4O0v4s3IzY", "Context": "There is ongoing debate about the reasoning capabilities of Large Language Models (LLMs). Initial hopes that reasoning would naturally emerge with increased scale have been challenged by various counterexamples. Despite this, many believe LLMs can iteratively self-critique and improve their solutions, based on the assumption that verifying correctness is easier than generating solutions, a notion rooted in computational complexity.", "Idea": "Conduct a systematic investigation into the effectiveness of iterative prompting for reasoning and planning, specifically examining GPT-4's performance in tasks like Game of 24, Graph Coloring, and STRIPS planning. The study explores both self-critique by the model and verification by an external reasoner, analyzing the impact of criticisms on performance and the effects of removing elements from the augmented system."}
{"id": "P4XmKjXTrM", "Context": "Reproducibility is a major challenge in machine learning for healthcare due to the private nature of datasets, model pipelines, and task or cohort definitions. This creates barriers in sharing, iterating, and understanding ML results on electronic health record datasets.", "Idea": "Introduce the Automatic Cohort Extraction System (ACES) for event-stream data, which simplifies the development and reproduction of tasks and cohorts in ML for healthcare. ACES offers a domain-specific configuration language for defining dataset-specific and agnostic criteria, and a pipeline to automatically extract patient records that meet these criteria from real-world data."}
{"id": "SgymXhOEA5", "Context": "Person re-identification (ReID) models often suffer from camera bias, which becomes more pronounced under data distribution shifts. Previous methods to address this bias have been limited to the training domains of the models. Additionally, unsupervised learning of ReID models shows a strong bias towards camera labels, even for seen domain data, indicating a need for improvement.", "Idea": "Revisit feature normalization on embedding vectors as a debiasing method for unseen domain data, analyzing its effectiveness in reducing bias related to low-level image properties and body angle. Propose simple training strategies to mitigate camera bias in unsupervised learning, demonstrating significant performance improvements with minor modifications to existing algorithms."}
{"id": "vh1e2WJfZp", "Context": "In high-resolution, fine-grained image segmentation, the main challenge is achieving a balance between broad contextual awareness and the precision needed for detailed object delineation. This involves capturing intricate details and the finest edges of objects. Diffusion models, trained on extensive datasets of image-text pairs, have transformed text-to-image synthesis by providing exceptional quality, fine detail resolution, and strong contextual awareness, making them a promising solution for high-resolution image segmentation.", "Idea": "Introduce DiffDIS, a diffusion-driven segmentation model that utilizes the pre-trained U-Net within diffusion models for high-resolution, fine-grained object segmentation. It leverages the generalization capabilities and image representation of SD models with a stable one-step denoising approach to reduce inference time while maintaining high-fidelity generation. An auxiliary edge generation task is also introduced to enhance the preservation of fine details and reconcile the probabilistic nature of diffusion with the deterministic demands of segmentation."}
{"id": "7YXaOvunqo", "Context": "Since the introduction of WGANs, there has been ongoing debate about whether their ability to generate realistic images is due to minimizing the Wasserstein distance between the distribution of generated images and the training distribution. The role of the discriminator architecture and its inductive biases in this process has been a point of contention.", "Idea": "Demonstrate that successful WGANs minimize the Wasserstein distance between patches in generated and training images, influenced by the discriminator's architecture. This is shown through theoretical and experimental results using discrete generators, where the Wasserstein distance can be computed exactly."}
{"id": "dj0TktJcVI", "Context": "Task arithmetic has gained attention for its ability to combine fine-tuned weights of various tasks into a unified model without additional training, offering efficiency and cost-effectiveness. However, this approach can cause interference from unrelated tasks due to a lack of weight disentanglement. Neural Tangent Kernel (NTK) linearization has been used to address this issue, but it increases training costs and reduces individual model performance.", "Idea": "Propose a method to finetune only the attention modules in the Transformer, which improves weight disentanglement by leveraging their kernel behavior. This approach differentiates the roles of the representation module and task-specific modules, highlighting the importance of the representation module in enhancing weight disentanglement."}
{"id": "cC3LxGZasH", "Context": "The Fréchet Video Distance (FVD) is a commonly used metric for assessing the quality of video generation distributions. However, it has several limitations, including the non-Gaussian nature of the I3D feature space, insensitivity to temporal distortions, and the need for impractically large sample sizes for reliable estimation. These issues compromise FVD's reliability as a standalone metric for video generation evaluation.", "Idea": "Propose JEDi, the JEPA Embedding Distance, which utilizes features from a Joint Embedding Predictive Architecture and is measured using Maximum Mean Discrepancy with a polynomial kernel, as a more reliable alternative to FVD for video generation evaluation."}
{"id": "OeKp3AdiVO", "Context": "In long-tailed recognition, the Decoupled Training paradigm has been effective by separating training into representation learning and classifier re-training. Previous efforts to enhance both stages simultaneously have made it difficult to isolate the impact of classifier re-training. Recent findings suggest that simple regularization can yield strong feature representations, prompting a reevaluation of classifier re-training methods.", "Idea": "Introduce two new metrics, Logits Magnitude and Regularized Standard Deviation, to assess classifier re-training methods. Develop a label over-smoothing approach that adjusts logits by softening one-hot labels, improving model performance without needing prior class distribution knowledge."}
{"id": "Gq7RDMeZi4", "Context": "Graph neural network (GNN) architectures are used to model data with cross-instance relations, often involving layers that iteratively reduce a graph-regularized energy function. These node embeddings serve as predictive features for tasks like node classification and as energy function minimizers, offering interpretability and inductive biases. However, scaling such GNN architectures is challenging due to the potential depth required for convergence during the forward pass.", "Idea": "Propose a sampling-based energy function and scalable GNN layers that iteratively reduce it, with convergence guarantees in certain settings, and develop a full GNN architecture based on these designs."}
{"id": "W8xukd70cU", "Context": "The data center industry is rapidly expanding due to advances in information technology and artificial intelligence, leading to a significant increase in electricity consumption, particularly for cooling systems. In typical data centers, 30-40% of energy is used for cooling rather than computing, highlighting the need for energy-saving optimization technologies. However, optimizing these systems is challenging due to the lack of reliable simulation environments, limited historical data, and strict safety and control requirements.", "Idea": "Introduce a novel physics-informed offline reinforcement learning framework for optimizing energy efficiency in data center cooling systems. This framework uses a graph neural network architecture to model complex dynamical patterns and physical dependencies, enabling sample-efficient and robust offline policy learning with limited operational data."}
{"id": "5ck9PIrTpH", "Context": "Large language models (LLMs) are proficient at solving arithmetic word problems, but their ability to generalize to more complex problems is not well understood. This is challenging to assess because much of the evaluation data has been seen by the models during training, and existing benchmarks do not adequately capture the complexity of problem proofs.", "Idea": "Introduce MathGAP, a data-generation framework that evaluates LLMs on problems with arbitrarily complex arithmetic proofs by generating problem statements and reasoning traces based on specified proof structures, allowing for systematic studies of generalization from easy to hard problems."}
{"id": "I4YAIwrsXa", "Context": "Lean is a sophisticated proof assistant that aids in formal theorem proving by offering interactive feedback. The challenge lies in enhancing large language models to construct formal proofs effectively, aligning them with formal verification systems.", "Idea": "Implement online reinforcement learning using Lean's verification outcomes as a reward signal to refine proof completion policies. Additionally, introduce RMaxTS, a Monte-Carlo tree search variant with an intrinsic-reward-driven exploration strategy to generate diverse proof paths, addressing the sparse-reward problem in proof search."}
{"id": "mP7uV59iJM", "Context": "Accurate camera pose estimation is crucial for various computer vision tasks, and current methods often rely on absolute pose regression and scene coordinate regression techniques. These methods typically require training feature extractors or descriptors and can struggle with localization accuracy, especially in challenging environments.", "Idea": "Introduce GS-CPR, a test-time camera pose refinement framework that leverages 3D Gaussian Splatting for scene representation. This approach enhances localization accuracy by rendering synthetic images and depth maps for 2D-3D correspondences, and operates directly on RGB images using the 3D foundation model, MASt3R, without needing to train feature extractors or descriptors."}
{"id": "j7cyANIAxV", "Context": "Drug-target binding affinity prediction is crucial for drug discovery and has been widely studied, yielding promising results. However, conventional evaluation methods, which use a randomized split of test sets, often result in test samples that are too similar to the training set. This leads to misleading performance metrics, as models struggle with samples that have lower similarity to the training set, a problem that is frequently overlooked.", "Idea": "Propose a similarity aware evaluation framework with a novel split methodology that adapts to any desired distribution. This is achieved through optimization problems solved by gradient descent, allowing for more accurate assessment of model performance on diverse sample similarities."}
{"id": "60Vd7QOXlM", "Context": "Current privacy auditing techniques for large language models (LLMs) are limited in effectiveness due to their reliance on basic canary generation methods. These methods result in weak membership inference attacks, providing only loose lower bounds on empirical privacy leakage.", "Idea": "Develop more effective canaries under realistic threat models to improve the detection of privacy leakage in LLMs, setting a new standard for measuring memorization rates and providing a more accurate privacy audit."}
{"id": "kGvXIlIVLM", "Context": "Classifier-Free Guidance (CFG) is a technique used to improve the sample quality of visual generative models. In autoregressive (AR) multi-modal generation, CFG creates inconsistencies between language and visual content, which conflicts with the goal of unifying different modalities in visual AR.", "Idea": "Propose Condition Contrastive Alignment (CCA) to enable guidance-free AR visual generation by fine-tuning pretrained models to match the desired sampling distribution, eliminating the need for guided sampling and reducing sampling costs."}
{"id": "fv9XU7CyN2", "Context": "The rise in antimicrobial resistance has made it crucial to identify new antibiotic compounds, as traditional drug development methods are costly and inefficient. Machine learning techniques have been explored to improve the prediction and development of novel antibiotics, but current efforts have not fully utilized the potential of multimodal molecular data. Contrastive learning frameworks have shown promise in representation learning across various domains, suggesting potential for antibiotic discovery.", "Idea": "Introduce CL-MFAP, an unsupervised contrastive learning-based multimodal foundation model designed to discover small molecules with antibiotic potential using three types of molecular data. The model pretrains three encoders: a transformer-based encoder for SMILES strings, another transformer-based encoder with a bi-level routing attention mechanism for molecular graphs, and a Morgan fingerprint encoder using a multilayer perceptron, leveraging 1.6 million bioactive molecules from the ChEMBL dataset."}
{"id": "dOAkHmsjRX", "Context": "Online continual learning (CL) often involves single-epoch training and limits on replay memory size, leading to varied computational demands across different CL algorithms. Additionally, the storage costs associated with storing logits or models alongside replay memory are frequently overlooked, complicating fair comparisons among CL algorithms due to differing computational and storage budgets.", "Idea": "Propose using floating point operations (FLOPs) and total memory size in bytes as standardized metrics for computational and memory budgets to enable fair comparison and development of CL algorithms. Introduce adaptive layer freezing to reduce computational costs by not updating layers for less informative batches, and a memory retrieval method to learn efficiently with fewer iterations."}
{"id": "Ox4AJ2Vurb", "Context": "Recent advancements in 3D reconstruction have leveraged kernel methods, particularly for oriented point clouds. Traditional methods, such as those based on the arc-cosine kernel, have been effective but present challenges in terms of implementation complexity, computational speed, and scalability.", "Idea": "Utilize Matérn kernels for implicit surface reconstruction, highlighting their ease of implementation, computational efficiency, and scalability. These kernels offer tunable surface reconstruction capabilities and demonstrate a theoretical connection to SIREN networks, providing a competitive alternative to existing methods."}
{"id": "cKlzKs3Nnb", "Context": "Large language model agents have demonstrated significant potential in addressing real-world software engineering problems. The most advanced open-source software engineering agents can resolve a notable percentage of real GitHub issues, but they exhibit varying strengths, excelling in some tasks while underperforming in others.", "Idea": "Introduce DEI (Diversity Empowered Intelligence), a framework that acts as a meta-module to manage and leverage the unique expertise of existing software engineering agent frameworks, enhancing their collective problem-solving capabilities."}
{"id": "kmgrlG9TR0", "Context": "Reward models (RMs) are crucial for aligning large language models (LLMs) with human-preferred behaviors. The current evaluation methods for RMs may not accurately reflect their alignment performance due to limited evaluation data distribution and methods that are not closely aligned with alignment objectives.", "Idea": "Propose RMB, a comprehensive benchmark for reward models that encompasses over 49 real-world scenarios and includes both pairwise and Best-of-N evaluations to better assess the effectiveness of RMs in alignment optimization."}
{"id": "wmV4cIbgl6", "Context": "Causal discovery involves identifying causal relationships from observational data, a task that is notoriously difficult due to the complexity, non-linearity, and evolving nature of real-world causal structures. Existing methods often rely on synthetic data and limited real-world examples, which do not adequately capture these complexities, making it challenging to select an appropriate causal discovery strategy.", "Idea": "Introduce CausalRivers, a comprehensive benchmarking kit for causal discovery in time-series data, featuring extensive datasets from river discharge measurements in Germany. It includes causal ground truth graphs that can be sampled to create diverse subgraphs for benchmarking, aiming to facilitate robust evaluations and comparisons of causal discovery methods."}
{"id": "Frok9AItud", "Context": "Random Projections are commonly used to create embeddings for graph learning tasks due to their efficiency, often justified by the Johnson-Lindenstrauss Lemma. However, there is limited understanding of how well these projections preserve dot product and cosine similarity when applied to graph matrices.", "Idea": "Investigate the preservation of dot product and cosine similarity by random projections on graph matrices, providing new asymptotic and finite-sample results, identifying pathological cases, and applying findings to a ranking application to assess the reliability of embeddings."}
{"id": "2ATD8a8P3C", "Context": "Conformal prediction is a strategy for quantifying the uncertainty of predictive models by modifying them to output sets of labels that are likely to contain the true label. Existing algorithms have focused on classification and regression, where prediction sets are simple and easy to interpret. However, for complex structured outputs like text generation, these sets can become large and difficult to interpret.", "Idea": "Propose a general framework for conformal prediction in structured prediction settings, modifying existing algorithms to output structured prediction sets that implicitly represent sets of labels. This approach can be applied in domains where prediction sets are represented as nodes in a directed acyclic graph, such as hierarchical labels in image classification."}
{"id": "2MLvV7fvAz", "Context": "Graph representation learning has been exploring the use of non-Euclidean geometries, such as Riemannian manifolds, to embed complex graph structures like scale-free, hierarchical, and cyclic patterns. Spectral filtering is known for its ability to process signal variations across graphs, making it effective in both homophilic and heterophilic settings. Combining these approaches could potentially enhance the quality of learned graph representations.", "Idea": "Introduce Spectro-Riemannian Graph Neural Networks (CUSP), a novel paradigm that integrates curvature (geometric) and spectral insights. CUSP is a mixed-curvature spectral GNN that optimizes node embeddings in products of constant curvature manifolds using three components: Cusp Laplacian for capturing curvature signals, Cusp Filtering for spectral cues, and Cusp Pooling for assessing substructure importance with curvature-based encoding."}
{"id": "din0lGfZFd", "Context": "Large language models have demonstrated impressive reasoning capabilities, with scaling laws indicating that a large parameter count, particularly in terms of depth, is crucial. However, many reasoning tasks may require significant depth without necessarily needing a large number of parameters. This insight opens up new possibilities for applying looped models to reasoning tasks.", "Idea": "Propose the use of looped models for reasoning tasks, where a $k$-layer transformer looped $L$ times can achieve performance comparable to a $kL$-layer non-looped model. This approach leverages iterative algorithms to solve reasoning problems effectively with optimal depth, and introduces a looping-based regularization to address both reasoning and memorization challenges."}
{"id": "VmJdqhuTCh", "Context": "Previous frequency-based Self-Supervised Learning (SSL) methods for pre-training involved masking out pre-defined frequencies in input images and using a reconstruction loss. These methods faced two main limitations: they did not account for the variability in image frequency responses, and models pre-trained with frequency-filtered images required more data to adapt to natural images during fine-tuning.", "Idea": "Propose FOurier transform compression with seLf-Knowledge distillation (FOLK), which adaptively selects masked-out frequencies based on image frequency responses and employs a two-branch framework with knowledge distillation to process both filtered and original images, reducing the burden on downstream tasks."}
{"id": "TjP1d8PP8l", "Context": "Large Language Models (LLMs) have demonstrated impressive reasoning abilities across various fields but struggle with complex embodied tasks that require a coherent long-term strategy and context-sensitive understanding of the environment. Previous approaches to refining LLMs have relied on outcome-supervised feedback, which can be both costly and ineffective.", "Idea": "Introduce a novel framework called Discriminator-Guided Action Optimization (DGAP) that optimizes LLM action plans using step-wise signals. This involves using a limited set of demonstrations to train a discriminator to learn a score function, which evaluates the alignment between LLM-generated actions and optimal actions at each step, guiding LLMs to generate actions that maximize this score."}
{"id": "CexatBp6rx", "Context": "In recent years, there has been a growing focus on developing inherently interpretable models for prediction, particularly those that learn high-level concepts. These models are valued for their ability to represent concepts in a way that is close to human communication. However, a significant challenge arises in visualizing and understanding the unsupervised dictionary of concepts, especially when dealing with large-scale images.", "Idea": "Propose a novel method that maps concept features to the latent space of a pretrained generative model, enabling high-quality visualization and providing an intuitive, interactive procedure for interpreting learnt concepts by imputing concept activations and visualizing generated modifications."}
{"id": "66NzcRQuOq", "Context": "Video generation involves navigating a complex spatiotemporal space, which requires substantial computational resources and data. Current methods use a cascaded architecture to manage this complexity by avoiding direct training with full resolution latent, but this approach limits knowledge sharing and flexibility due to the separate optimization of each sub-stage.", "Idea": "Introduce a unified pyramidal flow matching algorithm that reinterprets the denoising trajectory as pyramid stages, with only the final stage at full resolution. This design allows for interlinked flows across stages to maintain continuity and supports autoregressive video generation with a temporal pyramid, enabling end-to-end optimization with a single unified Diffusion Transformer (DiT)."}
{"id": "MT3aOfXIbY", "Context": "Sampling algorithms are crucial for managing the quality and runtime of diffusion model inference. Recent studies have provided algorithms for diffusion sampling with provable guarantees, demonstrating that it is possible to approximately sample in polynomial time for any data distribution, given an accurate estimate of its score functions at various noise levels.", "Idea": "Propose a new sampling scheme inspired by the randomized midpoint method for log-concave sampling, achieving improved dimension dependence for sampling from arbitrary smooth distributions in total variation distance. Additionally, the algorithm can be parallelized to run in logarithmic parallel rounds, offering the first provable guarantees for parallel sampling with diffusion models."}
{"id": "tZCqSVncRf", "Context": "Inductive reasoning is crucial for large language models (LLMs) to enhance their intelligence, requiring them to generalize rules from observed facts and apply them to new examples. Previous work in this area has been limited by inadequate evaluation and inflexible test data, hindering a comprehensive understanding of LLMs' reasoning capabilities.", "Idea": "Introduce 'Mirage', a synthetic dataset designed to evaluate LLMs' inductive and deductive reasoning abilities with flexible variations in input distribution, task scenarios, and difficulty levels, to better analyze factors affecting LLMs' reasoning processes."}
{"id": "or8mMhmyRV", "Context": "Describing skills in natural language can serve as a means to incorporate human decision-making knowledge into AI systems. This approach aims to make skill design more accessible and effective, particularly in creating adaptable and high-performing agents.", "Idea": "Introduce MaestroMotif, a method that utilizes Large Language Models (LLMs) to design and reuse skills by automatically generating rewards from natural language descriptions and employing code generation and reinforcement learning to train and combine these skills for complex behaviors."}
{"id": "j8lqABLgub", "Context": "The problem of online scheduling with class constraints involves allocating jobs to machines with the goal of minimizing makespan while adhering to class slot limitations. The offline version of this problem is well-studied with known efficient solutions, but the online version faces significant challenges, as traditional competitive analysis reveals strong impossibility results.", "Idea": "Explore the problem in a learning-augmented setting where algorithms can use potentially inaccurate predictions, leading to new algorithms with competitive ratios that do not depend on the number of machines. This approach also establishes tight lower bounds for various prediction models, providing insights into how additional information can enhance scheduling algorithm design."}
{"id": "BWS5gVjgeY", "Context": "Large language models (LLMs) have shown proficiency in complex reasoning tasks but often make errors in basic numerical understanding and processing, which is crucial for solving arithmetic and mathematical problems. Previous research has largely overlooked this aspect or focused on limited tasks, leaving a gap in comprehensive evaluation of numerical abilities in LLMs.", "Idea": "Introduce a benchmark that evaluates numerical understanding and processing ability (NUPA) in LLMs across four numerical representations and 17 tasks, resulting in 41 combinations. This benchmark is derived from educational curricula and aims to assess everyday numerical scenarios. Additionally, the study explores the effectiveness of various techniques, including finetuning and chain-of-thought methods, to enhance NUPA in LLMs."}
{"id": "YLIsIzC74j", "Context": "In modern chip design, macro placement is a critical stage where machine learning techniques have shown potential for enhancement. However, existing methods focus on online optimization of intermediate surrogate metrics available at the current placement stage, rather than directly targeting cross-stage metrics like timing performance, which measure the final chip quality. This is due to the high computational costs of evaluating such metrics post-placement, making online optimization impractical and often misaligned with actual performance improvements.", "Idea": "Introduce LaMPlace, a method that learns a mask for optimizing cross-stage metrics in macro placement. LaMPlace trains a predictor on offline data to estimate cross-stage metrics and uses this predictor to generate a mask, a pixel-level feature map that evaluates the impact of macro placement on design metrics, enabling decisions based on cross-stage metrics rather than intermediate surrogate metrics."}
{"id": "pymXpl4qvi", "Context": "Structured State Space Models (SSMs) have been considered as alternatives to transformers, particularly for capturing long-sequence dependencies. However, they are limited by a strong recency bias, which affects their ability to recall distant information and introduces robustness issues. As SSMs are scaled deeper, they face a dilemma between recency bias and over-smoothing, where token representations become indistinguishable, hindering scalability.", "Idea": "Propose a polarization technique for SSMs by setting two channels of the state transition matrices to zero and one, respectively, to address both recency bias and over-smoothing, enabling better associative recall accuracy and scalability with deeper architectures."}
{"id": "vzrs42hgb0", "Context": "Hypergraph Neural Networks (HGNNs) are known for their high accuracy in capturing complex dependencies in data. However, they often suffer from slow inference speeds and high memory consumption, which limits their applicability in real-time scenarios.", "Idea": "Propose a framework that enhances HGNNs by employing a teacher-student knowledge distillation strategy, where a teacher model transfers both soft labels and structural information to a lightweight Graph Convolutional Network (GCN) called TinyGCN, enabling faster inference and reduced computational cost while maintaining accuracy."}
{"id": "RAyRXQjsFl", "Context": "The separation power of a machine learning model is crucial for its ability to distinguish between different inputs and is often used to assess its expressivity. Understanding the separation power of model families is essential for achieving detailed universality results. Equivariant neural networks, such as convolutional and permutation-invariant networks, are analyzed for their separation power.", "Idea": "The paper provides a complete characterization of inputs indistinguishable by models from a given architecture and examines how separability is influenced by hyperparameters and architectural choices. It finds that all non-polynomial activations are equivalent in expressivity, depth improves separation power up to a point, and block decomposition of hidden representations forms a hierarchy in separation power."}
{"id": "PpYy0dR3Qw", "Context": "In distributed optimization and learning, particularly in federated learning, communication is a critical bottleneck due to its slow and costly nature. This challenge necessitates the development of methods that can reduce communication overhead while maintaining efficiency.", "Idea": "Introduce LoCoDL, a communication-efficient algorithm that combines local training to reduce communication frequency and compression techniques to send short bitstreams instead of full-dimensional vectors. LoCoDL is compatible with a wide range of unbiased compressors, including sparsification and quantization, and achieves accelerated communication complexity in heterogeneous settings with strongly convex functions."}
{"id": "Hz4BYVY8YM", "Context": "Large Vision-Language Models (LVLMs) have made significant progress on established benchmarks, but there is a lack of suitable evaluation for their effectiveness in the emerging field of long-context streaming video understanding. Existing benchmarks focus on isolated single-instance text inputs and do not adequately assess the ability to maintain temporal reasoning over the duration of video streams.", "Idea": "Introduce SVBench, a new benchmark designed for evaluating streaming video understanding in LVLMs through temporal multi-turn question-answering chains. This includes a semi-automated annotation pipeline to create QA pairs and chains, enabling comprehensive assessment of LVLMs' capabilities in sustaining temporal reasoning over video streams."}
{"id": "3n6DYH3cIP", "Context": "Learning the structure of Bayesian networks is a crucial but computationally demanding task, particularly as the number of variables increases. Traditional methods require complete retraining when new variables are added, making them unsuitable for dynamic or large-scale applications.", "Idea": "Propose an extendable structure learning strategy that efficiently integrates a new variable into an existing Bayesian network, leveraging existing information to reduce computational overhead. Introduce a novel iterative paradigm for structure learning, starting with a small subset of variables and iteratively adding more to construct a complete P-map graph."}
{"id": "nNYA7tcJSE", "Context": "Recent advancements in large-scale text-to-video and image-to-video diffusion models have significantly improved video generation, particularly in keyframe interpolation. However, current image-to-video diffusion models require adaptation for two-frame conditioned generation, which is crucial for effective bounded interpolation. Existing methods that attempt to fuse temporally forward and backward paths in parallel often encounter off-manifold issues, resulting in artifacts or necessitating multiple iterative re-noising steps.", "Idea": "Introduce a novel bidirectional sampling strategy that addresses off-manifold issues without extensive re-noising or fine-tuning by employing sequential sampling along both forward and backward paths, conditioned on start and end frames. This approach ensures coherent and on-manifold generation of intermediate frames, enhanced by advanced guidance techniques CFG++ and DDS, to improve the interpolation process."}
{"id": "Qro97zWC29", "Context": "Traditional self-supervised learning methods often rely on contrastive approaches that provide binary learning signals, such as 'attract' and 'repel', which may not fully capture the complexity of spatially dense features. These methods can be limited in their ability to generate high-quality dense feature encoders and may require significant computational resources.", "Idea": "Introduce NeCo: Patch Neighbor Consistency, a novel self-supervised training loss that enforces patch-level nearest neighbor consistency between a student and teacher model. This approach utilizes differentiable sorting on pretrained representations to provide a more fine-grained learning signal, enhancing the quality of dense feature encoders."}
{"id": "JSB171dSUU", "Context": "Adapting medical Large Language Models (LLMs) to local languages can help reduce barriers to healthcare access, but a major challenge is the scarcity of data, especially for low-resource languages. This necessitates the creation of high-quality medical datasets and an understanding of how multilingual LLMs can generalize to these languages.", "Idea": "Propose a novel Mixture of Experts (MoE) routing method that uses language-specific experts and cross-lingual routing, inspired by circuit theory, to enhance multilingual LLMs. Develop the Post-MoE architecture, which applies sparse routing in later layers, and introduce 'language family' experts to scale the model to 50 languages without adding parameters."}
{"id": "3bcN6xlO6f", "Context": "Identifying subtle differences between videos of the same action is a challenging task with applications in areas like coaching and skill learning. Existing large multimodal models struggle with this task, particularly in localizing relevant sub-actions and performing fine-grained frame comparisons.", "Idea": "Introduce Video Action Differencing (VidDiff), a method that addresses the task by breaking it into three stages: action difference proposal, keyframe localization, and frame differencing, each utilizing specialized foundation models. A benchmark dataset, VidDiffBench, is also created to support development in this area."}
{"id": "LgzRo1RpLS", "Context": "The quadratic complexity of the attention mechanism in transformer models has led researchers to explore alternative architectures with sub-quadratic complexity, such as state-space models. Mamba is a leading model in this area, achieving state-of-the-art results in language modeling benchmarks. However, its performance is limited by its pre-training context length, causing degradation when handling longer contexts due to out-of-distribution discretization steps.", "Idea": "Introduce MambaExtend, a framework that enhances Mamba's context extension capabilities by using a training-free approach to calibrate the scaling factors of discretization modules across different layers. This method employs both gradient-based and gradient-free zeroth-order optimization to determine optimal scaling factors, enabling significant context extension with minimal updates and reduced memory usage."}
{"id": "NUD03NBDOE", "Context": "Reasoning about Actions and Change (RAC) has been essential in addressing foundational AI challenges, such as the frame problem, and has contributed to advancements in non-monotonic and commonsense reasoning. It is vital for AI systems operating in dynamic environments or engaging in interactive scenarios. Despite the progress made by Large Language Models (LLMs) in various AI domains, their capabilities in RAC remain insufficiently explored.", "Idea": "Introduce a new diagnostic benchmark, ActionReasoningBench, designed to evaluate LLMs across six key RAC dimensions, including Fluent Tracking, State Tracking, Action Executability, Effects of Actions, Numerical RAC, and Composite Questions. This benchmark aims to rigorously assess LLMs' performance in RAC and includes new ramification constraints to better understand the indirect effects of actions."}
{"id": "rvXdGL4pCJ", "Context": "Reinforcement learning (RL) often involves trial and error, which can lead to undesirable outcomes, making it unsuitable for safety-critical applications. Typically, a safe agent is trained in a controlled environment and then transferred to the real world, but differences in dynamics between these environments can lead to safety violations.", "Idea": "Develop a methodology that robustifies an agent in the controlled environment and ensures a safe transfer to new environments, even when there are differences in safety-related dynamics."}
{"id": "xnssGv9rpW", "Context": "Generating novel crystalline materials is crucial for advancements in electronics, energy storage, and catalysis. Crystals are defined by their symmetry, which significantly influences their physical properties. Current methods for crystal generation either fail to produce materials with real-world crystal symmetries or merely replicate symmetry information from existing databases.", "Idea": "Propose SymmCD, a diffusion-based generative model that integrates crystallographic symmetry into the generation process by decomposing crystals into the asymmetric unit and the necessary symmetry transformations, using a novel representation to generalize across different symmetry groups."}
{"id": "eWNEqdH0vk", "Context": "The scaling of large language models has significantly enhanced their capabilities, but this growth necessitates efficient computational strategies. The Mixture-of-Experts (MoE) architecture is notable for scaling model size without greatly increasing training costs, yet it often suffers from parameter inefficiency. Current MoE models independently assign tokens in different layers without using historical routing information, which can lead to suboptimal token-expert combinations and parameter inefficiency.", "Idea": "Introduce the Layerwise Recurrent Router for Mixture-of-Experts (RMoE), which uses a Gated Recurrent Unit (GRU) to create dependencies between routing decisions across layers. This approach allows for efficient parallel computation of layerwise recurrence for input tokens and integrates a novel computation stage that is compatible with existing MoE architectures."}
{"id": "M42KR4W9P5", "Context": "End-to-end autonomous driving (E2E-AD) is gaining traction as a scalable, data-driven approach to designing autonomous systems. However, current E2E-AD methods typically follow a sequential perception-prediction-planning paradigm, which can result in cumulative errors and unstable training. Additionally, the manual ordering of tasks restricts the system's ability to exploit synergies between tasks, and the dense BEV representation used by these methods poses computational challenges for long-range perception and temporal fusion.", "Idea": "Introduce DriveTransformer, a simplified E2E-AD framework that enhances scalability through task parallelism, sparse representation, and streaming processing. This framework employs three unified operations: task self-attention, sensor cross-attention, and temporal cross-attention, which collectively reduce system complexity and improve training stability."}
{"id": "S4dItvpvAv", "Context": "Multi-Objective Markov Decision Processes (MO-MDPs) are increasingly important for real-world decision-making problems involving conflicting objectives. The Pareto front is crucial for identifying non-dominated policies, but finding it is challenging. Existing methods either rely on impractical continuous preference space traversal or focus only on deterministic policies, lacking techniques to fully characterize the Pareto front. Additionally, the structure of the Pareto front remains unclear even when the MDP is fully known.", "Idea": "Investigate the geometric structure of the Pareto front in MO-MDPs, revealing it as the boundary of a convex polytope with vertices corresponding to deterministic policies. Develop an efficient algorithm that identifies these vertices by solving a single-objective MDP once and traversing the Pareto front's edges, significantly reducing the complexity of finding the exact Pareto front."}
{"id": "YuHQTo6G9S", "Context": "Several medical Multimodal Large Language Models (MLLMs) have been developed to handle tasks involving visual images with textual instructions across various medical modalities. These models have achieved impressive results but are generally region-agnostic, treating the entire image as a holistic representation. This approach makes it difficult for the models to identify specific regions of focus when generating sentences, unlike doctors who review entire images before concentrating on specific regions for thorough evaluation.", "Idea": "Develop a Region-Aware medical MLLM, MedRegA, by formulating Region-Centric tasks and constructing a large-scale dataset, MedRegInstruct, to incorporate regional information into training. This model is designed to handle both image-level and region-level medical vision-language tasks across various modalities, enabling enhanced understanding of anatomical regions within medical scans."}
{"id": "ozZG5FXuTV", "Context": "Aligning machine learning algorithms with the decision-making processes of experienced radiologists is essential for reliable medical diagnosis. Existing methods have primarily focused on associational alignment with radiologists' behaviors as reflected in training data, which can lead to pseudo-correlations that do not generalize well.", "Idea": "Propose a causality-based alignment framework that uses counterfactual generation to identify the causal chain of model decisions and introduces a causal alignment loss to ensure the model focuses on causal factors. This approach employs the implicit function theorem and the conjugate gradient method for efficient optimization of the alignment process."}
{"id": "rAoEub6Nw2", "Context": "Large language models have significantly advanced natural language processing, with platforms like Chatbot Arena playing a crucial role in evaluating these models through extensive pairwise comparisons based on human judgments. This platform has become essential for ranking models in open-ended conversational tasks, providing valuable datasets for analysis.", "Idea": "Propose a statistical framework that introduces a factored tie model to better handle ties in human-judged comparisons, extends the framework to model covariance between competitors for deeper performance insights, and resolves optimization challenges with novel constraints for stable parameter estimation."}
{"id": "cznqgb4DNv", "Context": "Decentralized federated learning (DFL) involves clients performing model updates and aggregations without a central server. Existing DFL approaches have typically assumed a fixed number of local updates between model exchanges, not accounting for variations in communication and computation capabilities among clients.", "Idea": "Propose Decentralized Sporadic Federated Learning (DSpodFL), which incorporates a generalized concept of sporadicity in local gradient and aggregation processes. This method models the occurrence of gradient descent and model exchanges as arbitrary indicator random variables, accommodating heterogeneous and dynamic computation and communication scenarios."}
{"id": "faDMOmnsjx", "Context": "The cosine router in Mixture of Experts (MoE) has shown promising performance in image and language tasks, addressing issues like representation collapse, which can lead to parameter redundancy and limited representation potential. However, a detailed analysis of the cosine router's performance and limitations has been lacking, particularly concerning the slow estimation rates of experts and model parameters due to intrinsic interactions within the model.", "Idea": "Introduce the perturbed cosine router, which stabilizes the cosine router by adding noise to the ℓ²-norms, significantly improving the estimation rates of experts and model parameters to polynomial rates under strongly identifiable settings."}
{"id": "d1NWq4PjJW", "Context": "In machine learning, several Riemannian manifolds, such as Symmetric Positive Definite (SPD), Grassmann, spherical, and hyperbolic manifolds, have been shown to support gyro structures. This allows for the extension of Euclidean Deep Neural Networks (DNNs) to these manifolds, which is a significant challenge due to the non-Euclidean nature of these spaces.", "Idea": "Introduce a general Riemannian Batch Normalization (RBN) framework on gyrogroups, called GyroBN, which ensures theoretical control over sample statistics through concepts like pseudo-reduction and gyroisometric gyrations. GyroBN integrates various existing normalization methods and is applicable to different geometries, including Grassmannian and hyperbolic spaces."}
{"id": "GlAeL0I8LX", "Context": "In safety-critical situations, understanding the classifications made by deep neural networks is crucial. While recent models can explain individual decisions locally, providing a comprehensive global explanation of a model's overall behavior remains a challenging task.", "Idea": "Introduce the Quadratic Programming Enhanced Model (QPM), which learns globally interpretable class representations by using a binary assignment of a few features per class. This assignment is determined through discrete optimization based on similarity measures and interpretability constraints, allowing for easily comparable contrastive class representations."}
{"id": "aIMi2lOKIn", "Context": "3D sketches are commonly used to visually represent the shape and structure of objects or scenes in three dimensions. However, creating these sketches typically requires professional artistic skills, and existing research has mainly focused on improving interactive sketch generation in 3D virtual systems.", "Idea": "Introduce Diff3DS, a differentiable rendering framework that generates view-consistent 3D sketches by optimizing 3D parametric curves. This framework uses perspective projection to render 3D rational Bézier curves into 2D curves, which are then converted into 2D raster images through a customized differentiable rasterizer, enabling end-to-end optimization of 3D sketches using gradients from the 2D image domain."}
{"id": "jE5ZbtMtcU", "Context": "Modeling the nonlinear dynamics of neuronal populations is a significant challenge in computational neuroscience. Recent efforts have focused on jointly modeling neural activity and behavior to understand their interconnections, but these often require complex models or oversimplified assumptions. A key issue arises from the lack of perfectly paired neural-behavioral datasets in real-world applications, leading to the question of how to develop models that perform well using only neural activity at inference while leveraging behavioral insights during training.", "Idea": "Introduce BLEND, a behavior-guided neural population dynamics modeling framework using privileged knowledge distillation. This approach involves training a teacher model with both behavioral and neural data, and then distilling a student model using only neural activity. BLEND is model-agnostic and does not rely on strong assumptions about the behavior-neural activity relationship, allowing it to enhance existing architectures without the need for specialized models."}
{"id": "nsCOeCLR8e", "Context": "Random Forests are effective in classification and regression tasks across various domains like medical diagnosis, finance, and personalized recommendations. These areas are sensitive to privacy concerns due to the involvement of personal and confidential data. With regulations such as GDPR and CCPA emphasizing the right to be forgotten, the need for machine unlearning in Random Forests has become critical. However, existing methods face challenges in practical application.", "Idea": "Propose the DynFrs framework to enable efficient machine unlearning in Random Forests while maintaining predictive accuracy. DynFrs uses the subsampling method Occ(q) to limit the impact of deleting samples and the lazy tag strategy Lzy to delay tree node reconstruction, making it adaptable to any Random Forest variant."}
{"id": "aSy2nYwiZ2", "Context": "Jailbreak backdoor attacks on large language models (LLMs) have become notable for their effectiveness and stealth. Current methods typically depend on creating poisoned datasets and require the labor-intensive process of fine-tuning, which is time-consuming.", "Idea": "Introduce JailbreakEdit, a novel method that uses model editing techniques to inject a universal jailbreak backdoor into safety-aligned LLMs quickly and with minimal intervention. It employs multi-node target estimation to map the jailbreak space, creating pathways from the backdoor to this space, thereby enabling the model to perform jailbreak actions by shifting its attention through strong semantic associations."}
{"id": "rDLgnYLM5b", "Context": "Many real-world user queries, such as those asking for recipes, could benefit from systems that generate responses with both textual instructions and accompanying images, similar to a cookbook. However, models designed to generate interleaved text and images face challenges in maintaining consistency within and across these modalities.", "Idea": "Introduce ISG, a comprehensive evaluation framework for interleaved text-and-image generation that uses a scene graph structure to assess consistency, coherence, and accuracy across four levels of granularity. Additionally, present ISG-Bench, a benchmark dataset with complex language-vision dependencies, and develop ISG-Agent, a baseline agent using a 'plan-execute-refine' pipeline to improve performance."}
{"id": "JkCJBoNUcU", "Context": "Existing image super-resolution techniques struggle to generalize effectively in real-world settings due to the significant differences between training data and practical scenarios. Previous methods have attempted to simulate complex degradations or use learning-based techniques, but these approaches have been insufficient for generating large-scale, realistic, and diverse data.", "Idea": "Introduce a novel Realistic Decoupled Data Generator (RealDGen), an unsupervised learning framework that uses content and degradation extraction strategies within a content-degradation decoupled diffusion model to generate realistic low-resolution images from unpaired real LR and HR images."}
{"id": "rTQNGQxm4K", "Context": "Understanding the relationships and performance characteristics of Large Language Models (LLMs) is challenging, especially when there is a lack of transparent training information. Traditional methods do not adequately capture the evolutionary relationships between different LLMs, which can be crucial for predicting their capabilities and performance.", "Idea": "Introduce PhyloLM, a method that adapts phylogenetic algorithms to LLMs by calculating a phylogenetic distance metric based on the similarity of their outputs, and using this metric to construct dendrograms that capture known relationships among models."}
{"id": "qpXctF2aLZ", "Context": "Reinforcement learning (RL) has achieved significant success in various domains, but its widespread adoption is hindered by the opaque nature of neural network policies, which are difficult to interpret. Symbolic policies, on the other hand, offer a compact and interpretable representation of decision-making strategies, yet learning these policies directly within on-policy methods poses challenges.", "Idea": "Introduce SYMPOL, a novel method that integrates a tree-based model with a policy gradient method, allowing for the learning and adaptation of actions while maintaining interpretability. SYMPOL enables gradient-based, end-to-end learning of interpretable, axis-aligned decision trees within standard on-policy RL algorithms."}
{"id": "E1EHO0imOb", "Context": "Large language models have traditionally been trained using higher precision formats, which limits the efficiency and scalability of training on massive datasets. Recent efforts have aimed to reduce precision to improve computational efficiency, but this has introduced new challenges, such as training instabilities that arise over extended periods.", "Idea": "Introduce Smooth-SwiGLU, a modification to the SwiGLU activation function, to stabilize FP8 precision training by addressing outlier amplification issues. Additionally, implement FP8 quantization for Adam optimizer moments to enhance training efficiency, enabling successful training of large models with improved throughput."}
{"id": "9Ieq8jQNAl", "Context": "Learning rewards from preference feedback is crucial for aligning agentic models, typically using binary comparisons between multiple completions to gather large-scale human feedback. However, human feedback is often more diverse in other contexts, which can better support annotator goals and provide mutually informative insights or reveal biases in the reward learning process. Despite these advantages, the exploration of learning from diverse feedback types remains limited.", "Idea": "Enable experimentation and evaluation of multi-type feedback by generating high-quality simulated feedback of six different types, implementing reward models, and conducting downstream RL training across various environments to compare with preference-based baselines."}
{"id": "gTwRMU3lJ5", "Context": "Low-rank adaptation (LoRA) is a method used for parameter-efficient fine-tuning of foundation models. While it is computationally efficient, LoRA's performance is generally inferior to that of full fine-tuning. This performance gap is a significant challenge in optimizing the use of LoRA for various tasks.", "Idea": "Introduce LoRA-Pro, a method that enhances LoRA's performance by strategically adjusting the gradients of the low-rank matrices. This adjustment allows the low-rank gradient to more accurately approximate the full fine-tuning gradient, thereby narrowing the performance gap between LoRA and full fine-tuning."}
{"id": "ud8FtE1N4N", "Context": "Pruning is a technique used to remove unnecessary parameters in neural networks, addressing the increasing computational demands of large language models (LLMs). Traditionally, many approaches focus on post-training pruning, but sparse pre-training, which integrates pruning with pre-training, offers a simpler alternative.", "Idea": "Conduct a systematic exploration of optimal sparse pre-training configurations for LLMs by examining various pruning schedules across different sparsity levels and training durations. Propose a new scaling law that adapts the Chinchilla scaling law to account for the average parameter count during pre-training, providing a unified model for evaluation loss across both sparsely and densely pre-trained LLMs."}
{"id": "AvOhBgsE5R", "Context": "Previous approaches to 3D human motion generation have been successful but often require extensive training and are limited to specific tasks, lacking versatility and efficiency in generating, editing, and understanding human motion.", "Idea": "Introduce Motion-Agent, a conversational framework that uses a pre-trained language model to create MotionLLM, which encodes and quantizes motions into discrete tokens compatible with the language model's vocabulary. This allows for efficient fine-tuning and integration with GPT-4 to generate complex motion sequences through multi-turn conversations without additional training."}
{"id": "KEXoZxTwbr", "Context": "Previous methods for inverse rendering from multi-view images often relied on implicit irradiance fields or simplified ray tracing, which limited their ability to accurately capture complex lighting interactions such as self-shadowing and internal reflections. These approaches struggled with noise issues and were not well-suited for efficient optimization, making it challenging to achieve precise intrinsic decomposition of geometry, materials, and lighting.", "Idea": "Introduce MIRReS, a two-stage framework that first extracts an explicit triangular mesh and then refines it using a physically-based inverse rendering model with multi-bounce path tracing and Monte Carlo integration. This approach incorporates reservoir sampling to address noise and improve convergence, enabling accurate estimation of indirect illumination effects and efficient gradient-based optimization."}
{"id": "oRlANEuqG5", "Context": "Long-term point tracking in videos involves consistently identifying points across multiple frames, despite challenges such as changes in appearance, lighting, perspective, and occlusions. This task is particularly challenging in online tracking scenarios where frames are processed sequentially, making it suitable for real-world streaming applications.", "Idea": "Introduce Track-On, a transformer-based model for online long-term point tracking that processes video frames causally without future frame access. It utilizes spatial and context memory modules to capture temporal information, employing patch classification and refinement for accurate point tracking."}
{"id": "SOWZ59UyNc", "Context": "Traditional language model-based theorem proving relies on training models with formal proof data, assuming that this will enable them to learn theorem proving. However, this approach overlooks the informal thought processes that humans use when constructing proofs, which are not captured in formal proofs.", "Idea": "Introduce Lean-STaR, a framework that trains language models to generate informal thoughts before each proof step, enhancing theorem-proving capabilities. It uses retrospective ground-truth tactics to create synthetic thoughts for training and applies expert iteration to refine the model using correct proofs verified by the Lean solver."}
{"id": "RavSZTIe2s", "Context": "Distance field-based implicit representations, such as signed or unsigned distance fields, are commonly used in geometry modeling and analysis. These methods depend on the closest distance of points to a surface, which can lead to inaccuracies during surface extraction, particularly when interpolating along cube edges. Additionally, the gradients of these fields are not well-defined at certain points, resulting in distortions in the extracted surfaces.", "Idea": "Propose Shape as Line Segments (SALS), an implicit geometry representation using attributed line segments to accurately and efficiently model arbitrary structures. SALS employs a differentiable Line Segment Field to capture spatial relationships, using attributes like intersection flag and ratio for edge-based dual contouring to extract surfaces. Implement SALS with a neural network and design a learning-based pipeline for surface reconstruction from 3D point clouds."}
{"id": "0yvZm2AjUr", "Context": "Language models (LMs) often exhibit biases, sycophancy, backdoors, and other tendencies that result in unfaithful responses to input contexts. Understanding the internal states of LMs could aid in monitoring and correcting these unfaithful behaviors.", "Idea": "Introduce propositional probes to extract lexical concepts from token activations and bind them into logical propositions, identifying a binding subspace where bound tokens have high similarity. This method aims to decode faithful representations of input contexts from LMs' internal activations."}
{"id": "L14sqcrUC3", "Context": "In the transition from academic success to practical deployment, machine learning methods often face challenges due to differences between benchmark datasets and real-world data. Two common issues in industrial applications are the distribution drift over time and the complexity of data acquisition and feature engineering pipelines, which are not adequately represented in typical academic tabular datasets.", "Idea": "Introduce TabReD, a collection of eight industry-grade tabular datasets, to reassess tabular ML models and techniques under conditions that include time-based data splits and richer feature sets, revealing different method rankings compared to traditional academic benchmarks."}
{"id": "h1XoHOd19I", "Context": "Adapting large language models (LLMs) to specialized domains often involves using domain-specific corpora for continual pre-training to enhance knowledge memorization, followed by fine-tuning with related instructions to apply this knowledge. This approach can be inefficient as it separates knowledge memorization from utilization and requires LLMs to learn both knowledge utilization and format alignment simultaneously during fine-tuning, which involves divergent training objectives.", "Idea": "Propose a new domain adaptation framework called Mix-CPT, which includes domain knowledge learning and general format alignment. This framework involves a knowledge mixture continual pre-training that focuses on both knowledge memorization and utilization, and introduces a logit swap self-distillation constraint to prevent catastrophic forgetting. It then uses the acquired knowledge for efficient instruction tuning and alignment with a few general training samples to achieve format alignment."}
{"id": "QjO0fUlVYK", "Context": "Recent conjectures suggest that neural network solution sets reachable via stochastic gradient descent (SGD) are convex when considering permutation invariances, implying that a linear path can connect two independent solutions with low loss if the weights are appropriately permuted. However, testing this theory often requires very wide networks.", "Idea": "Propose the Starlight algorithm to identify a star model within the SGD solution set, which is linearly connected to other solutions via low-loss paths, considering permutations. This approach also offers improved uncertainty estimates for Bayesian Model Averaging and suggests star models as alternatives to model ensembles."}
{"id": "VQwI055flA", "Context": "Recent advancements in neural theorem proving involve integrating large language models with tree search algorithms like Monte Carlo Tree Search (MCTS). In this setup, the language model suggests tactics while the tree search identifies the complete proof path. However, the tactics proposed often converge to semantically or strategically similar ones, reducing diversity and increasing search costs by expanding redundant proof paths. Additionally, the trained value function faces issues such as false negatives, label imbalance, and domain gaps due to biased data construction.", "Idea": "Propose CARTS (diversified tactic CAlibration and bias-Resistant Tree Search), which enhances tactic diversity and importance while calibrating model confidence. CARTS also incorporates preference modeling and an adjustment term related to the ratio of valid tactics to improve the bias-resistance of the value function."}
{"id": "RoN6NnHjn4", "Context": "The task of synthesizing face images of non-existent persons is crucial for creating datasets that effectively train face recognition models. Key challenges include generating a large number of distinct identities with proper inter-class separation and ensuring adequate intra-class variation in appearance for each identity. Existing methods struggle with generating well-separated identities and often rely on external models for attribute augmentation.", "Idea": "Introduce Vec2Face, a holistic model that uses a sampled vector as input to flexibly generate and control face image identities and attributes. It consists of a feature masked autoencoder and an image decoder, supervised by face image reconstruction, allowing for the generation of well-separated identities and intra-class variation through vector manipulation."}
{"id": "dgR6i4TSng", "Context": "Parameter-efficient fine-tuning (PEFT) methods, such as low-rank adaptation (LoRA), are used to adapt models with fewer parameters. These methods typically involve additive approaches where the number of trainable parameters increases linearly with the ambient dimension, which can be inefficient as dimensions grow.", "Idea": "Introduce Quantum-PEFT, a method that utilizes quantum computations and a full-rank quantum unitary parameterization to achieve parameter-efficient fine-tuning. By employing Pauli parameterization, Quantum-PEFT allows the number of trainable parameters to grow logarithmically with the ambient dimension, significantly reducing the parameter count compared to traditional methods like LoRA."}
{"id": "zfeso8ceqr", "Context": "Training language models at scale is becoming increasingly expensive, leading to various attempts to enhance optimization efficiency. Despite these efforts, the Adam optimizer remains the most popular choice due to its perceived effectiveness. However, there is a need to evaluate different optimization algorithms in the context of autoregressive language modeling to determine their performance across various model sizes, hyperparameters, and architecture variants.", "Idea": "The study compares several optimization algorithms, including SGD, Adafactor, Adam, Lion, and Sophia, and finds that except for SGD, they perform comparably. It introduces two simplified versions of Adam: Signum, which maintains Adam's performance and hyperparameter stability, and Adalayer, a layerwise variant that highlights the necessity of adaptivity on both the last layer and LayerNorm parameters for performance and stability."}
{"id": "zJjzNj6QUe", "Context": "Evaluating large language models (LLMs) in diverse scenarios is crucial for aligning them with human preferences. Traditional human evaluations are costly, leading to the use of powerful LLMs as judges, which also presents challenges such as high expenses, privacy concerns, and reproducibility issues.", "Idea": "Propose RocketEval, an automated evaluation method using a lightweight LLM as a judge, which reframes evaluation tasks as multi-faceted Q&A with instance-specific checklists. This method addresses judgment accuracy issues by using checklist grading and reweighting to align with supervised annotations."}
{"id": "fNMKqyvuZT", "Context": "Generative Flow Networks (GFlowNets) are a new type of probabilistic samplers that excel in generating diverse sets of high-reward candidates, unlike traditional return maximization methods such as reinforcement learning, which often focus on a single optimal solution. Recent advancements have aimed at developing goal-conditioned GFlowNets to train a single model capable of achieving various specified outcomes. However, these models face challenges due to extremely sparse rewards, especially in high-dimensional problems, and suffer from limited coverage of explored trajectories during training, which is exacerbated when only offline data is available.", "Idea": "Introduce a novel method called Retrospective Backward Synthesis (RBS) for goal-conditioned GFlowNets, which synthesizes new backward trajectories to enhance the quality and diversity of training trajectories, providing abundant learnable signals to effectively address the sparse reward problem."}
{"id": "MKEHCx25xp", "Context": "Evaluating large language models (LLMs) using real-world user queries is challenging due to the complexity and variability of human-chatbot interactions. Traditional evaluation methods often rely on a single baseline model and may not adequately address issues such as length bias in model responses, leading to less reliable and interpretable results.", "Idea": "Introduce WildBench, an automated evaluation framework that benchmarks LLMs using 1,024 tasks from human-chatbot conversation logs. It employs two metrics, WB-Reward and WB-Score, for systematic evaluation with task-specific checklists and structured explanations. WB-Reward uses fine-grained pairwise comparisons with multiple baseline models, while WB-Score provides a fast, cost-efficient individual output evaluation, incorporating a method to mitigate length bias."}
{"id": "JvH4jDDcG3", "Context": "Deep clustering has shown impressive results, but it suffers from an overconfidence problem where the predicted confidence for a sample's cluster membership is much higher than its actual prediction accuracy. This issue has not been adequately addressed in previous research.", "Idea": "Develop a calibrated deep clustering framework featuring a dual head model with a calibration head and a clustering head. The calibration head adjusts overconfident predictions, aligning prediction confidence with the model's learning status, while the clustering head selects reliable high-confidence samples for pseudo-label self-training. Additionally, introduce a network initialization strategy to improve training speed and robustness."}
{"id": "E48QvQppIN", "Context": "Biologists aim to develop effective therapeutics by iteratively mutating antibody sequences to enhance their binding and stability. This process can be guided by previous experimental data or by learning from extensive antibody databases, but the vastness of the typical antibody space makes it challenging to find suitable antibodies within budget constraints.", "Idea": "Introduce Clone-informed Bayesian Optimization (CloneBO), a method that leverages a generative model trained on clonal families to optimize antibody sequences. This approach uses a large language model, CloneLM, to design mutations that align with the immune system's natural optimization process, and incorporates a twisted sequential Monte Carlo procedure to guide designs based on previous measurements."}
{"id": "7xCSK9BLPy", "Context": "Evaluating instruction-following large language models (LLMs) at a human-level is crucial for improving their performance. Traditional methods like greedy decoding and best-of-N decoding with reference-free judges have limitations in selecting high-quality outputs. There is a need for more effective evaluation and supervision techniques to enhance the test-time performance of these models.", "Idea": "Utilize Minimum Bayes Risk (MBR) decoding with reference-based LLM judges to improve the test-time performance of instruction-following LLMs. Additionally, implement iterative self-training on MBR-decoded outputs using Direct Preference Optimisation to retain performance improvements while reducing test-time costs."}
{"id": "dDpB23VbVa", "Context": "The development of next-generation Large Language Models (LLMs) is hindered by the high training costs associated with these models. This has become a significant bottleneck, as reducing these costs without compromising performance is a major challenge.", "Idea": "Introduce patch-level training for LLMs, where multiple tokens are aggregated into a 'patch' to serve as the fundamental text unit for training. This approach involves training the model on shorter sequences of patches to predict the next patch, followed by token-level training to align with inference mode, thereby significantly reducing training costs."}
{"id": "lmKJ1b6PaL", "Context": "Causal opacity refers to the challenge of understanding the hidden causal structures in deep neural network (DNN) models, which complicates the verification and reliability of these systems, particularly in critical applications. This issue highlights a significant challenge at the intersection of deep learning, interpretability, and causality.", "Idea": "Introduce Causal Concept Graph Models (Causal CGMs), which are interpretable models designed to ensure causally transparent decision-making processes."}
{"id": "5ZEbpBYGwH", "Context": "Combining data from different sources can enhance data analysis tasks like clustering. However, existing multi-view clustering methods are often domain-specific or rely on a suboptimal, computationally intensive two-stage process involving separate representation learning and clustering.", "Idea": "Propose an end-to-end deep learning-based multi-view clustering framework applicable to general data types, utilizing a novel permutation-based canonical correlation objective to generate fused representations. Cluster assignments are determined by identifying consistent pseudo-labels across multiple views, with a theoretical analysis provided to show the approximation to supervised linear discriminant analysis (LDA)."}
{"id": "JBXO05r4AV", "Context": "Recent developments in long-context large language models have introduced the many-shot in-context learning paradigm, which suggests that using a larger number of demonstrating examples can enhance performance compared to the traditional few-shot approach. However, it remains uncertain which factors primarily contribute to these benefits and whether merely increasing the number of examples is the most effective strategy for improving many-shot in-context learning.", "Idea": "Propose BRIDGE, an algorithm that alternates between an optimization step using Bayesian optimization to identify influential example sets and a generation step to expand reasoning paths, thereby enhancing many-shot in-context learning by leveraging these influential examples."}
{"id": "HZVIQE1MsJ", "Context": "Learning from preference feedback is a common method for aligning large language models with human values. Traditionally, this involves encoding preference data into a scalar reward model, which connects a value head with a language model to produce a scalar score. However, these scalar models often lack interpretability and are prone to biases present in datasets.", "Idea": "Utilize the language model itself to learn from preference data by prompting it to generate initial judgment pairs with contrastive preferences in natural language. This approach, termed LLM-as-a-Judge, employs Direct Preference Optimization to enhance the model's reasoning capabilities, ensuring interpretability through generated rationales and demonstrating increased robustness against bias."}
{"id": "ZFxpclrCCf", "Context": "In the field of autonomous driving, generating and simulating diverse real-world scenes is crucial, particularly for handling corner cases. Recent methods have utilized neural radiance fields or diffusion models to create novel views or synthetic data in driving scenarios. However, these methods face challenges with unseen scenes and limited video length, resulting in insufficient adaptability for data generation and simulation.", "Idea": "Propose a framework named Glad for generating video data in a frame-by-frame manner, ensuring temporal consistency through a latent variable propagation module that uses previous frame features as noise prior. Additionally, introduce a streaming data sampler to sample original images in a video clip continuously, allowing Glad to function as a streaming simulator for specific scenes."}
{"id": "nDj45w5wam", "Context": "Current Reinforcement Learning (RL) methods often face challenges with sample inefficiency due to exploration strategies that fail to consider causal relationships among states, actions, and rewards. Recent causal approaches attempt to address this issue but lack a grounded modeling of reward-guided causal understanding, which is crucial for goal-oriented learning and efficiency.", "Idea": "Propose a method called Causal Information Prioritization (CIP) that enhances sample efficiency by using factored MDPs to infer causal relationships between states, actions, and rewards. CIP prioritizes causal information through counterfactual data augmentation and integrates a causality-aware empowerment learning objective to improve reward-guided exploration in complex environments."}
{"id": "pPyJyeLriR", "Context": "Graph unlearning is crucial for privacy protection in applications using Graph Neural Networks (GNNs) with sensitive data. Certified graph unlearning offers strong privacy guarantees but is inefficient for large-scale graphs due to the need for costly re-computation of graph propagation for each unlearning request. While scalable techniques exist to speed up graph propagation, they introduce approximation errors, which conflict with the exact node embeddings required for certified guarantees.", "Idea": "Introduce ScaleGUN, a method that scales certified graph unlearning to billion-edge graphs by integrating approximate graph propagation techniques while maintaining certified guarantees for node feature, edge, and node unlearning scenarios."}
{"id": "FCMpUOZkxi", "Context": "Stochastic contextual bandits with knapsack constraints (CBwK) involve a learner who observes a context, takes an action, receives a reward, and incurs a vector of costs in each round. The goal is to maximize cumulative rewards over a series of rounds while adhering to knapsack constraints with a given budget. This problem is particularly challenging in the small budget regime where the budget is proportional to the square root of the number of rounds.", "Idea": "Propose an Adaptive and Universal Primal-Dual algorithm (AUPD) that achieves strong regret performance in CBwK settings. AUPD is designed to balance reward maximization and budget consumption through an adaptive budget-aware approach, utilizing the Lyapunov drift method for analyzing budget consumption and refining the analysis of cumulative variance."}
{"id": "IXyfbaGlps", "Context": "Group convolutional neural networks (GCNNs) have been developed to handle various geometric transformations, including rotations and scaling, and have shown improved interpretability, accuracy, and generalizability. However, their application in perceptual quantities, such as color variations, has been limited. The CEConv network attempted to address hue transformations using GCNNs but faced issues with invalid RGB values, which compromised performance and equivariance.", "Idea": "Introduce a lifting layer in GCNNs to transform input images directly, resolving the issue of invalid RGB values and significantly improving equivariance error. Extend color equivariance to include saturation and luminance shifts, enhancing generalization to perceptual variations and sample efficiency."}
{"id": "6jxUsDAdAu", "Context": "Benign overfitting is a phenomenon where an over-parameterized model fits the training data perfectly, including noise, yet still generalizes well to unseen test data. While previous research has explored this under in-distribution conditions, modern machine learning often deals with Out-of-Distribution (OOD) scenarios, where the test distribution differs from the training distribution.", "Idea": "This work explores benign overfitting in the OOD regime by examining over-parameterized linear models under covariate shift, providing non-asymptotic guarantees for benign overfitting in standard ridge regression under specific structural conditions of target covariance. It also identifies key quantities affecting OOD generalization and presents theoretical results showing that Principal Component Regression (PCR) achieves a faster statistical rate than standard ridge regression for a general family of target covariance matrices."}
{"id": "K7xpl3LZQp", "Context": "Large vision-language models (LVLMs) have shown exceptional capabilities in image understanding and dialogue, making them suitable for various visual question answering tasks. However, their availability has led to concerns about unauthorized use and copyright infringement, as individuals can fine-tune published models to create their own LVLMs.", "Idea": "Propose a method called Parameter Learning Attack (PLA) to track the copyright of LVLMs without altering the original model. This involves creating adversarial images that prompt specific outputs from the model, allowing it to learn trigger images by updating parameters in the opposite direction during adversarial attacks, ensuring effectiveness on fine-tuned models for copyright tracking."}
{"id": "YcML3rJl0N", "Context": "Fine-tuned large language models often suffer from overconfidence, especially when trained on small datasets, leading to poor calibration and inaccurate uncertainty estimates. Evidential Deep Learning (EDL) offers a computationally efficient method for uncertainty estimation but is susceptible to overfitting, resulting in overly concentrated probability distributions.", "Idea": "Propose regularizing Evidential Deep Learning by incorporating an information bottleneck, creating IB-EDL, which suppresses spurious information and enhances the influence of predictive information on predictions and uncertainty estimates."}
{"id": "DJSZGGZYVi", "Context": "Recent studies have identified that the denoising process in generative diffusion models can create meaningful representations, but these are not as effective as those from self-supervised learning methods. A significant challenge in training large-scale diffusion models is effectively learning these representations, which can be improved by using high-quality external visual representations instead of relying solely on the models themselves.", "Idea": "Introduce a regularization technique called REPresentation Alignment (REPA), which aligns the projections of noisy input hidden states in denoising networks with clean image representations from external, pretrained visual encoders to enhance training efficiency and generation quality."}
{"id": "QC2qE1tcmd", "Context": "Topological deep learning (TDL) is increasingly used for modeling complex interactions in relational data. Despite its potential, issues like oversquashing in topological message-passing are not well understood and lack comprehensive theoretical analysis.", "Idea": "Propose a unifying axiomatic framework that connects graph and topological message-passing by interpreting simplicial and cellular complexes as relational structures, thereby extending graph-theoretic results and algorithms to higher-order structures to address oversquashing in topological message-passing networks."}
{"id": "X6y5CC44HM", "Context": "The field of topological deep learning (TDL) is focused on designing neural networks that can leverage higher-order interactions in complex systems, particularly using higher-order domains like simplicial complexes. However, the advancement of this field is limited by the lack of datasets suitable for benchmarking these higher-order models.", "Idea": "Introduce MANTRA, a comprehensive dataset specifically designed for benchmarking higher-order models, consisting of over 43,000 and 250,000 triangulations of surfaces and three-dimensional manifolds, respectively, to facilitate the evaluation and development of topological methods in TDL."}
{"id": "uMEsKEiB7J", "Context": "Recent advancements in Large Language Models (LLMs) have significantly improved natural language processing capabilities, particularly in understanding long contexts. However, evaluating these models' abilities to comprehend extended narratives remains challenging due to the inadequacies of existing benchmarks.", "Idea": "Introduce NovelQA, a benchmark specifically designed to evaluate LLMs on complex, extended narratives derived from English novels, featuring a comprehensive manual annotation process and diverse question types to assess nuanced comprehension."}
{"id": "WOt1owGfuN", "Context": "Large Language Models (LLMs) are computationally intensive, and not all samples and tokens contribute equally to the model's output. Efficiently pruning these models without compromising performance is a significant challenge, as it requires identifying and removing less important weights while maintaining the model's effectiveness.", "Idea": "Introduce Probe Pruning (PP), a framework for online, dynamic, structured pruning of LLMs that operates in a batch-wise manner. PP involves three stages: probing to identify crucial hidden states, history-informed pruning to integrate these states with historical data, and full inference on the pruned model. This method allows for efficient pruning without additional neural network modules or fine-tuning, leveraging a PP importance score to assess weight channel importance."}
{"id": "6XUSDvBFkV", "Context": "Large Language Models (LLMs) have achieved impressive performance but are limited by their memory-intensive nature during inference, which restricts their use on devices with limited resources. Binarization of weights to 1-bit precision can significantly improve computational efficiency, but maintaining model performance while achieving such compression remains a challenge.", "Idea": "Introduce a structural binarization method called STBLLM that employs an N:M sparsity technique and a novel Standardized Importance (SI) metric to assess weight significance. This method allows for layer-wise sparsification with varying N:M ratios and applies distinct quantization schemes to different weight regions, supported by a specialized CUDA kernel for efficient implementation."}
{"id": "N5fVv6PZGz", "Context": "Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures have demonstrated strong performance across various tasks. However, their large model sizes pose challenges in resource-constrained environments, particularly where GPU memory is limited. Existing solutions attempt to leverage CPU resources but often encounter issues such as significant data transfer overhead between CPU and GPU or fail to account for the distinct characteristics of these processors.", "Idea": "Propose Fiddler, a resource-efficient inference system for MoE models that optimally utilizes both CPU and GPU resources by determining the best execution strategy for environments with limited GPU resources."}
{"id": "78Nn4QJTEN", "Context": "Auto-regressive language models often exhibit a phenomenon known as 'attention sink,' where significant attention is given to the first token regardless of its semantic importance. This issue is prevalent in various applications like streaming generation and model optimization, yet a comprehensive understanding of it is lacking. Attention sinks are observed universally across different models and emerge during pre-training, influenced by factors such as optimization, data distribution, and model architecture.", "Idea": "Investigate the emergence of attention sinks in language models and propose replacing softmax attention with alternative operations like sigmoid attention without normalization to prevent the formation of attention sinks in models up to 1B parameters."}
{"id": "lqHv6dxBkj", "Context": "Sparse pretraining of large language models (LLMs) often leads to reduced accuracy. To address this, previous approaches have relied on using dense models during the fine-tuning phase, which can be resource-intensive and inefficient.", "Idea": "Introduce SLoPe, a method that enhances the accuracy of sparsely pretrained LLMs by incorporating low-rank adapters in the final stages of pretraining. Additionally, SLoPe employs a double-pruned backward pass with N:M sparsity structures to accelerate sparse backward passes without significant overhead."}
{"id": "8DBTq09LgN", "Context": "Programmatic reinforcement learning (PRL) aims to represent policies through programs to achieve interpretability and generalization. However, current PRL methods suffer from sample inefficiency, requiring extensive program-environment interactions.", "Idea": "Introduce a novel LLM-guided search framework (LLM-GS) that utilizes the programming expertise and reasoning capabilities of large language models to enhance search efficiency. This includes a Pythonic-DSL strategy for generating domain-specific language programs and a Scheduled Hill Climbing algorithm to optimize the search process."}
{"id": "XdRIno98gG", "Context": "Self-supervised monocular depth estimation (SSMDE) aims to predict dense depth maps from monocular images without requiring ground-truth depth labels, by learning from RGB image sequences. This approach simplifies data acquisition compared to supervised methods but faces challenges with reflective surfaces, which violate Lambertian reflectance assumptions, leading to inaccurate training on such surfaces.", "Idea": "Propose a novel training strategy for SSMDE using triplet mining to identify reflective regions at the pixel level, guided by camera geometry between different viewpoints. Introduce a reflection-aware triplet mining loss to penalize inappropriate photometric error minimization on reflective regions while maintaining depth accuracy on non-reflective areas, and incorporate a reflection-aware knowledge distillation method for selective pixel-level learning."}
{"id": "Vp2OAxMs2s", "Context": "In scientific research, obtaining a generative model of system dynamics from observed time series is crucial. While effective methods exist for reconstructing dynamical systems from single-domain data, integrating data from multiple dynamical regimes to enhance generalization remains a challenge. This is particularly significant when individual time series are short, necessitating the use of group-level information to compensate for gaps in single-domain data.", "Idea": "Introduce a hierarchical framework that integrates group-level multi-domain information while preserving single-domain characteristics, enabling the discovery of common low-dimensional feature spaces where datasets with similar dynamics cluster. This unsupervised methodology also facilitates transfer learning and generalization to new parameter regimes."}
{"id": "u1cQYxRI1H", "Context": "Diffusion-based image generators are increasingly used for illumination harmonization and editing, but face challenges in scaling up training due to difficulties in preserving image details and maintaining intrinsic properties like albedos. Without proper constraints, training with complex or varied data can lead to random image generation rather than precise illumination manipulation.", "Idea": "Propose the Imposing Consistent Light (IC-Light) transport method during training, which leverages the physical principle of consistent linear blending of appearances under different illumination conditions to ensure stable and scalable illumination learning, allowing for uniform handling of diverse data sources and maintaining intrinsic image properties while modifying illumination."}
{"id": "58lbAsXCoZ", "Context": "Incompressible fluid simulation on surfaces is crucial for visual effects, liquid crystal film design, and scientific analysis of atmospheric and oceanic phenomena. This task faces challenges such as extending physical laws to 3D surfaces and preserving energy and volume. Traditional methods using grids or meshes for spatial discretization suffer from high memory consumption and lack robustness and adaptability across different mesh qualities and representations. Implicit representation-based simulators offer storage efficiency and continuity but struggle with surface simulation and energy dissipation.", "Idea": "Propose a neural physical simulation framework using implicit neural representation, constructing a parameterized vector field with exterior calculus and the Closest Point Method to ensure divergence-free properties and enable simulations on various surface representations. Implement a covariant derivative-based advection process for surface flow dynamics and energy preservation, achieving higher accuracy, flexibility, and memory efficiency with low energy dissipation."}
{"id": "lBntjGbyv0", "Context": "Large language models have significantly advanced various applications, but their deployment is hindered by memory limitations on local devices. The focus has shifted from enhancing capabilities to improving availability, highlighting the need for efficient memory management. Traditional compression methods, such as quantization, often require fixed compression ratios and separate processes for each setting, complicating deployment in environments with varying memory constraints.", "Idea": "Introduce BitStack, a novel, training-free weight compression method that allows for flexible memory usage and model performance trade-offs. BitStack uses weight decomposition to dynamically adjust model size with minimal data transfer between memory and storage, iteratively decomposing weight matrices and stacking residual blocks as basic transmission units based on available memory."}
{"id": "9VGTk2NYjF", "Context": "Adversarial multiplayer games, particularly polymatrix zero-sum games, are a key focus in multiagent learning due to their efficiently computable Nash equilibria. The complexity of computing Nash equilibria in polymatrix games, where players engage in either zero-sum or coordination games, is of significant interest. While the problem is known to be PPAD-complete for three teams, the complexity for two teams has been unresolved.", "Idea": "Establish that the two-team version of the problem is CLS-hard, demonstrating the complexity of computing Nash equilibria in this setting. Additionally, prove that this complexity is tight when one team consists of multiple independent adversaries, and show the hardness of finding stationary points in specific non-convex-concave min-max constrained optimization problems."}
{"id": "GBfYgjOfSe", "Context": "Developing a generalist model for user interface understanding is difficult due to challenges such as platform diversity, resolution differences, and limited data availability. These foundational issues complicate the creation of a universal model that can effectively operate across various devices and platforms.", "Idea": "Introduce Ferret-UI 2, a multimodal large language model designed for universal UI understanding, featuring support for multiple platform types, high-resolution perception through adaptive scaling, and advanced task training data generation using GPT-4o with set-of-mark visual prompting."}
{"id": "GLWf2fq0bX", "Context": "Humans are adept at reusing prior knowledge to tackle new challenges and develop skills while solving problems. This approach is increasingly popular in creating autonomous agents that can self-evolve like humans. However, existing methods struggle with training efficiency when expanding new skills and do not fully utilize prior knowledge to aid in learning new tasks.", "Idea": "Introduce Parametric Skill Expansion and Composition (PSEC), a framework that iteratively evolves agents' capabilities by maintaining a skill library. This library integrates skill primitives as Low-Rank Adaptation (LoRA) modules for parameter-efficient finetuning, allowing flexible skill expansion and direct skill compositions by merging LoRA modules. Additionally, a context-aware module dynamically activates different skills to collaboratively address new tasks."}
{"id": "Q6PAnqYVpo", "Context": "In natural language processing and computational linguistics, researchers often analyze real language usage using large-scale corpora. Common tools like grep and keyword-in-context concordancers are employed for pattern matching, but they are limited to surface-level string matching, which fails to handle orthographic variations and paraphrasing. Additionally, continuous approaches like dense vector search can be too coarse, retrieving unrelated texts that merely share similar topics.", "Idea": "Propose a novel algorithm that performs soft semantic pattern matching by integrating word embeddings to relax surface-level matching. This algorithm is scalable for large corpora using inverted indexes, and an efficient implementation is provided along with an accessible web tool."}
{"id": "ISqx8giekS", "Context": "Large language models (LLMs) have demonstrated significant potential across various fields, but they face challenges due to high memory requirements and inference costs. Post-training quantization (PTQ) is a technique used to address these issues by reducing memory usage and decoding latency. However, recent quantization methods often rely on specialized computations or custom data formats, limiting their compatibility with popular frameworks and requiring specific hardware and software platforms. Additionally, these methods can be resource-intensive and computationally demanding, making it difficult to scale to models with hundreds of billions of parameters.", "Idea": "Introduce LeanQuant, a novel quantization method that is accurate, versatile, and scalable. It addresses the limitations of prior methods by learning loss-error-aware grids instead of using non-adaptive min-max affine grids, preserving model quality and enhancing compatibility with a wider range of quantization types and frameworks."}
{"id": "KGZAs8VcOM", "Context": "3D assets created through reconstruction and generation have reached a quality comparable to manually crafted assets, but their potential is hindered by the need to convert them into meshes for industry applications. Current mesh extraction methods produce meshes that are inferior to those created by human artists, as they rely on dense faces and overlook geometric features, resulting in inefficiencies and lower representation quality.", "Idea": "Introduce MeshAnything, a model that treats mesh extraction as a generation problem, producing artist-quality meshes aligned with specified shapes. The architecture includes a VQ-VAE for learning a mesh vocabulary and a shape-conditioned decoder-only transformer for shape-conditioned autoregressive mesh generation."}
{"id": "OQqNieeivq", "Context": "The growing size of large language models (LLMs) leads to increased computational and memory demands when adapting these models for specific tasks or domains. Parameter-efficient fine-tuning (PEFT) methods have been developed to address these issues by updating a small set of parameters for task-specific model adjustments. However, existing methods like LoRA and its variants fail to account for noisy or irrelevant knowledge, which can negatively affect model performance.", "Idea": "Introduce Knowledge-aware Singular-value Adaptation (KaSA), a PEFT method that utilizes singular value decomposition (SVD) with knowledge-aware singular values to dynamically activate relevant knowledge for specific tasks."}
{"id": "tozlOEN4qp", "Context": "Diffusion models are known for their high-quality generation across various applications, but they struggle to accurately capture rare or extreme events in heavy-tailed distributions. Traditional diffusion and flow-matching models with standard Gaussian priors are inadequate for modeling heavy-tailed behavior.", "Idea": "Repurpose the diffusion framework for heavy-tail estimation by using multivariate Student-t distributions, developing a tailored perturbation kernel, and deriving a denoising posterior based on the conditional Student-t distribution. Introduce a training objective inspired by γ-divergence for heavy-tailed distributions, allowing controllable tail generation with a single scalar hyperparameter, and propose t-EDM and t-Flow as extensions of existing models using a Student-t prior."}
{"id": "roNSXZpUDN", "Context": "Current benchmarks for language agents are inadequate for ensuring interaction with human users or adherence to domain-specific rules, which are crucial for safe and realistic deployment. These benchmarks fail to simulate dynamic conversations in specific domains, such as retail and airline, where agents must follow guidelines and use domain-specific tools.", "Idea": "Introduce τ-bench, a benchmark designed to simulate dynamic conversations in retail and airline domains, using language models to emulate users and providing agents with domain-specific API tools and policy guidelines. Implement an evaluation process that compares the database state post-conversation with the annotated goal state, and propose a new metric, pass^k, to assess agent reliability over multiple trials."}
{"id": "SVRRQ8goQo", "Context": "Evaluating models' reasoning abilities in out-of-distribution settings often relies heavily on domain-specific knowledge, which can limit the accuracy of such evaluations. There is a need for a method that minimizes this reliance to better assess models' reasoning capabilities.", "Idea": "Introduce Knowledge-Orthogonal Reasoning (KOR) and the Knowledge-Orthogonal Reasoning Benchmark (KOR-Bench), which includes five task categories designed to evaluate models' ability to apply new rule descriptions to solve novel rule-driven questions without relying on domain-specific knowledge."}
{"id": "kx8i1yfkRX", "Context": "The active online assortment optimization problem with preference feedback is a framework for modeling user choices and maximizing subsetwise utility, applicable in areas like ad placement, online retail, and recommender systems. Despite previous studies, existing solutions lack practicality and efficiency, often requiring unrealistic conditions such as a 'strong reference' item in choice sets and repeated offering of the same assortments until selection, which are not feasible in real-world applications.", "Idea": "Develop efficient algorithms for regret minimization in assortment selection using Plackett Luce based user choices, introducing a novel concentration guarantee for estimating score parameters through 'Pairwise Rank-Breaking', thus overcoming the limitations of existing methods."}
{"id": "8G3FyfHIko", "Context": "Recent interactive point-based image manipulation methods have become popular due to their user-friendly nature. However, these methods encounter two main types of ambiguity issues: intention ambiguity, which leads to misinterpretation of user purposes, and content ambiguity, where target image areas are distorted by distracting elements.", "Idea": "Propose a novel task-aware, training-free framework called GDrag, which defines a taxonomy of atomic manipulations to reduce intention ambiguity and introduces two strategies, ADT and SMS, to mitigate content ambiguity by modeling fine-grained target contexts and generating precise trajectories."}
{"id": "wQEdh2cgEk", "Context": "Process Reward Modeling (PRM) is essential for tasks involving complex reasoning and decision-making, where the accuracy of each intermediate step significantly impacts the final outcome. Current PRM methods, typically treated as classification problems, use cross-entropy loss to evaluate each step's correctness independently. This approach can result in suboptimal reward distribution and fails to adequately consider the interdependencies between steps.", "Idea": "Introduce the Process Q-value Model (PQM), a new framework that redefines PRM within a Markov Decision Process context. PQM optimizes Q-value rankings using a novel comparative loss function, improving the model's ability to capture the complex dynamics among sequential decisions."}
{"id": "UiEjzBRYeI", "Context": "The Segment Anything model (SAM) has demonstrated a generalized capability to group image pixels into patches. However, it encounters significant challenges when applied to semantic-aware segmentation, particularly in handling a large number of semantic classes and patches.", "Idea": "Introduce SAM-CP, a method that uses two types of composable prompts to enhance SAM for versatile segmentation. Type-I prompts assess alignment between SAM patches and text labels, while Type-II prompts determine if two SAM patches with the same text label belong to the same instance. A unified framework is established to calculate affinity between queries and SAM patches, facilitating the merging of patches with high affinity."}
{"id": "8HuLgtjqOD", "Context": "Training Large Language Models (LLMs) faces a significant communication bottleneck due to the increasing scale of gradient communication across multi-device clusters. Existing compression methods are inadequate, as they often overlook the characteristics of the gradient, making it challenging to reduce communication overhead effectively.", "Idea": "Introduce SEPARATE, a simple low-rank projection method for gradient compression in large-scale model training, leveraging the low-rank properties of gradient and Hessian. SEPARATE uses common random Gaussian variables and an improved moving average error-feedback technique to achieve dimensional reduction while maintaining the original convergence rate for SGD and Adam-Type optimizers."}
{"id": "5xmXUwDxep", "Context": "Diffusion models have shown great promise in generating high-quality images, audio, and videos, but their iterative inference process is computationally expensive, hindering practical applications. Accelerated sampling methods have been developed to reduce the number of timesteps needed for sample generation, but this reduction often leads to increased prediction errors and degraded output quality. Additionally, exposure bias in diffusion models exacerbates these errors.", "Idea": "Utilize a manifold hypothesis to address the exposure bias problem in diffusion models by proposing a manifold constraint that reduces exposure bias during accelerated sampling. This method requires no additional training and minimal hyperparameter tuning."}
{"id": "v6iLQBoIJw", "Context": "Training deep neural networks is complex due to their high-dimensional nature and intricate loss landscapes. Recent findings indicate that during training, the gradient tends to align with a low-rank top eigenspace of the training loss Hessian, known as the dominant subspace. This alignment raises questions about the potential for more efficient training methods by focusing on this subspace.", "Idea": "Investigate the feasibility of training neural networks within the dominant subspace by projecting the SGD update onto it, revealing that this approach does not further decrease the training loss, suggesting the alignment is spurious. Additionally, explore the effectiveness of projecting out the dominant subspace, which surprisingly maintains training efficacy across various setups."}
{"id": "LGafQ1g2D2", "Context": "Large Language Models (LLMs) have become popular in time series forecasting, yet their application in anomaly detection within time series data remains underexplored. There is a need to understand whether LLMs can effectively identify anomalies in time series, especially in zero-shot and few-shot scenarios. Existing conjectures from time series forecasting research suggest potential capabilities of LLMs in this domain, but these have not been thoroughly investigated.", "Idea": "Formulate hypotheses about LLMs' capabilities in time series anomaly detection and design experiments to test these hypotheses, focusing on their understanding of time series as images, their reasoning abilities, and performance variations across different models."}
{"id": "0DZEs8NpUH", "Context": "Large language models (LLMs) are generally designed to reflect broad human values and behaviors, but they often struggle to capture the unique characteristics and preferences of individual users. This limitation presents challenges in personalizing AI interactions to align with individual user preferences, especially given the constraints of limited personal data, diverse user preferences, and scalability requirements.", "Idea": "Introduce the concept of Personality Alignment, which customizes LLMs' responses to match individual user preferences by utilizing the Personality Alignment with Personality Inventories (PAPI) dataset. Develop an activation intervention optimization method, PAS, to enhance LLMs' alignment with individual behavioral preferences efficiently, using minimal data and computational resources."}
{"id": "pB1XSj2y4X", "Context": "Generative models have become popular in drug discovery as efficient alternatives to traditional virtual screening methods. However, a major limitation of these models is their lack of consideration for synthesizability, which restricts their practical application in real-world drug development.", "Idea": "Introduce RxnFlow, a model that assembles molecules using predefined molecular building blocks and chemical reaction templates to ensure a feasible synthetic pathway. It employs generative flow networks (GFlowNets) for training and incorporates a novel action space subsampling method to handle large action spaces efficiently, allowing for flexible adaptation to new objectives or building blocks without retraining."}
{"id": "WLSrq1254E", "Context": "Large Language Models (LLMs) are trained on extensive corpora and exhibit impressive capabilities, but they are generally designed for broad applications. This general-purpose training may not suffice for specific real-world scenarios, leading to a need for customizing these models to align with new human preferences while maintaining their original abilities. However, the customization of publicly available LLMs remains under-explored.", "Idea": "Propose a method to customize pre-trained LLMs by optimizing the sum of two reward functions: one for the original training and another for new human preferences. This is achieved using the residual Q-learning framework, which allows customization without needing the original reward function, and introduces an adapter module, named Q-Adapter, to approximate the residual Q-function for aligning the LLM with new preferences."}
{"id": "tIBAOcAvn4", "Context": "Hard-label attacks are a challenging type of black-box adversarial attack where only the top-1 predicted label is available. A common approach involves searching for the optimal direction from the benign image to minimize the distance to the adversarial region, transforming the problem into a continuous optimization task. This method, however, incurs a high query cost, and existing solutions attempt to reduce this cost using gradient estimation techniques.", "Idea": "Propose a novel prior-guided approach that enhances ray search efficiency by integrating transfer-based priors from surrogate models into gradient estimators. This method approximates the projection of the true gradient onto a subspace defined by these priors and random directions, improving query efficiency."}
{"id": "Cnwz9jONi5", "Context": "Reward Models (RMs) are essential for aligning language models with human preferences, and their evaluation typically relies on accuracy against a validation set of manually annotated preference data. However, the connection between RM accuracy and the performance of downstream policies is not well understood, and current methods may not fully capture the potential for RM overoptimization.", "Idea": "Investigate the relationship between RM accuracy and policy performance in a synthetic setting, revealing that similar RM accuracies can lead to different policy performances and that the method of measuring accuracy significantly affects its predictive power for policy outcomes."}
{"id": "rxVvRBgqmS", "Context": "Artificial intelligence techniques are increasingly being applied in education, yet designing effective systems for music instrument instruction, particularly for piano, remains challenging. While key presses can be derived from sheet music, the transitional movements between these presses require more detailed guidance for effective piano performance.", "Idea": "Develop a piano-hand motion generation benchmark to guide hand movements and fingerings, using an annotated dataset called PianoMotion10M, which includes 116 hours of piano playing videos with 10 million annotated hand poses. Introduce a baseline model that generates hand motions from audio using a position predictor and a position-guided gesture generator, along with evaluation metrics to assess model performance."}
{"id": "HQHnhVQznF", "Context": "Large Language Models (LLMs) can generate biased responses, leading to representational harms. Traditional methods for evaluating biases in LLM responses across different demographic groups, known as counterfactual bias, are inadequate as they do not scale well with a large number of inputs and lack guarantees.", "Idea": "Propose LLMCert-B, a framework that certifies LLMs for counterfactual bias by providing high-confidence bounds on the probability of unbiased responses for any set of counterfactual prompts, which are prompts differing by demographic groups. This certification is applied to distributions of counterfactual prompts created using prefixes sampled from various prefix distributions, including random token sequences and perturbations in the LLM's embedding space."}
{"id": "YzxMu1asQi", "Context": "Adversarial attacks on language models can manipulate model predictions by targeting activations rather than input tokens. These attacks reveal a scaling law where the number of tokens an attacker can control is linearly related to the number of activations manipulated. The stability of attack resistance across different model sizes and families suggests a dimensionality mismatch between input and output spaces as a potential cause of these vulnerabilities.", "Idea": "Investigate adversarial attacks on language model activations to derive scaling laws for attack susceptibility, demonstrating that controlling a small subset of activations can predict a large number of subsequent tokens. This approach provides insights into the dimensionality theory of adversarial attacks and highlights the potential for stronger attacks compared to those targeting input tokens."}
{"id": "4w99NAikOE", "Context": "Advanced diffusion models have made significant progress in compositional text-to-image generation, but they often show varying strengths in handling attribute binding and spatial relationships. This inconsistency underscores the need for a method that can integrate the strengths of different models to enhance overall composition capability.", "Idea": "Introduce IterComp, a framework that aggregates model preferences from multiple diffusion models using an iterative feedback learning approach to improve compositional generation. It involves creating a composition-aware model preference dataset and employing a closed-loop iterative feedback method for self-refinement of diffusion and reward models."}
{"id": "eNbA8Fqir4", "Context": "The emergence of large language models (LLMs) driven by data scaling laws has highlighted the importance of selecting appropriate pre-training data. Current methods for data selection rely heavily on limited heuristics and human intuition, lacking comprehensive guidelines. This has created a need for more systematic approaches to improve the performance of LLMs.", "Idea": "Introduce a method inspired by 'reverse thinking' to prompt LLMs to self-identify beneficial criteria for performance. Develop a system called DataMan to learn quality ratings and domain recognition, using it to annotate a large pre-training corpus with quality ratings and domain types, thereby facilitating improved data selection for training LLMs."}
{"id": "3MnMGLctKb", "Context": "Generative modeling of single-cell RNA-seq data is essential for tasks such as trajectory inference, batch effect removal, and simulating realistic cellular data. Current deep generative models often use pre-processed continuous gene expression data, which fails to capture the discrete nature of single-cell data, limiting their effectiveness and the ability to incorporate robust noise models. Additionally, there is a lack of exploration in controllable multi-modal and multi-label generation of cellular data.", "Idea": "Introduce CellFlow for Generation (CFGen), a flow-based conditional generative model that maintains the discrete nature of single-cell data, enabling reliable generation of whole-genome, multi-modal, single-cell data. CFGen addresses generative tasks like rare cell type augmentation and batch correction, and includes a novel framework for compositional data generation using Flow Matching."}
{"id": "wJv4AIt4sK", "Context": "The growing size of deep neural networks (DNNs) requires effective model compression techniques to minimize their computational and memory demands. Sparsity and quantization are two widely used methods for compressing DNNs, known for significantly reducing these demands while maintaining model accuracy. However, the interaction between these two methods when used together is not well understood, with many assuming they are orthogonal and do not introduce additional errors when combined.", "Idea": "Provide a mathematical proof demonstrating that sparsity and quantization are non-orthogonal, meaning their combined use can introduce additional errors. Highlight the importance of the order in which these methods are applied, as applying quantization before sparsity can disrupt tensor element importance, and show that even in the correct order, compounded errors can harm accuracy."}
{"id": "3PDklqqqfN", "Context": "Document retrieval tasks often deal with unstructured datasets, which consist of free-form text lacking explicit internal structure. However, documents can sometimes have a semi-structured format, containing fields like article titles, message bodies, or HTML headers, which are not fully utilized in traditional retrieval methods.", "Idea": "Introduce Multi-Field Adaptive Retrieval (mFAR), a framework that processes semi-structured data by decomposing documents into fields, indexing them independently using dense and lexical methods, and employing a model to adaptively predict field importance based on the document query for optimized retrieval."}
{"id": "2ea5TNVR0c", "Context": "Large language models (LLMs) are increasingly used for tasks requiring reasoning, such as mathematics, code generation, and logical reasoning. However, achieving state-of-the-art performance in these areas remains challenging, particularly when compared to proprietary models like GPT-3.5 Turbo. The effectiveness of these models often depends on the quality and specificity of the training data used, especially for complex reasoning tasks.", "Idea": "Introduce EURUS, a suite of LLMs optimized for reasoning, finetuned from existing models and achieving state-of-the-art results. The key innovation is ULTRAINTERACT, a large-scale, high-quality dataset designed for complex reasoning tasks, which supports supervised fine-tuning, preference learning, and reward modeling. This dataset includes reasoning chains, multi-turn interaction trajectories, and pairwise responses, enabling a novel reward modeling objective that improves performance in reasoning tasks."}
{"id": "Tkkrm3pA35", "Context": "Preconditioning is crucial for iterative solutions of large, sparse linear systems in scientific fields. Traditional algebraic approaches, which rely solely on the matrix data, struggle with ill-conditioned matrices, posing significant challenges.", "Idea": "Propose using graph neural networks as a general-purpose preconditioner, offering improved performance for various problems, especially when traditional preconditioners are ineffective."}
{"id": "HE6pJoNnFp", "Context": "Large language models (LLMs) augmented with retrieval are known for their robust performance and versatility by incorporating external contexts. However, as the number of retrieved documents increases, the input length grows linearly, leading to a significant increase in latency.", "Idea": "Introduce Sparse RAG, a paradigm that reduces computation costs through sparsity by encoding retrieved documents in parallel to eliminate latency from long-range attention. LLMs then selectively decode outputs by attending only to highly relevant caches, chosen via special control tokens, combining document assessment and response generation into a single process."}
{"id": "FSjIrOm1vz", "Context": "The scaling of inference computation has enabled long-context large language models (LLMs) to perform better in various settings, particularly for knowledge-intensive tasks. However, merely expanding the context by incorporating more external knowledge does not always improve performance unless this knowledge is effectively utilized. The challenge lies in optimizing the use of increased computational resources to enhance the model's ability to acquire and use contextual information.", "Idea": "Investigate inference scaling for retrieval augmented generation (RAG) by combining multiple strategies such as in-context learning and iterative prompting, beyond just increasing the amount of knowledge. Develop a computation allocation model to predict optimal inference parameters under various constraints, thereby enhancing RAG performance by effectively utilizing scaled inference computation."}
{"id": "Tpjq66xwTq", "Context": "Designing efficient geometries for architectural structures such as shells, towers, and bridges is a costly and iterative process. Traditional optimization methods used for solving these inverse problems are slow and computationally expensive, which limits the speed of iteration and design exploration. Although neural networks offer potential solutions through data-driven amortized optimization, they often require extensive fine-tuning and fail to ensure critical design criteria like mechanical integrity.", "Idea": "Combine neural networks with a differentiable mechanics simulator to create a model that accelerates shape approximation problems for architectural structures represented as bar systems, ensuring compliance with mechanical constraints while closely matching target geometries."}
{"id": "LTDtjrv02Y", "Context": "In computer vision, pre-trained image autoencoders are commonly used, but the application of inverse graphics in 2D latent spaces remains under-explored. This approach could reduce training and rendering complexity and enable interoperability with other latent-based 2D methods. However, a significant challenge is that inverse graphics cannot be directly applied to image latent spaces due to the absence of underlying 3D geometry.", "Idea": "Propose an Inverse Graphics Autoencoder (IG-AE) that regularizes an image autoencoder with 3D-geometry by aligning its latent space with jointly trained latent 3D scenes. This method facilitates the integration of NeRFs into the latent space through a latent NeRF training pipeline, implemented as an open-source extension of the Nerfstudio framework."}
{"id": "9mjZ800m7Y", "Context": "In multi-objective optimization (MOO) for neural architecture search (NAS), the challenge lies in finding a diverse set of Pareto optimal solutions, especially when dealing with expensive objectives that require training neural networks. The task involves balancing performance and hardware metrics across various devices, and previous NAS approaches have simplified this by incorporating hardware constraints into the objective function. However, profiling the Pareto front remains computationally expensive as it requires a separate search for each constraint.", "Idea": "Propose a novel NAS algorithm that encodes user preferences to balance performance and hardware metrics, producing diverse architectures across multiple devices in a single search run. This is achieved by parameterizing the joint architectural distribution via a hypernetwork conditioned on hardware features and preference vectors, allowing for zero-shot transferability to new devices."}
{"id": "armbJRJdrH", "Context": "Robustness is crucial for deep neural networks, particularly in security-sensitive applications. Randomized smoothing offers theoretical guarantees for certifying robustness against adversarial perturbations. Diffusion models have been used for randomized smoothing to purify noise-perturbed samples before classification, but they face challenges with larger perturbations and high computational costs during inference.", "Idea": "Reformulate the generative modeling task along diffusion trajectories in pixel space as a discriminative task in latent space using instance discrimination to align temporally adjacent points. This approach enables implicit denoising-then-classification via a single prediction, significantly reducing inference costs."}
{"id": "PY56Wur7S0", "Context": "Large language models (LLMs) are capable of generating code from examples without being restricted to a domain-specific language (DSL). However, these models lack a search mechanism, as the generated programs are independent and do not consider the value of each line of code in subsequent iterations.", "Idea": "Utilize an LLM as a policy to generate lines of code, joining these lines to implicitly estimate their value in subsequent iterations. Enhance this process by executing each line and annotating it with results, allowing the model to search for programs within a single expanding prompt by reasoning in both syntactic and semantic spaces."}
{"id": "iOMnn1hSBO", "Context": "There is growing interest in decision-focused machine learning methods that improve performance by considering how predictions are used in downstream optimization problems. Current methods for uncertainty quantification fail to incorporate information about these downstream decisions, which is crucial for high-stakes decision-making.", "Idea": "Develop a framework using conformal prediction to create prediction sets that consider a downstream decision loss function, enhancing their suitability for high-stakes decision-making. This approach leverages the strengths of conformal methods, such as modularity and statistical coverage guarantees, while integrating downstream decisions and user-specified utility functions."}
{"id": "dliIIodM6b", "Context": "Human alignment in large language models (LLMs) is a significant research focus. Traditional methods like reinforcement learning from human feedback (RLHF) involve complex processes, including a reward learning stage, to align LLMs with human preferences.", "Idea": "Introduce a novel approach that utilizes the implicit reward model from direct preference optimization (DPO) to further align LLMs. This method constructs a preference dataset from current LLM rewards for subsequent DPO rounds, incorporating length-regularized reward shaping and experience replay to enhance dataset quality."}
{"id": "l11DZY5Nxu", "Context": "Diagnosing the root cause of anomalies in complex interconnected systems is a significant challenge in cloud services and industrial operations. Traditional methods rely on counterfactuals derived from Structural Causal Models (SCMs) trained on historical data, which are unreliable due to the rarity of anomalies that fall outside the training distribution.", "Idea": "Introduce In-Distribution Interventions (IDI), an algorithm that predicts root causes by identifying nodes with anomalous values and assessing if normalizing these values would prevent anomalies, using interventional estimates from in-distribution inputs of the fitted SCM."}
{"id": "zqtql1YmlS", "Context": "Offline reinforcement learning (RL) represents a significant shift in RL research, focusing on improving algorithm performance and training efficiency by determining the optimal subset of offline datasets. Understanding the necessary volume of offline data is crucial for tackling similar challenges.", "Idea": "Propose the identification of Reduced Datasets for Offline RL (ReDOR) by framing it as a gradient approximation optimization problem, transforming the actor-critic framework into a submodular objective, and constructing subsets using a modified orthogonal matching pursuit (OMP) method."}
{"id": "yRKelogz5i", "Context": "Incorporating user preferences into large language models (LLMs) can improve personalization and reliability, making them more applicable to real-world scenarios. However, this can lead to sycophancy, where LLMs prioritize user preferences over the correctness of their outputs due to reliance on spurious correlations.", "Idea": "Develop a novel framework called CAUSM, which uses structured causal models to address sycophancy in LLMs by eliminating spurious correlations through causally motivated head reweighting and calibrating intra-head knowledge along the causal representation direction."}
{"id": "1ThYY28HXg", "Context": "Recent advancements in 2D visual generation have been highly successful, but generating 3D and 4D visuals remains difficult in practical applications. This is primarily due to the scarcity of large-scale 4D data and the lack of effective model designs for these dimensions.", "Idea": "Propose a data curation pipeline to extract camera poses and object motion from videos, leading to the creation of a large-scale real-world 4D scene dataset called CamVid-30K. Develop a framework, GenXD, which uses multiview-temporal modules to disentangle camera and object movements, and employs masked latent conditions to support various conditioning views for generating 3D and 4D scenes."}
{"id": "UpQLu9bzAR", "Context": "The visual-instruction correlation (VIC) problem involves learning reward models for long-horizon manipulation tasks from action-free videos and language instructions. Existing methods struggle with challenges such as lack of sub-stage awareness, difficulty in modeling task complexities, and inadequate object state estimation, which hinder their ability to learn effective rewards for these tasks.", "Idea": "Introduce VICtoR, a hierarchical VIC reward model that provides effective reward signals for long-horizon manipulation tasks by using a novel stage detector and motion progress evaluator, trained solely on primitive motion demonstrations."}
{"id": "tZdqL5FH7w", "Context": "Concept erasure is a technique used to reduce the risk of harmful content generation in diffusion models by selectively unlearning undesirable concepts. Previous methods typically map a specific concept to a fixed generic concept, such as a neutral concept or an empty text prompt. However, this approach is suboptimal as it does not consider the impact of erasing one concept on others.", "Idea": "Introduce the Adaptive Guided Erasure (AGE) method, which models the concept space as a graph and dynamically selects optimal target concepts for each undesirable concept, minimizing unintended side effects and preserving unrelated concepts."}
{"id": "awz1JPyXNK", "Context": "Deep learning has made significant progress in various advanced fields, but its 'black box' nature creates challenges in understanding and trusting the decision-making processes of neural networks.", "Idea": "Introduce InnerSightNet, an algorithm that analyzes the inner workings of deep neural networks by examining neuron communities. It operates in three phases: transforming learnable units into a structured network, aggregating neurons into communities, and evaluating these communities to understand information flow and decision-making."}
{"id": "yWoV4Ca6ji", "Context": "Causal Transformers are used for next-token prediction in sequences, relying on self-attention to encode causal structures. However, the exact mechanism enabling their autoregressive learning capability is not well understood. The challenge lies in understanding how Transformers approximate the next-token prediction when the function governing the sequence is context-dependent and varies with each sequence.", "Idea": "Investigate the approximation ability of causal Transformers by constructing a Transformer that learns the mapping function in-context using a causal kernel descent method. This method estimates the next token based on past and current observations, with theoretical connections to the Kaczmarz algorithm in Hilbert spaces."}
{"id": "CbpWPbYHuv", "Context": "Transformers are widely used across various domains due to their strong fitting capabilities, largely attributed to their inherent nonlinearity. Researchers have explored alternative modules to the ReLU function, such as GeLU and SwishGLU, to enhance nonlinearity and improve representational capacity.", "Idea": "Propose a new category of polynomial composition activations (PolyCom) to optimize transformer dynamics, providing a mathematical analysis of its enhanced expressivity and efficacy. PolyCom networks achieve the optimal approximation rate, requiring minimal parameters to approximate general smooth functions in Sobolev spaces."}
{"id": "JVkdSi7Ekg", "Context": "Robotic manipulation in open-world settings requires not only task execution but also the ability to detect and learn from failures. Recent advances in vision-language models (VLMs) and large language models (LLMs) have improved robots' spatial reasoning and problem-solving abilities, but they still struggle with failure recognition, limiting their real-world applicability.", "Idea": "Introduce AHA, an open-source vision-language model designed to detect and reason about failures in robotic manipulation using natural language. AHA frames failure detection as a free-form reasoning task, identifying failures and providing detailed, adaptable explanations across different robots, tasks, and environments. It is fine-tuned using FailGen, a scalable framework that generates a large-scale dataset of robotic failure trajectories, the AHA dataset."}
{"id": "MQXrTMonT1", "Context": "Large Language Models (LLMs) are increasingly trained on data generated by other LLMs, either because such data becomes part of the pre-training corpus or as a substitute for costly human-annotation. This practice raises concerns about 'model collapse,' a decline in model performance when training sets include generated data. It is generally easier for both humans and machines to distinguish between good and bad examples than to generate high-quality samples.", "Idea": "Investigate the use of verification on synthesized data to prevent model collapse by providing a theoretical framework using Gaussian mixtures, linear classifiers, and linear verifiers. Derive conditions with measurable proxies to assess the effectiveness of verifiers in selecting synthesized data that leads to optimal performance."}
{"id": "DhH3LbA6F6", "Context": "Reinforcement learning (RL) is increasingly used for real-world planning problems, especially in handling large state spaces and time horizons. However, a significant challenge arises when RL methods encounter large, combinatorially structured action spaces, where even representing feasible actions at a single step can require complex discrete optimization.", "Idea": "Propose SEQUOIA, an RL algorithm that optimizes long-term reward over feasible action spaces by embedding a Q-network into a mixed-integer program to select combinatorial actions at each timestep, specifically addressing planning over restless bandits with combinatorial actions."}
{"id": "JtGPIZpOrz", "Context": "Large language models (LLMs) have shown impressive performance but are constrained by the limitations of their training data. Efforts to enhance these models have included generating synthetic data for self-improvement, yet this approach can eventually lead to diminishing returns.", "Idea": "Propose a method where a multiagent society of language models is fine-tuned, with each model starting from the same base and independently specialized through data generated from multiagent interactions, allowing for specialization and diversification across models."}
{"id": "M29nUGozPa", "Context": "SMILES is a textual representation of molecular structures that has become important for pre-trained language models. However, existing pre-trained SMILES language models primarily focus on single-token level supervision, which limits their ability to capture detailed molecular semantic information. Additionally, these models only process corrupted SMILES inputs during pre-training, leading to a mismatch between training and inference phases.", "Idea": "Introduce SMI-Editor, an edit-based pre-trained SMILES language model that disrupts molecular substructures randomly and uses the resulting SMILES to train the model to restore the original structure. This method provides fragment-level training signals and allows the use of valid SMILES inputs, enabling the model to learn to reconstruct complete molecules from incomplete structures."}
{"id": "pDDODPtpx9", "Context": "Quantifying uncertainty is crucial in predictive modeling, particularly for high-stakes decision-making. In classification tasks, uncertainty is naturally included as class probabilities, but regression tasks typically focus on predicting the expected value of the target variable. Probabilistic extensions often rely on parametric distributions around this expected value, which can limit the model's ability to capture complex distributions such as skewed or multi-modal ones.", "Idea": "Propose a nondeterministic neural network regression architecture optimized for loss functions based on a sample-based approximation of the continuous ranked probability score (CRPS), allowing a distribution-free approach by learning to sample from the target's aleatoric distribution instead of predicting explicit densities."}
{"id": "kbm6tsICar", "Context": "Data-driven modeling of dynamical systems is essential in machine learning, particularly for applications requiring a deep understanding of model behavior, such as pharmacokinetic models in drug development. Traditional methods involve discovering closed-form ordinary differential equations (ODEs) and analyzing them, but this process is often time-consuming, requires significant expertise, and can be challenging if the equations are complex or need modification.", "Idea": "Propose a direct semantic modeling approach that predicts the semantic representation of a dynamical system directly from data, bypassing the need for complex post-hoc analysis. This method allows for the incorporation of intuitive inductive biases and direct editing of the model's behavior, simplifying the modeling pipeline and enhancing model transparency and flexibility."}
{"id": "f65RuQgVlp", "Context": "Federated Continual Learning (FCL) is increasingly studied for its ability to handle more realistic and dynamic problems. A significant challenge in this area is catastrophic forgetting, where models tend to prioritize recent tasks at the expense of previously learned knowledge. Current solutions often rely on generative-based methods, which require multiple training epochs and are typically designed for vision tasks, operating in an offline setting with static datasets.", "Idea": "Propose an uncertainty-aware memory-based approach to address catastrophic forgetting in an online scenario with streaming data. This method uses a Bregman Information-based estimator to compute model variance at the sample level, allowing for the retrieval and retraining on specific samples to mitigate forgetting while ensuring data confidentiality and communication efficiency."}
{"id": "6RiBl5sCDF", "Context": "Multi-modal Large Language Models (MLLMs) face challenges in automatic Geometry Problem Solving (GPS) due to the need for understanding diagrams, interpreting symbols, and performing complex reasoning. These models are limited by their pre-training on natural images and texts and the absence of automated verification in problem-solving. Additionally, current geometric specialists are constrained by task-specific designs, reducing their effectiveness for broader geometric problems.", "Idea": "Introduce GeoX, a multi-modal large model designed for geometric understanding and reasoning, featuring unimodal pre-training for a diagram encoder and symbol decoder, geometry-language alignment to bridge modality gaps, and a Generator-And-Sampler Transformer (GS-Former) to enhance query generation and representation filtering. GeoX also incorporates visual instruction tuning to process geometric images and questions, generating verifiable solutions."}
{"id": "v1f6c7wVBm", "Context": "Neural radiance fields have significantly advanced novel-view synthesis, achieving high-quality renderings. However, these methods often compromise on geometry accuracy, which limits their application in areas such as relighting and deformation. The challenge remains to synthesize photo-realistic renderings while accurately reconstructing geometry.", "Idea": "Introduce AniSDF, a novel approach that learns fused-granularity neural surfaces with physics-based encoding for high-fidelity 3D reconstruction. This method balances overall structures and fine geometric details, and uses blended radiance fields with anisotropic spherical Gaussian encoding to separate geometry from reflective appearance, enabling accurate geometry reconstruction and high-quality renderings without complex hyperparameter tuning."}
{"id": "97D725GJtQ", "Context": "Vision-language pre-training models like CLIP have shown strong adaptability to downstream tasks through fine-tuning and are widely used across various applications. However, these models face challenges when downstream tasks have limited image-text paired data, making it difficult to bridge the domain gap between pre-training and target tasks.", "Idea": "Propose a semi-supervised training method called SemiCLIP that enhances CLIP's cross-modal alignment by using a small amount of image-text pairs and a large volume of images without text. Introduce semantic concept mining to improve visual representations by matching images with relevant concepts from labeled data, and construct learnable surrogate captions for unlabeled images to optimize a trapezoidal consistency in the representation space."}
{"id": "x83w6yGIWb", "Context": "As large language models are increasingly used across various fields, there is a growing need for model compression to reduce costs and improve inference efficiency. Post-training pruning is a promising method for this purpose, as it does not require resource-intensive iterative training and only needs a small amount of calibration data to evaluate parameter importance. However, the effects of calibration data on post-training pruning have not been systematically explored, and it is unclear if better calibration data construction strategies exist.", "Idea": "Develop a self-generating calibration data synthesis strategy to construct feasible calibration data, which is crucial for effective post-training pruning, especially at high sparsity levels. This strategy focuses on using a small amount of data that is similar to the pre-training data to enhance the performance of strong pruning methods."}
{"id": "txZVQRc2ab", "Context": "Differentially private diffusion models (DPDMs) are designed to maintain the generative capabilities of diffusion models while ensuring differential privacy for sensitive data. However, current DPDM training methods often experience significant utility loss, large memory requirements, and high inference costs, which limit their practical application.", "Idea": "Introduce RAPID, a novel approach that incorporates retrieval augmented generation into DPDM training. RAPID uses public data to create a knowledge base of sample trajectories, retrieves similar trajectories during early sampling steps, and focuses on training later sampling steps in a differentially private manner."}
{"id": "6jjAYmppGQ", "Context": "Electroencephalography (EEG) is a non-invasive technology used to record brain electrical activity and is applied in areas such as sleep staging, emotion recognition, and motor imagery. However, existing EEG models struggle to generalize in clinical settings due to individual discrepancies among new patients, as these models are typically trained on fixed datasets and cannot adapt to the continual flow of unseen subjects.", "Idea": "Introduce a novel Unsupervised Individual Continual Learning paradigm, called BrainUICL, which allows EEG-based models to continuously adapt to new subjects and absorb new knowledge during each adaptation, thereby enhancing generalization ability for unseen subjects."}
{"id": "9HsfTgflT7", "Context": "Spiking Neural Networks (SNNs) are inspired by brain mechanisms and offer energy-efficient implementation on neuromorphic hardware. However, current direct training approaches limit SNNs to specific time steps, causing issues with deployment on time-step-free event-driven chips and hindering energy-performance balance through dynamic inference time steps.", "Idea": "Introduce Mixed Time-step Training (MTT), a method that enhances the temporal flexibility of SNNs by allowing them to adapt to various temporal structures. MTT assigns random time steps to different SNN stages during training, facilitating communication between stages via modules, and enabling deployment on both time-stepped and fully event-driven platforms."}
{"id": "XnDyddPcBT", "Context": "Recent advancements in large language models, particularly those based on transformer architectures, have generated significant interest in understanding their internal mechanisms. Traditional models often rely on fixed weight-sharing assumptions, which may limit their interpretability and adaptability.", "Idea": "Introduce a novel approach using non-autonomous neural ordinary differential equations to model transformer architectures, where all weights of attention and feed-forward blocks are parameterized through neural networks as functions of a continuous layer index, enhancing interpretability and adaptability."}
{"id": "WzCEiBILHu", "Context": "The Schrödinger Bridge problem aims to find the most likely random evolution between two boundary distributions with respect to a reference process. While effective in Euclidean domains, existing methods struggle with topological domains like graphs and simplicial complexes, which are important for data over network entities such as node signals and edge flows.", "Idea": "Introduce the Topological Schrödinger Bridge problem for matching signal distributions on topological domains by setting the reference process to follow topology-aware stochastic dynamics. Develop models that parameterize unknowns in the optimal process as topological neural networks, learning them through likelihood training."}
{"id": "WpZyPk79Fu", "Context": "High-quality preference data is crucial for aligning foundation models with human values through preference learning. Manual annotation of such data is often time-consuming and costly. Recent methods that allow models to generate and annotate their own preference data can lead to inaccuracies due to shared weights between the reward model and the target model, which amplifies inherent biases.", "Idea": "Propose Anyprefer, a framework that synthesizes high-quality preference data by framing the data synthesis process as a cooperative two-player Markov Game involving a target model and a judge model. Introduce external tools to assist the judge model in accurately rewarding the target model’s responses and implement a feedback mechanism to optimize prompts for both models, enhancing collaboration and improving data quality."}
{"id": "slO3xTt4CG", "Context": "Evaluating the quality of performance metrics is essential to ensure that model outputs align with human preferences. However, existing metrics often excel in specific areas but fail to capture the full range of human preferences, necessitating a systematic approach to calibrate metrics to these diverse aspects.", "Idea": "Introduce MetaMetrics, a calibrated meta-metric that optimizes the combination of existing metrics to better align with human preferences across different modalities in a supervised manner, enhancing their applicability in language and vision tasks."}
{"id": "eB7T1bqthA", "Context": "Multi-armed bandits (MAB) are used in sequential online decision-making where the reward of each decision is an unknown random variable. Traditionally, the focus is on maximizing total reward, but in some scenarios, minimizing the total cost of decisions while meeting a reward constraint is more important. This is particularly relevant in domains where cost is the primary metric constrained by a secondary metric, such as reward, and the rewards are unknown.", "Idea": "Introduce the Pairwise-Elimination (PE) algorithm for the variant of MAB-CS where the reward is constrained by a known reference arm, and generalize it to PE-CS for the variant constrained by the subsidized best reward. Both algorithms are shown to have an order-wise logarithmic upper bound on Cost and Quality Regret, with PE being order-optimal for all known reference arm problem instances."}
{"id": "xQBRrtQM8u", "Context": "Dynamical generative models, such as Flow Matching and denoising diffusion models, are commonly used to produce samples through iterative processes. However, there is a lack of theoretically-sound methods for enhancing these models with reward fine-tuning, which is crucial for improving their performance.", "Idea": "Reframe reward fine-tuning as a stochastic optimal control problem and enforce a specific memoryless noise schedule during fine-tuning. Introduce a new algorithm called Adjoint Matching, which treats SOC problems as a regression problem to enhance the performance of reward fine-tuning in generative models."}
{"id": "DSsSPr0RZJ", "Context": "Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have shown significant capabilities in language and vision reasoning, leading to their use in specialized applications like shopping assistants and AI software engineers. Despite the development of various data science benchmarks to assess their performance, these benchmarks often fail to reflect the complexity of real-world data science tasks due to their oversimplified nature.", "Idea": "Introduce DSBench, a comprehensive benchmark designed to evaluate data science agents on realistic tasks, including 466 data analysis tasks and 74 data modeling tasks from Eloquence and Kaggle competitions, featuring long contexts, multimodal backgrounds, and complex data structures."}
{"id": "hPOt3yUXii", "Context": "Photo-realistic image restoration algorithms are evaluated by distortion measures like PSNR and SSIM, as well as perceptual quality measures such as FID and NIQE. The goal is to minimize distortion without compromising perceptual quality. Current methods often sample from the posterior distribution or optimize a weighted sum of distortion and perceptual quality losses. However, achieving an optimal estimator that minimizes MSE while ensuring the reconstructed images' distribution matches the ground-truth is a challenge.", "Idea": "Introduce Posterior-Mean Rectified Flow (PMRF), an algorithm that approximates the optimal estimator by first predicting the posterior mean and then transporting it to a high-quality image using a rectified flow model, which approximates the optimal transport map."}
{"id": "JDzTI9rKls", "Context": "Existing off-policy reinforcement learning algorithms often depend on an explicit state-action-value function representation, which poses challenges in high-dimensional action spaces due to the curse of dimensionality. This reliance leads to data inefficiency as maintaining such a function in these spaces is difficult.", "Idea": "Introduce an efficient approach called Vlearn that uses only a state-value function as the critic for off-policy deep reinforcement learning, eliminating the need for an explicit state-action-value function. This method employs a weighted importance sampling loss for learning deep value functions from off-policy data, incorporating novel design choices like robust policy updates, twin value function networks, and importance weight clipping."}
{"id": "ugXGFCS6HK", "Context": "Image representations, whether artificial or biological, are often evaluated based on their global geometric structure. However, representations with similar global structures can exhibit significantly different local geometries, which are not adequately captured by existing comparison methods.", "Idea": "Propose a framework for comparing image representations based on their local geometries using the Fisher information matrix. This framework introduces a metric for local geometry around a base image, allowing for optimal differentiation of models by identifying 'principal distortions' that maximize model variance under this metric."}
{"id": "Kwo20MWWCb", "Context": "Distributed learning problems often require efficient algorithms to handle asynchronous computations. Existing methods typically rely on prior information about system delays, which can complicate the tuning process and affect the accuracy of the objective function approximation.", "Idea": "Propose a novel asynchronous bundle method that computes iterates using a more accurate approximation of the objective function without needing prior information about maximal information delay, making it faster and easier to tune."}
{"id": "OUuhwVsk9Z", "Context": "Creating high-quality data for training language-instructed agents in embodied AI is a persistent challenge. Traditional methods often require extensive human annotation, which is time-consuming and limits scalability. The need for robust and scalable data generation methods is critical for advancing language-guided navigation learning.", "Idea": "Introduce a Self-Refining Data Flywheel (SRDF) that autonomously generates and refines large-scale navigational instruction-trajectory pairs through iterative collaboration between an instruction generator and a navigator, eliminating the need for human annotation. This process continuously improves the dataset quality, enhancing the training of language-guided navigation models."}
{"id": "5RZoYIT3u6", "Context": "The increasing size of large language models (LLMs) poses significant challenges for deployment due to their high computational and memory demands. Existing model pruning techniques often rely on external calibration datasets to determine which parameters to prune, limiting their flexibility and scalability across different compression ratios. These methods can also lead to severe performance degradation, especially in downstream tasks, when subjected to higher compression rates.", "Idea": "Introduce PruneNet, a novel model compression method that reformulates model pruning as a policy learning process, decoupling it from the model architecture and eliminating the need for calibration datasets. PruneNet learns a stochastic pruning policy to assess parameter importance based on intrinsic model properties, preserving the spectral structure to minimize information loss."}
{"id": "chanJGoa7f", "Context": "Vision-Language Models (VLMs) are essential for processing and understanding both text and images. These models integrate visual and textual information, but the specific mechanisms of how visual tokens are processed and integrated within the language model component remain underexplored.", "Idea": "Investigate the processing of visual tokens in the language model component of LLaVA by analyzing object information localization, the evolution of visual token representations across layers, and the integration mechanism for predictions, revealing insights into the alignment and interpretability of visual and textual tokens."}
{"id": "1SYUKPeM12", "Context": "Audio plays a crucial role in multimodal video understanding, providing complementary information to visual data. Current Video-LLMs and AV-LLMs struggle to effectively utilize audio information, resulting in poor comprehension and hallucinations.", "Idea": "Introduce a fine-grained AV-LLM called Dolphin, which aligns audio and visual modalities in both temporal and spatial dimensions using an audio-visual multi-scale adapter and interleaved merging. Additionally, develop an audio-visual caption and instruction-tuning dataset, AVU, with 5.2 million diverse data tuples and a novel data partitioning strategy."}
{"id": "Trf0R8eoGF", "Context": "Recent advancements in diffusion models have improved the generation and animation of 4D full-body human-object interactions. However, existing methods are limited by their reliance on SMPL-based motion generation, which suffers from a lack of realistic large-scale interaction data, hindering the creation of everyday HOI scenes. Additionally, diffusion models struggle with understanding the spatial and interaction dynamics between humans and objects.", "Idea": "Introduce AvatarGO, a framework for generating animatable 4D HOI scenes from textual inputs. It addresses spatial challenges with LLM-guided contact retargeting to identify contact body parts from text prompts, and interaction dynamics with correspondence-aware motion optimization using the linear blend skinning function from SMPL-X to construct motion fields for human and object models."}
{"id": "N8Oj1XhtYZ", "Context": "Generating high-resolution images from text descriptions is a computationally intensive task, often requiring large models and significant processing power. Traditional methods struggle with efficiency and speed, especially when aiming for high-quality text-image alignment and high resolutions.", "Idea": "Introduce Sana, a text-to-image framework that efficiently generates high-resolution images using a deep compression autoencoder, linear attention in DiT, a decoder-only text encoder, and Flow-DPM-Solver for efficient training and sampling, enabling deployment on a laptop GPU."}
{"id": "c01YB8pF0s", "Context": "Large Generative Models (LGMs) like GPT and Stable Diffusion are trained on vast and diverse datasets across multiple domains, enhancing the creativity and diversity of their outputs. In contrast, existing graph-generative models have been limited to training on single datasets, preventing them from achieving similar breakthroughs.", "Idea": "Propose a large-scale training paradigm for graph generative models using a corpus of over 5000 graphs from 13 domains, leading to the development of large graph generative models (LGGMs) with superior zero-shot generative capabilities and the ability to be fine-tuned for specific domains. Additionally, introduce a Text-to-Graph generation capability inspired by Stable Diffusion, allowing for detailed control over the generated graphs using textual descriptions."}
{"id": "M8OGl34Pmg", "Context": "In shared environments, successful collaboration between humans and robots depends on the robot's ability to adapt in real-time to human motion. In Social Navigation, robots must maintain proximity to assist humans while avoiding collisions, which requires understanding human trajectories. However, these trajectories are only partially observable from the robot's perspective and are complex to process computationally.", "Idea": "Introduce the Social Dynamics Adaptation model (SDA), which uses a two-stage Reinforcement Learning framework. The first stage encodes human trajectories into social dynamics and learns a motion policy based on this information, while the second stage allows the model to infer social dynamics from the history of previous actions and statuses without direct trajectory access."}
{"id": "iXbUquaWbl", "Context": "Diffusion models optimized via variational inference are used for generating samples from unnormalized target densities by simulating a stochastic differential equation starting from a simple prior, usually a Gaussian distribution. These models face challenges when the prior's support differs significantly from the target distribution, leading to exploration difficulties and large discretization errors. Additionally, learning the prior distribution can cause mode-collapse due to the mode-seeking nature of reverse Kullback-Leibler divergence.", "Idea": "Propose end-to-end learnable Gaussian mixture priors (GMPs) to improve exploration control, adaptability to target support, and expressiveness to counteract mode collapse. Introduce a strategy to iteratively refine the model by adding mixture components during training."}
{"id": "GcbhbZsgiu", "Context": "Machine unlearning is a research area focused on protecting data privacy by allowing the removal of sensitive information from machine learning models. A significant challenge in this field is catastrophic unlearning, where removing specific data inadvertently erases essential knowledge, causing the model to diverge from a retrained version.", "Idea": "Introduce a generator-unlearner framework, MixUnlearn, which uses synthesized mixup samples to regularize the unlearning process. The generator creates challenging mixup examples to guide the unlearner in effectively forgetting target information without losing critical knowledge, using a novel contrastive objective and additional contrastive loss terms."}
{"id": "8X74NZpARg", "Context": "Graph Neural Networks (GNNs) have shown significant success in various graph-based machine learning tasks. However, a major challenge remains in evaluating the importance of neighbors of testing nodes, as it is difficult to assess data importance without having access to test labels.", "Idea": "Propose Shapley-Guided Utility Learning (SGUL), a framework that combines transferable data-specific and model-specific features to approximate test accuracy without ground truth labels. SGUL uses Shapley values as a preprocessing step and feature Shapley values as input to optimize Shapley value prediction directly, reducing computational demands and improving generalization to unseen test-time structures."}
{"id": "VOoJEQlLW5", "Context": "The effectiveness of statistical and machine learning methods is heavily reliant on the characterization of data features. Informative and interpretable latent representations with controlled complexity are crucial for visualizing data structures and facilitating efficient model building through dimensionality reduction. Latent variable models, like Gaussian Process Latent Variable Models (GP-LVM), are popular for learning complex, nonlinear representations as alternatives to Principal Component Analysis (PCA).", "Idea": "Propose a novel class of latent variable models based on the Q-exponential process (QEP), which generalizes GP-LVM with a tunable complexity parameter, offering greater flexibility in managing representation complexity while enhancing interpretability. The approach, called Q-exponential Process Latent Variable Model (QEP-LVM), incorporates sparse variational inference within a Bayesian training framework to ensure scalability."}
{"id": "HfWcFs7XLR", "Context": "Writing compelling fiction involves multiple elements such as plot development, character creation, and the use of evocative language. Large language models have potential in story writing but are currently limited by their dependence on complex prompting, which restricts their usability.", "Idea": "Introduce Agents' Room, a generation framework inspired by narrative theory that breaks down narrative writing into subtasks handled by specialized agents, along with a new dataset and evaluation framework for long narratives."}
{"id": "z2z9suDRjw", "Context": "Machine Learning-based heuristics have demonstrated impressive performance in solving hard combinatorial optimization problems (COPs). However, these approaches typically require a separate, specialized neural model for each problem, necessitating model adjustments and re-training for any problem variation.", "Idea": "Introduce GOAL, a generalist model capable of efficiently solving multiple COPs, featuring a single backbone with light-weight problem-specific adapters for input and output processing. GOAL employs mixed-attention blocks to handle graph-based problems with various node, edge, and instance-level features, and uses a novel multi-type transformer architecture to manage heterogeneous node or edge types."}
{"id": "wMgr7wBuUo", "Context": "In the primate neocortex, neurons with similar functions are often spatially close, a feature that has been simulated in artificial neural networks using Kohonen's self-organizing map (SOM). However, integrating these maps into deep neural networks has been challenging, as self-organized deep neural networks often experience reduced capacity for visual recognition due to a mismatch between bottom-up learning updates and top-down, credit-based learning approaches.", "Idea": "Propose an alternative self-organization algorithm designed to align with top-down learning processes in deep neural networks, enhancing the emulation of cortical topography and improving the performance of topographical models."}
{"id": "imT03YXlG2", "Context": "Adapting foundation models for specific tasks is a common practice in building machine learning systems for various applications. However, the mechanisms involved in this adaptation process remain unclear, particularly in understanding how model inputs are associated with interpretable concepts during adaptation.", "Idea": "Develop a new Sparse Autoencoder (SAE) for the CLIP vision transformer, called PatchSAE, to extract interpretable concepts at granular levels and their spatial attributions, and explore how these concepts influence model outputs in image classification tasks and how prompt-based adaptation techniques affect these associations."}
{"id": "Essg9kb4yx", "Context": "Large language models have shown impressive performance across various tasks, but they face significant security issues. Machine unlearning has been developed to enhance model safety by removing the influence of unwanted data. However, current methods struggle with continuous unlearning requests, leading to potential utility loss, and often overlook data access limitations due to privacy and copyright concerns.", "Idea": "Propose the OOO framework, which includes an Orthogonal low-rank adapter (LoRA) for continual unlearning and an Out-Of-Distribution (OOD) detector to assess input similarity with unlearned data. The framework uses a novel contrastive entropy loss and a glocal-aware scoring mechanism to manage unlearning without relying on retained data."}
{"id": "62Ff8LDAJZ", "Context": "Learning generative models of 3D point clouds is a fundamental challenge in 3D generative learning, with a key property being permutation invariance, meaning the order of points does not affect the shape they represent. Recent models like equivariant OT flows aim to learn permutation invariant generative models for point-based molecular data but face scalability issues with large point clouds and complexity in learning due to the nature of flow trajectories.", "Idea": "Propose not-so-optimal transport flow models that use an offline OT precomputation to efficiently construct OT pairs for training, and introduce a hybrid coupling by combining approximate OT and independent coupling to simplify the learning of target flow models."}
{"id": "stK7iOPH9Q", "Context": "Pre-trained text-to-image diffusion models have potential for improving zero-shot generalization in dense prediction tasks. However, the traditional diffusion formulation used in these models may not be optimal for dense prediction due to differences from image generation tasks. Existing methods often use the original diffusion approach without considering these differences, which can lead to inefficiencies and suboptimal performance.", "Idea": "Introduce Lotus, a diffusion-based visual foundation model that adapts the diffusion process for dense prediction by training to predict annotations directly instead of noise, and reformulating the diffusion process into a single-step procedure. Additionally, implement a novel tuning strategy called detail preserver to achieve more accurate and fine-grained predictions."}
{"id": "YK9G4Htdew", "Context": "The DreamerV3 algorithm has shown exceptional performance in various environments by utilizing a world model based on Recurrent Neural Networks (RNNs). Despite the success of model-based reinforcement learning and the adoption of Transformer architectures for their training efficiency, attempts to replace RNN-based models with Transformer-based models, such as in the STORM approach, have not significantly improved performance. These methods have struggled to develop competitive Transformer-based world models.", "Idea": "Introduce TWISTER, a Transformer-based world model that extends predictions to longer time horizons using action-conditioned Contrastive Predictive Coding, enhancing the learning of high-level temporal feature representations to improve agent performance."}
{"id": "Ze4aPP0tIn", "Context": "Enhancing the multi-step reasoning capabilities of Large Language Models (LLMs) has been challenging. Current verification methods, which aim to improve solution consistency by evaluating outputs, face issues such as sampling inefficiencies and the high cost of acquiring extensive process supervision for training effective verifiers.", "Idea": "Introduce a novel verification method using Twisted Sequential Monte Carlo (TSMC) that refines sampling efforts to focus on promising candidates, improving the efficiency of generating high-quality solutions. This method estimates expected future rewards at partial solutions, simplifying the training process by removing the need for step-wise human annotations."}
{"id": "YslOW2SO6S", "Context": "Subseasonal-to-Seasonal (S2S) climate forecasting is crucial for applications like agriculture planning and disaster preparedness but remains challenging due to its chaotic nature. Recent data-driven models have shown potential but are limited by their inadequate handling of geometric inductive biases, often treating spherical weather data as planar images, which leads to inaccurate spatial representations.", "Idea": "Propose the Circular Transformer (CirT), a geometric-inspired model that addresses the cyclic nature of the graticule by decomposing weather data into circular patches by latitude as input tokens and using Fourier transform in self-attention to capture global information and spatial periodicity."}
{"id": "scKAXgonmq", "Context": "Conventional 2D pose estimation models are limited to specific object categories, restricting their applicability to predefined objects. Category-agnostic pose estimation (CAPE) emerged to address this limitation by enabling keypoint localization across diverse object categories using a unified model that can generalize from minimal annotated support images. Recent CAPE methods rely on arbitrary keypoint definitions annotated on a user-provided support image.", "Idea": "Our work introduces a text-based approach for CAPE, replacing the need for a support image with a pose-graph where nodes represent keypoints described with text. This method leverages the abstraction of text descriptions and the structure of the graph to improve symmetry breaking, structure preservation, and occlusion handling."}
{"id": "Z8RZrvngm5", "Context": "Artificial neural networks are recognized as state-of-the-art models in various applications like natural language processing and image recognition. However, designing an effective neural network is challenging and demands significant computational resources. Neural Architecture Search (NAS) aims to automate the selection of optimal network architectures, but many NAS methods still require training some networks, which is resource-intensive.", "Idea": "Propose the zero-cost proxy Network Expressivity by Activation Rank (NEAR), which uses the effective rank of the pre- and post-activation matrix to identify optimal networks without training. This method also provides a straightforward approach to estimate optimal layer sizes in multi-layer perceptrons and select hyperparameters such as activation functions and weight initialization schemes."}
{"id": "wm5wwAdiEt", "Context": "In collaborative multi-agent systems, effective communication is crucial, especially in scenarios where explicit messaging is not possible. Historically, implicit communication has been a common solution in such situations. Previous approaches to learning implicit communication have primarily relied on the theory of mind (ToM), where agents deduce the mental states and intentions of others by analyzing their actions. However, ToM-based methods struggle with accuracy in complex tasks.", "Idea": "Introduce the Implicit Channel Protocol (ICP) framework, which enables agents to communicate through implicit channels by using a subset of actions called scouting actions. These actions are mapped to encode and decode messages, and training algorithms are proposed for agents to learn messaging and actions using both randomly initialized and delayed information maps."}
{"id": "UvTo3tVBk2", "Context": "Linear Recurrent Neural Networks (LRNNs) like Mamba, RWKV, GLA, mLSTM, and DeltaNet have been developed as efficient alternatives to Transformers for processing long sequences. However, both Transformers and LRNNs face challenges in state-tracking tasks, such as code evaluation, where they fail to solve even simple tasks like parity in a single forward pass. This limitation is attributed to the restricted value range of diagonal state-transition matrices in LRNNs.", "Idea": "Extend the eigenvalue range of LRNNs' state-transition matrices to include negative values, allowing them to solve parity and improve performance on state-tracking tasks. This involves using non-diagonal matrices and ensuring that state-transition matrices are products of identity minus vector outer product matrices with eigenvalues in the range [-1, 1]."}
{"id": "B9kUJuWrYC", "Context": "Federated learning (FL) has seen advancements, but integrating generative models into FL faces challenges like high communication costs and unstable training in environments with heterogeneous data. These issues hinder the effective deployment of generative models in FL, especially in non-IID and privacy-preserving settings.", "Idea": "Propose PRISM, a federated learning framework for generative models that uses a stochastic binary mask to identify a sparse subnetwork with high generative performance, minimizing communication overhead. This approach includes maximum mean discrepancy loss and a mask-aware dynamic moving average aggregation method to ensure stable performance and resource efficiency."}
{"id": "96beVMeHh9", "Context": "In modern medical research, real-time monitoring generates functional longitudinal data, which involves continuous-time measurements of outcomes, treatments, and confounders. This results in complex treatment-confounder feedback loops that traditional causal inference methods struggle to address.", "Idea": "Propose a nonparametric causal identification framework inspired by the coarsened data framework, utilizing stochastic process theory, measure theory, and net convergence. This framework generalizes classical methods like g-computation, inverse probability weighting, and doubly robust formulas to handle time-varying outcomes in functional longitudinal data."}
{"id": "XAN8G0rvoB", "Context": "Detecting training data for large language models is increasingly important, particularly in applications that demand high reliability. Existing methods primarily focus on accuracy but often lack the ability to provide controllable results.", "Idea": "Propose the Knockoff Inference-based Training data Detector (KTD), a method that ensures rigorous false discovery rate control in training data detection by generating synthetic knockoff samples to replace original data points while maintaining contextual integrity. A novel knockoff statistic is used to achieve FDR control and maintain high power."}
{"id": "0oWGVvC6oq", "Context": "In sequential decision problems, agents perform tasks repeatedly, experiencing regret and gaining information that can be used in future rounds. Sometimes, agents can also gather information from external sources to avoid regret. The challenge lies in balancing the information accumulated by the agent with the regret experienced.", "Idea": "Develop information-theoretic methods to establish regret lower bounds and introduce the first Bayesian regret lower bounds based on accumulated information. Additionally, derive regret upper bounds using the information gathered by the agent, demonstrating that information can be exchanged for reduced regret."}
{"id": "OhUoTMxFIH", "Context": "Effective asynchronous planning is crucial for agents that need to manage time delays, reason over diverse long-horizon tasks, and collaborate with other agents. Current benchmarks for large language model agents primarily focus on short-horizon tasks and do not adequately evaluate asynchronous planning capabilities.", "Idea": "Introduce Robotouille, a benchmark environment designed to test LLM agents' ability to handle long-horizon asynchronous scenarios, with datasets that present complex planning challenges requiring management of overlapping tasks and interruptions."}
{"id": "oDbiL9CLoS", "Context": "Language models are known for storing extensive factual knowledge, but their ability to effectively utilize this knowledge for various downstream tasks remains uncertain. This study explores four key knowledge manipulation tasks: retrieval, classification, comparison, and inverse search, highlighting the challenges language models face, particularly in classification, comparison, and inverse search tasks.", "Idea": "Conduct a controlled, synthetic experiment to demonstrate that the difficulties language models face in knowledge manipulation are inherent, even when the knowledge is perfectly stored. This experiment reveals that language models, including advanced ones like GPT-4, struggle with efficiently manipulating pre-trained knowledge, leading to the development of Turing tests to differentiate between human and AI capabilities."}
{"id": "owEQ0FTfVj", "Context": "Glycans are fundamental biomolecules that play crucial roles in biological systems. The growing volume of functional glycan data presents an opportunity for machine learning to enhance glycan understanding. However, there is currently no standardized machine learning benchmark for predicting glycan properties and functions.", "Idea": "Develop a comprehensive benchmark called GlycanML for glycan property and function prediction, incorporating diverse tasks such as glycan taxonomy prediction and protein-glycan interaction prediction. GlycanML supports both sequence and graph representations, allowing evaluation of sequence-based models and graph neural networks. Additionally, introduce the GlycanML-MTL testbed for multi-task learning algorithms to explore the impact of taxonomy prediction on other function prediction tasks."}
{"id": "PstM8YfhvI", "Context": "Understanding how cells respond to external stimuli is essential for deciphering biological mechanisms and advancing therapeutic development. High-content image-based assays offer a cost-effective method to study cellular phenotypes resulting from various interventions, providing valuable insights into biological processes and cellular states.", "Idea": "Introduce MorphoDiff, a generative pipeline that predicts high-resolution cell morphological responses under different conditions using perturbation encoding. This framework uniquely integrates perturbation embeddings as guiding signals within a 2D latent diffusion model to produce guided, high-resolution predictions of cell morphology across chemical and genetic interventions."}
{"id": "69Fp4dcmJN", "Context": "Correlated noise mechanisms like DP Matrix Factorization (DP-MF) have been effective alternatives to DP-SGD in training scenarios with large epsilon and few epochs. The current state-of-the-art, DP-BandMF, optimally balances privacy amplification and noise correlation but faces severe scalability issues, limiting its application in large-scale training scenarios with extensive iterations and numerous model parameters.", "Idea": "Develop techniques to scale DP-BandMF, enabling it to handle over a million training iterations and a billion model parameters without degrading utility at smaller scales."}
{"id": "vf5aUZT0Fz", "Context": "Language model pre-training typically involves using diverse data mixtures to improve performance across various domains and languages. However, this process is resource-intensive and costly due to the significant differences in lexical, syntactic, and semantic aspects of the data sources, which can lead to negative interference known as the 'curse of multilinguality'.", "Idea": "Propose a communication-efficient pre-training framework called DEPT, which separates embeddings from the transformer body and allows simultaneous training on multiple data sources without a shared vocabulary. This method reduces token embedding parameters to match the data source vocabulary, cuts communication costs, and enhances the transformer's adaptability and generalization."}
{"id": "SI2hI0frk6", "Context": "Training multi-modal models over both discrete and continuous data presents challenges, particularly in effectively combining text and image data. Traditional approaches often involve quantizing images and training language models over discrete image tokens, which can be inefficient and limit scalability.", "Idea": "Introduce Transfusion, a method that combines language modeling loss with diffusion to train a single transformer on mixed-modality sequences, incorporating modality-specific encoding and decoding layers to enhance performance and scalability."}
{"id": "xIUUnzrUtD", "Context": "Humans are adept at learning abstract patterns across sequences, filtering out irrelevant details, and transferring these concepts to new sequences. In contrast, many sequence learning models struggle with abstraction, leading to inefficiencies in memory and poor transfer capabilities. Large language models, in particular, have difficulty transferring abstract variables as effectively as humans.", "Idea": "Introduce a non-parametric hierarchical variable learning model (HVM) that learns chunks from sequences and abstracts contextually similar chunks as variables, efficiently organizing memory and uncovering abstractions for compact sequence representations."}
{"id": "falBlwUsIH", "Context": "Out-of-distribution (OOD) detection is crucial for safety-critical autonomous systems to reject invalid inputs that could lead to errors. Traditional OOD detection methods rely on labeled data, which is costly, prompting exploration into self-supervised, unlabeled, and zero-shot OOD detection. However, challenges remain in ensuring the reliability of these methods, particularly in real-world data scenarios.", "Idea": "Identify conditions for theoretical failure in unlabeled OOD detection from an information-theoretic perspective, introducing the concept of 'label blindness' where zero mutual information exists between the learning objective and in-distribution labels. Define a new OOD task, Adjacent OOD detection, to test for label blindness and address a safety gap in existing benchmarks."}
{"id": "nYjAzwor9R", "Context": "Determining meaningful distances between high-dimensional data samples is a critical scientific challenge. Traditional methods often focus on embedding samples in hyperbolic space to compute distances efficiently, but these approaches may not effectively capture latent feature hierarchies within the data.", "Idea": "Propose a new tree-Wasserstein distance (TWD) specifically designed for data with a latent feature hierarchy by embedding features into a multi-scale hyperbolic space using diffusion geometry. Introduce a novel tree decoding method that draws analogies between hyperbolic embeddings and trees to learn the latent feature hierarchy efficiently."}
{"id": "cWHonXThtM", "Context": "Knowledge distillation is a model compression technique that transfers learning representations from large, resource-intensive teacher models to smaller, efficient student models. In the domain of image super-resolution, existing methods are often specific to particular teacher-student architectures, which restricts their adaptability and broader application.", "Idea": "Introduce a novel knowledge distillation framework called Mixture of Priors Knowledge Distillation (MiPKD) for image super-resolution models, applicable to various architectures at both feature and block levels. The framework integrates the teacher's knowledge with the student's features using a Feature Prior Mixer and dynamically propagates reconstructed features during training with a Block Prior Mixer."}
{"id": "hoYFLRNbhc", "Context": "Large language models have shown improvements in machine translation quality, but challenges remain in maintaining translation consistency and accuracy for entire documents. Current research struggles with these issues, particularly in processing and translating documents effectively.", "Idea": "Introduce DelTA, a Document-levEL Translation Agent, which utilizes a multi-level memory structure to store and update information across different granularities, enhancing translation consistency and accuracy. DelTA employs a sentence-by-sentence translation strategy and features components like Proper Noun Records, Bilingual Summary, Long-Term Memory, and Short-Term Memory, managed by auxiliary LLM-based components."}
{"id": "v1B4aet9ct", "Context": "Estimating matrices within the symmetric positive-definite (SPD) cone is crucial for various applications, including computer vision and graph learning. Traditional convex optimization-based estimators are limited in expressivity due to their model-based nature. Designing neural architectures for SPD learning is challenging, especially when additional structural constraints like element-wise sparsity are required. Existing methods either fail to ensure all desired properties or lack expressivity.", "Idea": "Introduce SpodNet, a novel and generic learning module that ensures SPD outputs while supporting additional structural constraints, such as element-wise sparsity, effectively addressing the challenge of learning jointly SPD and sparse matrices."}
{"id": "0fJfVOSUra", "Context": "Mapping AI architectures to GPU hardware is a significant bottleneck in AI development. Despite efforts, custom kernels often fail to achieve optimal performance, even for established operations like linear attention. The complexity and diverse capabilities of GPUs suggest a need for varied techniques to achieve high performance.", "Idea": "Introduce ThunderKittens (TK), a framework that simplifies writing performant AI kernels through key abstractions. These abstractions map to the GPU hierarchy levels: providing 16x16 matrix tiles and PyTorch-like operations at the warp-level, templates for asynchronous operations at the thread-block level, and tools to manage block launch and memory costs at the grid-level."}
{"id": "rGP2jbWt0l", "Context": "Attribution algorithms are used to explain computer vision models by attributing the model's response to specific pixels in the input. Existing methods generate explanations by transforming internal model representations like class activation maps, gradients, attention, or relevance scores. The effectiveness of these attribution maps is evaluated using attribution quality metrics.", "Idea": "Propose a Metric-Driven Attribution method for Vision Transformers (ViT) called MDA, which uses attribution quality metrics to guide the creation of attribution maps. This method optimizes patch order and magnitude across all patch tokens, allowing for a smooth trade-off between sparse and dense attributions by adjusting the optimization objective."}
{"id": "tn2mjzjSyR", "Context": "Recent advancements in large language models (LLMs) have focused on improving their reasoning capabilities. Previous methods have utilized various prompting strategies to aid reasoning, such as step-by-step thinking and program-based solutions. However, these strategies often apply static reasoning actions uniformly across all questions, without adapting to the specific characteristics of each question or the capabilities of the LLM.", "Idea": "Introduce DOTS, a method that enables LLMs to reason dynamically by searching for optimal reasoning trajectories tailored to each question's characteristics and the LLM's capabilities. This involves defining atomic reasoning action modules, searching for optimal action trajectories through iterative exploration, and training an LLM to plan reasoning trajectories for new questions using two learning paradigms: fine-tuning an external LLM as a planner or directly fine-tuning the task-solving LLM."}
{"id": "UvfI4grcM7", "Context": "The transformation of sensory inputs into motor functions is a fundamental aspect of neuroscience and essential for developing embodied intelligence. Sensory-motor integration involves complex neural circuits and diverse neuronal types, posing a challenge in bridging biological realism with behavioral functionality. The columnar structure of the superficial layers of the mouse barrel cortex serves as a model system for studying these processes.", "Idea": "Develop a biologically constrained model of the mouse barrel cortex with 4,218 neurons across 13 subtypes, using anatomical data to guide neural distribution and connection strengths. Introduce a construction and training pipeline tailored for this model, and convert a simulated whisker sweep dataset into a spiking-based format to train and test the network on biologically realistic neural signals."}
{"id": "WOzffPgVjF", "Context": "Transformer models have gained popularity in spatio-temporal video grounding (STVG) due to their end-to-end processing capabilities and promising results. Current Transformer-based STVG methods typically use object queries initialized with zeros, which learn target position information through interactions with multimodal features. However, these zero-initialized queries struggle to capture discriminative target information in complex scenarios, such as those with distractors or occlusion, leading to performance degradation.", "Idea": "Introduce a Target-Aware Transformer for STVG (TA-STVG) that adaptively generates object queries by exploring target-specific cues from video-text pairs. This approach employs two modules: text-guided temporal sampling (TTS) for selecting target-relevant temporal cues using text information, and attribute-aware spatial activation (ASA) for exploiting fine-grained visual attributes, enhancing object query initialization with target-specific cues for improved interaction with multimodal features."}
{"id": "NWb128pSCb", "Context": "Accurate interpretation and visualization of human instructions are essential for text-to-image (T2I) synthesis. Current models face challenges in capturing semantic variations due to changes in word order, and existing evaluation methods, which rely on indirect metrics like text-image similarity, fail to effectively assess these challenges. This results in poor performance on complex or uncommon linguistic patterns being obscured by a focus on frequent word combinations.", "Idea": "Propose a novel metric called SemVarEffect and a benchmark named SemVarBench to evaluate the causality between semantic variations in inputs and outputs in T2I synthesis. Semantic variations are introduced through two types of linguistic permutations, avoiding easily predictable literal variations, to establish an effective evaluation framework for understanding human instructions in T2I synthesis."}
{"id": "IIVYiJ1ggK", "Context": "Recent advancements in Transformer-based large language models have significantly improved natural language processing capabilities. However, these models face challenges due to the high computational costs associated with the classical softmax attention, resulting in a complexity of O(T) for per-token generation, where T is the context length.", "Idea": "Introduce Rodimus, a model that uses a data-dependent tempered selection mechanism within a linear attention-based, purely recurrent framework to reduce memory usage while maintaining accuracy. Rodimus+ enhances this by integrating Sliding Window Shared-Key Attention in a hybrid approach, combining semantic, token, and head compression techniques for improved efficiency."}
{"id": "Ev4iw23gdI", "Context": "Mamba-based architectures have emerged as a promising approach for deep learning models due to their competitive performance and efficient deployment speed. However, existing Mamba multi-modal large language models (MLLM) struggle with extracting visual features effectively, resulting in imbalanced cross-modal alignment between visual and textual data, which adversely affects performance in multi-modal tasks.", "Idea": "Introduce Empowering Multi-modal Mamba with Structural and Hierarchical Alignment (EMMA), which enhances MLLM's ability to extract detailed visual information through a pixel-wise alignment module for spatial image-level feature processing and a multi-scale feature fusion (MFF) module for combining visual features from different layers, ensuring both structural and hierarchical alignment."}
{"id": "oQoQ4u6MQC", "Context": "Text-to-Image (T2I) diffusion models have become popular for generating high-quality images from text descriptions. Despite their success, these models struggle with creating diverse and customized images that incorporate specific visual attributes from reference images. The challenge lies in personalizing these models to adapt abstract concepts or categories while maintaining sufficient variation in the generated images.", "Idea": "Introduce a method that enables a pretrained T2I diffusion model to learn a set of soft prompts, which allows for the generation of novel images by sampling from the learned prompt distribution. This approach provides text-guided editing capabilities and flexibility in controlling variation and mixing between multiple distributions, with adaptability to other tasks such as text-to-3D."}
{"id": "yVGGtsOgc7", "Context": "Intelligent perception and interaction with the world depend on internal representations that capture its underlying structure, known as disentangled or abstract representations. These representations isolate latent factors of variation in the world, facilitating feature-based generalization. The challenge lies in ensuring the emergence of such representations in systems that solve multi-task evidence accumulation classification tasks, which are common in neuroscience.", "Idea": "The study provides theoretical and experimental results that guarantee the emergence of disentangled representations in agents solving multi-task classification tasks. It introduces conditions under which these representations emerge, based on noise, number of tasks, and evidence accumulation time, and offers closed-form expressions for extracting them from the model's latent state. The framework is validated in RNNs and shows robustness across various architectures, with transformers being particularly effective for disentangling representations."}
{"id": "ZTpWOwMrzQ", "Context": "Transformer models have shown outstanding performance in various applications. However, the dot-product attention mechanism, which is central to Transformer models, struggles with scalability for long-context data due to its quadratic time complexity with respect to context length.", "Idea": "Propose Radar, a training-free method that enhances inference by dynamically identifying the most important context tokens, thereby reducing decoding time complexity for any pre-trained Transformer without additional training or heuristic token eviction."}
{"id": "We5z3UEnUY", "Context": "In partially observable environments, effective decision-making requires robust memory management. Current deep-learning memory models, despite their success in supervised learning, face challenges in reinforcement learning settings that are partially observable and long-term. These models struggle to efficiently capture relevant past information, adapt to changing observations, and maintain stable updates over extended episodes.", "Idea": "Introduce the Stable Hadamard Memory, a novel memory model for reinforcement learning agents that dynamically adjusts memory by erasing unnecessary experiences and reinforcing crucial ones. This model uses the Hadamard product to calibrate and update memory, enhancing memory capacity while addressing numerical and learning challenges."}
{"id": "MnJzJ2gvuf", "Context": "Multi-modal Large Language Models (MLLMs) have shown strong performance in general visual tasks, but their mathematical capabilities, particularly in visual encoding of math diagrams, diagram-language alignment, and chain-of-thought reasoning, remain under-explored. There is a need for an effective training paradigm and a comprehensive dataset with detailed CoT rationales, which is difficult and costly to create manually.", "Idea": "Introduce MAVIS, a MAthematical VISual instruction tuning pipeline for MLLMs, featuring an automatic data engine to generate mathematical visual datasets without human intervention. This includes two datasets, MAVIS-Caption and MAVIS-Instruct, and a four-stage training process to enhance diagram visual encoding, vision-language alignment, problem-solving skills, and CoT reasoning capabilities."}
{"id": "3IFRygQKGL", "Context": "In reinforcement learning, planning with options, which are sequences of primitive actions, has proven effective in complex environments. Previous research has primarily focused on using predefined options or options learned from expert demonstration data. However, there is a need for methods that can autonomously discover and utilize options without relying on human knowledge.", "Idea": "Introduce OptionZero, a novel approach that integrates an option network into MuZero, enabling the autonomous discovery of options through self-play games. Additionally, modify the dynamics network to provide environment transitions when using options, allowing for deeper search under the same simulation constraints."}
