{
  "id": "HE6pJoNnFp",
  "target_idea": "Introduce Sparse RAG, a paradigm that reduces computation costs through sparsity by encoding retrieved documents in parallel to eliminate latency from long-range attention. LLMs then selectively decode outputs by attending only to highly relevant caches, chosen via special control tokens, combining document assessment and response generation into a single process.",
  "context": "Large language models (LLMs) augmented with retrieval are known for their robust performance and versatility by incorporating external contexts. However, as the number of retrieved documents increases, the input length grows linearly, leading to a significant increase in latency.",
  "initial_idea": "Develop a dynamic retrieval system for LLMs that employs hierarchical document clustering prior to retrieval. Initially, only summaries or key extracts of top-tier clusters are retrieved and evaluated by the LLM. If the model's confidence in generating a response is below a certain threshold, it then delves deeper into more specific clusters or retrieves full documents, optimizing both response quality and computational efficiency.",
  "final_idea": "Introduce an attention-based mechanism that prioritizes sections of documents during the retrieval phase, focusing computational resources on high-relevance areas and disregarding less pertinent information, thus optimizing processing time.",
  "final_sim_score": 0.72,
  "rounds_run": 40,
  "explored_total": 138,
  "elapsed_sec": 1183.780868768692
}