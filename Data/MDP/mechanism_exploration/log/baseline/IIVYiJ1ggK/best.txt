Best score: 0.72
Best idea:
Develop a transformer-based language model that adopts dynamic context pruning with a focus on semantic relevance detection. This model would use an initial lightweight pass to determine which tokens in the context are most relevant for predicting the next token, subsequently pruning away less relevant tokens to decrease the contextual length T before computing the softmax attention. This approach aims to significantly reduce computational overhead while maintaining or improving the quality of token predictions by focusing computational resources on the most impactful parts of the input sequence.
