{
  "id": "UvTo3tVBk2",
  "target_idea": "Extend the eigenvalue range of LRNNs' state-transition matrices to include negative values, allowing them to solve parity and improve performance on state-tracking tasks. This involves using non-diagonal matrices and ensuring that state-transition matrices are products of identity minus vector outer product matrices with eigenvalues in the range [-1, 1].",
  "context": "Linear Recurrent Neural Networks (LRNNs) like Mamba, RWKV, GLA, mLSTM, and DeltaNet have been developed as efficient alternatives to Transformers for processing long sequences. However, both Transformers and LRNNs face challenges in state-tracking tasks, such as code evaluation, where they fail to solve even simple tasks like parity in a single forward pass. This limitation is attributed to the restricted value range of diagonal state-transition matrices in LRNNs.",
  "initial_idea": "Develop an augmented Linear Recurrent Neural Network model that integrates adaptive state-transition scaling. This model would use a trainable meta-layer that dynamically scales the diagonal elements of the state-transition matrix based on the contextual requirements of the sequence. By learning optimal scaling policies directly from data, the model can expand the effective value range of state transitions, enhancing its ability to manage complex state-tracking tasks like code evaluation.",
  "final_idea": "Develop an augmented Linear Recurrent Neural Network model that integrates adaptive state-transition scaling. This model would use a trainable meta-layer that dynamically scales the diagonal elements of the state-transition matrix based on the contextual requirements of the sequence. By learning optimal scaling policies directly from data, the model can expand the effective value range of state transitions, enhancing its ability to manage complex state-tracking tasks like code evaluation.",
  "final_sim_score": 0.75,
  "rounds_run": 40,
  "explored_total": 58,
  "elapsed_sec": 882.1644930839539
}