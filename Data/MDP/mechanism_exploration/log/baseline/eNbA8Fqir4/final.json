{
  "id": "eNbA8Fqir4",
  "target_idea": "Introduce a method inspired by 'reverse thinking' to prompt LLMs to self-identify beneficial criteria for performance. Develop a system called DataMan to learn quality ratings and domain recognition, using it to annotate a large pre-training corpus with quality ratings and domain types, thereby facilitating improved data selection for training LLMs.",
  "context": "The emergence of large language models (LLMs) driven by data scaling laws has highlighted the importance of selecting appropriate pre-training data. Current methods for data selection rely heavily on limited heuristics and human intuition, lacking comprehensive guidelines. This has created a need for more systematic approaches to improve the performance of LLMs.",
  "initial_idea": "Develop an adaptive data selection framework for LLMs that employs reinforcement learning (RL) to optimize pre-training datasets dynamically. The framework would use an RL agent that evaluates the LLM's performance on a diverse set of proxy tasks to determine the value of including specific subsets of data in the training set. This method continuously refines the data selection process by learning which types of training examples lead to the most substantial performance gains across different types of evaluation tasks, thereby customizing the dataset for optimal efficacy and efficiency in training.",
  "final_idea": "Introduce a meta-learning approach where a model is trained to predict the utility of different data subsets for training LLMs, using historical performance data as a basis for learning. This model could then guide the selection of data in a way that is both context-sensitive and adaptive, optimizing for both short-term gains and long-term generalization capabilities.",
  "final_sim_score": 0.65,
  "rounds_run": 40,
  "explored_total": 115,
  "elapsed_sec": 1207.6045551300049
}