{
  "id": "eB7T1bqthA",
  "target_idea": "Introduce the Pairwise-Elimination (PE) algorithm for the variant of MAB-CS where the reward is constrained by a known reference arm, and generalize it to PE-CS for the variant constrained by the subsidized best reward. Both algorithms are shown to have an order-wise logarithmic upper bound on Cost and Quality Regret, with PE being order-optimal for all known reference arm problem instances.",
  "context": "Multi-armed bandits (MAB) are used in sequential online decision-making where the reward of each decision is an unknown random variable. Traditionally, the focus is on maximizing total reward, but in some scenarios, minimizing the total cost of decisions while meeting a reward constraint is more important. This is particularly relevant in domains where cost is the primary metric constrained by a secondary metric, such as reward, and the rewards are unknown.",
  "initial_idea": "Develop a dynamic adjustment algorithm for multi-armed bandits that incorporates a cost function reflective of real-time economic conditions, like inflation rates, commodity prices, or service pricing. The algorithm would weigh each arm's cost against the probability of meeting or exceeding a reward threshold, continuously updating its choices based on cost fluctuations, shifted priorities, or resource availability. These adjustments ensure optimal decision-making under varying economic scenarios, targeting minimal costs while satisfying reward constraints, thereby offering a practical solution for budget-sensitive sectors like public health, utilities management, or nonprofit operations.",
  "final_idea": "Develop a dual-objective optimization model for multi-armed bandits that integrates a cost minimization layer with a constraint on achieving a minimum threshold of reward, using a modified UCB (Upper Confidence Bound) algorithm that incorporates cost parameters into its confidence bounds.",
  "final_sim_score": 0.68,
  "rounds_run": 40,
  "explored_total": 108,
  "elapsed_sec": 1083.299518108368
}