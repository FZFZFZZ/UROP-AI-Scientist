{
  "id": "7xCSK9BLPy",
  "target_idea": "Utilize Minimum Bayes Risk (MBR) decoding with reference-based LLM judges to improve the test-time performance of instruction-following LLMs. Additionally, implement iterative self-training on MBR-decoded outputs using Direct Preference Optimisation to retain performance improvements while reducing test-time costs.",
  "context": "Evaluating instruction-following large language models (LLMs) at a human-level is crucial for improving their performance. Traditional methods like greedy decoding and best-of-N decoding with reference-free judges have limitations in selecting high-quality outputs. There is a need for more effective evaluation and supervision techniques to enhance the test-time performance of these models.",
  "initial_idea": "Develop a dynamic evaluation protocol that employs a mixed-reality setup, where LLMs interact with both virtual environments and human agents in real-time scenarios to execute complex tasks (e.g., planning a travel itinerary or troubleshooting a software issue). The model's performance can be analyzed based on its ability to adapt to changes in the environment, understand nuanced human feedback, and achieve task success. This method provides continuous feedback loops and diverse interaction data, allowing more granular insights into the model's practical utility and adaptability.",
  "final_idea": "Design an ensemble method where multiple LLMs with varied training backgrounds evaluate and score each otherâ€™s outputs, using consensus to determine areas needing improvement and iteratively retraining the group.",
  "final_sim_score": 0.65,
  "rounds_run": 40,
  "explored_total": 138,
  "elapsed_sec": 1389.9108951091766
}