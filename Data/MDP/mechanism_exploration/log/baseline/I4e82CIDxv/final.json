{
  "id": "I4e82CIDxv",
  "target_idea": "Introduce sparse feature circuits, which are causally implicated subnetworks of human-interpretable features, to provide a detailed understanding of neural network mechanisms. These circuits are based on fine-grained units, making them useful for downstream tasks, such as improving classifier generalization by ablating task-irrelevant features. Additionally, an unsupervised and scalable interpretability pipeline is demonstrated by discovering thousands of sparse feature circuits for automatically discovered model behaviors.",
  "context": "Previous methods for explaining language model behaviors relied on circuits composed of polysemantic and difficult-to-interpret units such as attention heads or neurons. These circuits were not suitable for many downstream applications due to their complexity and lack of interpretability.",
  "initial_idea": "Develop a method that decomposes language model behavior into a series of \"micro-decisions\" based on simpler decision-making modules, called Decision Elements (DEs), that each capture specific, understandable linguistic or cognitive functions. These DEs would be systematically derived from the language model by applying constrained factorization techniques to isolate dependencies and effects that individual neural network components (like specific layers, neurons, or groups of neurons) have on output decisions. This modular approach allows the construction of a decision tree where each path and node are labeled with human-readable explanations of the DEs involved, dramatically enhancing interpretability while maintaining model performance.",
  "final_idea": "Develop a method that decomposes language model behavior into a series of \"micro-decisions\" based on simpler decision-making modules, called Decision Elements (DEs), that each capture specific, understandable linguistic or cognitive functions. These DEs would be systematically derived from the language model by applying constrained factorization techniques to isolate dependencies and effects that individual neural network components (like specific layers, neurons, or groups of neurons) have on output decisions. This modular approach allows the construction of a decision tree where each path and node are labeled with human-readable explanations of the DEs involved, dramatically enhancing interpretability while maintaining model performance.",
  "final_sim_score": 0.85,
  "rounds_run": 40,
  "explored_total": 103,
  "elapsed_sec": 1571.506843805313
}