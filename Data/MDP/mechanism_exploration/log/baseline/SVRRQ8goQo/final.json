{
  "id": "SVRRQ8goQo",
  "target_idea": "Introduce Knowledge-Orthogonal Reasoning (KOR) and the Knowledge-Orthogonal Reasoning Benchmark (KOR-Bench), which includes five task categories designed to evaluate models' ability to apply new rule descriptions to solve novel rule-driven questions without relying on domain-specific knowledge.",
  "context": "Evaluating models' reasoning abilities in out-of-distribution settings often relies heavily on domain-specific knowledge, which can limit the accuracy of such evaluations. There is a need for a method that minimizes this reliance to better assess models' reasoning capabilities.",
  "initial_idea": "Develop a \"reasoning robustness score\" (RRS) for AI models that measures their ability to apply learned logic and reasoning to new, unseen scenarios without previous domain-specific training. This would involve training models on a diverse set of reasoning tasks and subsequently testing them on carefully designed out-of-distribution (OOD) tasks that recombine elements of the training tasks in novel ways. The RRS would quantitatively reflect how well a model generalizes its reasoning skills and adapts to new contexts, thereby providing a standardized metric to compare different models' abilities to handle OOD problems effectively.",
  "final_idea": "Design an 'abstract reasoning corpus' with tasks derived from logical, spatial, and mathematical reasoning that are stripped of domain-specific language and context. Models would be tested on this corpus to evaluate their pure reasoning abilities and their performance stability across varied contexts.",
  "final_sim_score": 0.88,
  "rounds_run": 40,
  "explored_total": 160,
  "elapsed_sec": 1527.8142278194427
}