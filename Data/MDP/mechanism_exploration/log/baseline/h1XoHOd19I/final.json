{
  "id": "h1XoHOd19I",
  "target_idea": "Propose a new domain adaptation framework called Mix-CPT, which includes domain knowledge learning and general format alignment. This framework involves a knowledge mixture continual pre-training that focuses on both knowledge memorization and utilization, and introduces a logit swap self-distillation constraint to prevent catastrophic forgetting. It then uses the acquired knowledge for efficient instruction tuning and alignment with a few general training samples to achieve format alignment.",
  "context": "Adapting large language models (LLMs) to specialized domains often involves using domain-specific corpora for continual pre-training to enhance knowledge memorization, followed by fine-tuning with related instructions to apply this knowledge. This approach can be inefficient as it separates knowledge memorization from utilization and requires LLMs to learn both knowledge utilization and format alignment simultaneously during fine-tuning, which involves divergent training objectives.",
  "initial_idea": "Develop a modular adaptation framework for LLMs that employs a two-phase \"learn-and-link\" mechanism. In the first phase, LLMs undergo domain-specific pre-training with a focus on encoding factual knowledge using a dynamically evolving knowledge graph embedded within the neural architecture to enhance contextual relationships and dependency tracking. In the second phase, a separate but interconnected neural module is introduced that specializes in interpreting and applying this knowledge via task-driven simulations, emphasizing active learning where the model practices decision-making and problem-solving tasks relevant to the domain, therefore linking knowledge retention directly to its application. This approach aims to harmonize the learning of factual content with its practical usage, improving both efficiency and performance in specialized domains.",
  "final_idea": "Develop a dual-stream training architecture where one stream focuses on deep knowledge encoding using domain-specific corpora, and the other stream simultaneously manages task-oriented training. This structure ensures continuous cross-pollination between the streams, enhancing the model's ability to integrate and apply knowledge.",
  "final_sim_score": 0.78,
  "rounds_run": 40,
  "explored_total": 128,
  "elapsed_sec": 1394.1332828998566
}