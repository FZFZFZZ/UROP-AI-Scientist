{
  "id": "M29nUGozPa",
  "target_idea": "Introduce SMI-Editor, an edit-based pre-trained SMILES language model that disrupts molecular substructures randomly and uses the resulting SMILES to train the model to restore the original structure. This method provides fragment-level training signals and allows the use of valid SMILES inputs, enabling the model to learn to reconstruct complete molecules from incomplete structures.",
  "context": "SMILES is a textual representation of molecular structures that has become important for pre-trained language models. However, existing pre-trained SMILES language models primarily focus on single-token level supervision, which limits their ability to capture detailed molecular semantic information. Additionally, these models only process corrupted SMILES inputs during pre-training, leading to a mismatch between training and inference phases.",
  "initial_idea": "Develop a hierarchical pre-training approach for SMILES language models that operates at multiple levels of molecular granularity, such as atoms, functional groups, and entire molecular motifs. This method would use a multi-task learning framework that simultaneously learns to predict the presence and type of individual components and their higher-order interactions, enhancing the model's ability to understand complex molecular relationships and properties. Additionally, integrate a \"molecular cloze\" mechanism where both random and chemically relevant substructures are masked during training, encouraging the model to better infer molecular characteristics in a manner more consistent with how these models are used during inference.",
  "final_idea": "Develop a hierarchical pre-training approach for SMILES language models that operates at multiple levels of molecular granularity, such as atoms, functional groups, and entire molecular motifs. This method would use a multi-task learning framework that simultaneously learns to predict the presence and type of individual components and their higher-order interactions, enhancing the model's ability to understand complex molecular relationships and properties. Additionally, integrate a \"molecular cloze\" mechanism where both random and chemically relevant substructures are masked during training, encouraging the model to better infer molecular characteristics in a manner more consistent with how these models are used during inference.",
  "final_sim_score": 0.78,
  "rounds_run": 40,
  "explored_total": 62,
  "elapsed_sec": 855.2841498851776
}