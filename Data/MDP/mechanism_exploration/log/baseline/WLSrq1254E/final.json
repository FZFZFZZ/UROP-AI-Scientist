{
  "id": "WLSrq1254E",
  "target_idea": "Propose a method to customize pre-trained LLMs by optimizing the sum of two reward functions: one for the original training and another for new human preferences. This is achieved using the residual Q-learning framework, which allows customization without needing the original reward function, and introduces an adapter module, named Q-Adapter, to approximate the residual Q-function for aligning the LLM with new preferences.",
  "context": "Large Language Models (LLMs) are trained on extensive corpora and exhibit impressive capabilities, but they are generally designed for broad applications. This general-purpose training may not suffice for specific real-world scenarios, leading to a need for customizing these models to align with new human preferences while maintaining their original abilities. However, the customization of publicly available LLMs remains under-explored.",
  "initial_idea": "Develop a framework for modular meta-training of LLMs where the base model is trained on a broad corpus and additional, smaller \"overlay\" modules are trained for specific domains or preferences. These overlay modules can be dynamically attached or detached from the base model depending on the user's current needs, allowing for flexible adaptation to multiple scenarios without the computational and data overhead of retraining the entire model. This approach would use reinforcement learning techniques to optimize the selection and blending of these modules in real-time based on user feedback.",
  "final_idea": "Develop a framework for modular meta-training of LLMs where the base model is trained on a broad corpus and additional, smaller \"overlay\" modules are trained for specific domains or preferences. These overlay modules can be dynamically attached or detached from the base model depending on the user's current needs, allowing for flexible adaptation to multiple scenarios without the computational and data overhead of retraining the entire model. This approach would use reinforcement learning techniques to optimize the selection and blending of these modules in real-time based on user feedback.",
  "final_sim_score": 0.82,
  "rounds_run": 40,
  "explored_total": 121,
  "elapsed_sec": 1333.0734298229218
}