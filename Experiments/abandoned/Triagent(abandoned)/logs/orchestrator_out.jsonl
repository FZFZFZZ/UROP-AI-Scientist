{"uid": "p_alphaedit", "problem": "Large language models (LLMs) often exhibit hallucinations and stale facts; locating-then-editing methods face a trade-off between update accuracy and preserving unrelated knowledge, especially under sequential edits.", "aligned": false, "iters": 6, "student": "We propose a refined approach to model editing in large language models, addressing the trade-off between update accuracy and knowledge preservation under sequential edits. Our method leverages a meta-learning paradigm to modulate parameter updates, ensuring that new knowledge is incorporated without compromising existing information. A principled subspace decomposition technique is employed to define a protected subspace, safeguarding unrelated knowledge from interference. The optimization objective is reformulated to focus on the new knowledge, implicitly preserving existing knowledge through a novel distributional alignment objective. Furthermore, we introduce a parameter control mechanism, utilizing orthogonal projections to guarantee non-interference with the representation space of preserved knowledge. This approach eschews explicit constraints, instead relying on mathematical operations to ensure stability and preserve semantic structure, thereby providing a more focused and effective solution to the challenge of sequential edits in large language models.", "judge": {"aligned": false, "reason": "The student's proposal mentions orthogonal projections and protected subspace but does not specifically reference the 'null space of preserved knowledge,' which is a required key concept."}}
