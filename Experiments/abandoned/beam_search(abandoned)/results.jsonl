{"custom_id": "2e8f2d9e1823256c", "problem": "Deep learning models for tabular data have yet to consistently outperform gradient-boosted decision trees, and existing retrieval-augmented approaches for tabular DL are often complex and inefficient.", "target_idea": "Develop a feed-forward neural network architecture incorporating a custom k-Nearest-Neighbors-inspired retrieval component with an attention-like mechanism to efficiently leverage information from similar training instances for improved tabular data modeling.", "best_idea": "Bias-Aware Modular Neural Architectures for Handling Class Imbalance in Tabular Data Retrieval-Augmentation", "best_score": 0.5, "iters": 3, "beams": [{"idea": "Bias-Aware Modular Neural Architectures for Handling Class Imbalance in Tabular Data Retrieval-Augmentation", "score": 0.5}, {"idea": "Designing a Simple yet Efficient Retrieval Mechanism for Tabular Data using Knowledge Graph Embeddings", "score": 0.3}, {"idea": "Exploring the Impact of Knowledge Distillation on Tabular Data: A Meta-Learning Approach for Improved Retrieval-Augmented Deep Learning Models", "score": 0.25}], "gen_model": "llama3.2", "score_model": "o4-mini"}
{"custom_id": "1ec939c27a741902", "problem": "Simplicial neural networks face prohibitive memory and training-time requirements due to the large number of simplices and the need for on-the-fly aggregation, limiting their scalability for higher-order graph representation learning.", "target_idea": "Introduce a scalable simplicial-aware neural network that leverages pre-aggregated simplicial features as inputs, enabling constant run-time and memory usage regardless of the size or density of the simplicial complex while maintaining strong structural inductive bias.", "best_idea": "Memory-efficient simplicial neural networks via on-the-fly computation graph optimization", "best_score": 0.85, "iters": 3, "beams": [{"idea": "Memory-efficient simplicial neural networks via on-the-fly computation graph optimization", "score": 0.85}, {"idea": "Graph-Based Attention Fusion for Simplicial Neural Networks", "score": 0.55}, {"idea": "Meta-Learning-based Aggregation via Simplices on Simplices", "score": 0.53}], "gen_model": "llama3.2", "score_model": "o4-mini"}
{"custom_id": "3f7930e45125dc0d", "problem": "Current chain-of-thought prompting methods for large language models are limited by their reliance on static exemplars and insufficient mechanisms for iterative exploration and self-evaluation during complex reasoning tasks.", "target_idea": "Introduce an automated prompting framework that iteratively explores multiple reasoning paths, self-evaluates them, and uses error analysis to revise prompts, thereby enabling the model to accumulate and leverage an ensemble of reasoning experiences for improved problem solving.", "best_idea": "Develop an iterative meta-learning framework that refines prompts based on model performance on adversarial examples generated by internal exploration mechanisms", "best_score": 0.88, "iters": 3, "beams": [{"idea": "Develop an iterative meta-learning framework that refines prompts based on model performance on adversarial examples generated by internal exploration mechanisms", "score": 0.88}, {"idea": "Create a meta-reinforcement learning framework for chain-of-thought prompting that leverages transferable knowledge from the task's solution space to augment prompt refinement and answer evaluation.", "score": 0.8}, {"idea": "Employ meta-learnable attention mechanisms within the model to dynamically adapt to task-specific prompts and iteratively refine question-hypothetical alignment.", "score": 0.78}], "gen_model": "llama3.2", "score_model": "o4-mini"}
{"custom_id": "7f8fd92d4b2b5c90", "problem": "Existing privacy research on large language models has predominantly addressed the risk of memorized data extraction, overlooking the threat posed by LLMs inferring sensitive personal attributes from user-provided text during inference.", "target_idea": "Conduct a comprehensive analysis of pretrained LLMs' ability to infer personal attributes from real-world user text, and examine the effectiveness of current privacy mitigations against such inference-based privacy risks.", "best_idea": "Differential Privacy by Design for Large Language Models: Adapting Input Data Transformations and Model Update Strategies to Mitigate Attribute Inference Vulnerabilities", "best_score": 0.81, "iters": 3, "beams": [{"idea": "Differential Privacy by Design for Large Language Models: Adapting Input Data Transformations and Model Update Strategies to Mitigate Attribute Inference Vulnerabilities", "score": 0.81}, {"idea": "Attacking the Representations: Understanding the Effects of Pre-trained Large Language Models on Inference-based Attribute Extraction using Counterfactuals and Perturbations", "score": 0.79}, {"idea": "A reinforcement learning approach to training LLMs with adversarial examples generated from sensitive attributes, enabling the model to learn detection mechanisms for sensitive information during inference.", "score": 0.78}], "gen_model": "llama3.2", "score_model": "o4-mini"}
{"custom_id": "415798ee3c9c2379", "problem": "Online model-based reinforcement learning faces challenges due to data nonstationarity, leading to catastrophic forgetting in neural networks, and existing approaches for optimal online fitting require computationally expensive retraining on all past data.", "target_idea": "Develop a world model using linear regression over nonlinear random features, combined with a locality sensitive sparse encoding, to enable efficient incremental FTL updates and maintain model expressiveness for online learning in nonstationary environments.", "best_idea": "Temporal Fusion with Self-Attention for Online Model-Based RL", "best_score": 0.55, "iters": 3, "beams": [{"idea": "Temporal Fusion with Self-Attention for Online Model-Based RL", "score": 0.55}, {"idea": "Meta-Learning for Online Model-Based Reinforcement Learning with Time-Varying Contextual Embeddings", "score": 0.45}, {"idea": "Implementing Incremental Learning Schedules with Adaptive Loss Weights for Online Model-Based Reinforcement Learning", "score": 0.4}], "gen_model": "llama3.2", "score_model": "o4-mini"}
{"custom_id": "2e5c40d7167b443e", "problem": "Current neural subset selection methods often neglect the informative context provided by the superset when modeling set functions, limiting their ability to fully capture relationships between subsets and their supersets.", "target_idea": "Introduce an information aggregation module that merges representations of subsets and supersets in a permutation-invariant manner, ensuring that learning incorporates sufficient statistics of the superset for improved subset selection.", "best_idea": "Graph Convolutional Networks with Superset Embeddings: Enhancing Subset Selection via Adaptive Representation Learning", "best_score": 0.88, "iters": 3, "beams": [{"idea": "Graph Convolutional Networks with Superset Embeddings: Enhancing Subset Selection via Adaptive Representation Learning", "score": 0.88}, {"idea": "Investigate the application of graph neural networks to model subset-superset relationships, leveraging graph convolutional layers to capture contextual information from the superset.", "score": 0.85}, {"idea": "Probabilistic Graph Neural Networks for Superset-Supervised Subset Selection: Bridging the Gap between Predictive and Explanatory Performance", "score": 0.85}], "gen_model": "llama3.2", "score_model": "o4-mini"}
{"custom_id": "a6af25222211b026", "problem": "Frozen large language models lack the ability to directly interpret visual information, and existing linear transformation methods inadequately align the embedding spaces of pretrained vision models and LLMs for general-purpose multimodal understanding.", "target_idea": "Propose a framework that maps visual embeddings into the LLM word embedding space via a linear transformation and formulates the alignment as an optimal transport problem, enabling consistent assignment and shared semantic grounding between visual and linguistic modalities.", "best_idea": "Contrastive Multimodal Pretraining with Self-Supervised Vision-Lingual Alignment", "best_score": 0.48, "iters": 3, "beams": [{"idea": "Contrastive Multimodal Pretraining with Self-Supervised Vision-Lingual Alignment", "score": 0.48}, {"idea": "Pretrained Vision-LLM Fusion via Progressive Residual Network Embeddings", "score": 0.48}, {"idea": "Visual-LLM Mutual Information Maximization with Conditional Normalizing Flows", "score": 0.47}], "gen_model": "llama3.2", "score_model": "o4-mini"}
{"custom_id": "55bce820f6dcb229", "problem": "The shift in NLP tasks towards human-aligned applications introduces new challenges for evaluating large language models, particularly in terms of generality, flexibility across protocols, and interpretability, which are not adequately addressed by existing evaluation methods.", "target_idea": "Develop a large generative judge model trained on diverse real-world user queries and LLM responses, capable of handling multiple evaluation protocols and providing structured natural language critiques to comprehensively assess aligned language models.", "best_idea": "Develop a multi-protocol task framework utilizing transfer learning and ensemble methods to evaluate large language models' adaptability across varied NLP tasks.", "best_score": 0.6, "iters": 3, "beams": [{"idea": "Develop a multi-protocol task framework utilizing transfer learning and ensemble methods to evaluate large language models' adaptability across varied NLP tasks.", "score": 0.6}, {"idea": "Propose a human-in-the-loop evaluation protocol incorporating active learning and meta-learning techniques to adapt evaluation metrics and ranking protocols based on model performance on unseen tasks", "score": 0.52}, {"idea": "Construct a hybrid evaluation framework integrating knowledge graph-based retrieval, human annotation, and transfer learning to evaluate the robustness and flexibility of large language models in human-aligned applications.", "score": 0.5}], "gen_model": "llama3.2", "score_model": "o4-mini"}
{"custom_id": "257de07b566d0fa5", "problem": "There is a limited theoretical understanding of the expressive power of Graph Neural Networks, particularly regarding what they can and cannot learn, despite various enhancements based on Weisfeiler-Lehman tests.", "target_idea": "Analyze GNN expressiveness through a probabilistic lens by relating their predictions to inference in probabilistic graphical models, and develop a hierarchical framework to characterize and enhance their ability to capture complex dependencies.", "best_idea": "Investigate the role of attention mechanisms in Graph Neural Networks on Weisfeiler-Lehman similarity, exploring their impact on the theoretical understanding of their expressive power.", "best_score": 0.6, "iters": 3, "beams": [{"idea": "Investigate the role of attention mechanisms in Graph Neural Networks on Weisfeiler-Lehman similarity, exploring their impact on the theoretical understanding of their expressive power.", "score": 0.6}, {"idea": "Develop and analyze novel model architectures that incorporate insights from graph signal processing to improve the understanding of Graph Neural Networks' limitations on learning graph structures.", "score": 0.55}, {"idea": "Investigate the application of differential algebraic topology to refine Graph Neural Networks' expressiveness by identifying and optimizing the topological invariants that can be learned.", "score": 0.53}], "gen_model": "llama3.2", "score_model": "o4-mini"}
{"custom_id": "2739fd533a598e39", "problem": "Existing methods for predictive uncertainty estimation often fail to capture diverse and instance-specific notions of uncertainty, limiting their effectiveness for both robust training interventions and test-time applications such as selective classification.", "target_idea": "Introduce an instance-conditional reweighting approach that leverages an auxiliary network trained via a bilevel optimization framework with a meta-objective of minimizing dropout variance, thereby unifying and enhancing uncertainty estimation for both training and inference scenarios.", "best_idea": "Design a Bayesian neural network with a novel hierarchical attention mechanism to model instance-specific uncertainty.", "best_score": 0.58, "iters": 3, "beams": [{"idea": "Design a Bayesian neural network with a novel hierarchical attention mechanism to model instance-specific uncertainty.", "score": 0.58}, {"idea": "Explore attention-based neural network ensembling to capture context-dependent uncertainty relationships", "score": 0.55}, {"idea": "Develop and investigate ensemble methods leveraging different machine learning paradigms (e.g., ensemble neural networks, support vector machines) to aggregate multiple models with diverse uncertainty estimates", "score": 0.52}], "gen_model": "llama3.2", "score_model": "o4-mini"}
